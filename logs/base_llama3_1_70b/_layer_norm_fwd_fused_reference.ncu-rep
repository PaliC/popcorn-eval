==PROF== Connected to process 2516827 (/home/sahanp/.conda/envs/parity-bench/bin/python3.11)
==PROF== Disconnected from process 2516827
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","hz","1,583,660,130.72",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","hz","1,320,526,960.78",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,839",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","24.03",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.23",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","ns","8,160",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","37.77",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","33.41",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,855.08",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","24.15",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.","",""
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","16",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.48",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","16",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","28",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","42.79",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.39",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (42.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","local","57.21"
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2,743.33",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","620,288",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6,855.08",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,422,264",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6,793.78",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,044,864",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6,855.08",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,422,264",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","6,532.01",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5,689,056",
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.46% above the average, while the minimum instance value is 13.35% below the average.","global","7.289"
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.15% above the average, while the minimum instance value is 15.31% below the average.","global","7.972"
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.46% above the average, while the minimum instance value is 13.35% below the average.","global","7.289"
"0","2516827","python3.11","127.0.0.1","_layer_norm_fwd_fused","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.46% above the average, while the minimum instance value is 13.51% below the average.","global","10.9"
