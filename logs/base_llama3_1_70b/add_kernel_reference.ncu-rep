==PROF== Connected to process 2514260 (/home/sahanp/.conda/envs/parity-bench/bin/python3.11)
==PROF== Disconnected from process 2514260
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","hz","1,562,169,312.17",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","hz","1,327,659,970.24",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","5,361",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.93",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","8.17",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","ns","4,032",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","8.71",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","19.99",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,360.01",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","0.77",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details.","",""
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Grid Size","","97",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","26",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Threads","thread","12,416",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.05",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","The grid for this launch is configured to execute only 97 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","26.52"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","16",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.02",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.85",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","local","93.98"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","514.67",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","302,336",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1,360.01",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","706,592",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2,536.19",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","514,080",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1,360.01",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","706,592",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1,284.12",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","2,826,368",
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 35.51% above the average, while the minimum instance value is 100.00% below the average.","global","9.023"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 36.27% above the average, while the minimum instance value is 100.00% below the average.","global","8.701"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 35.51% above the average, while the minimum instance value is 100.00% below the average.","global","9.023"
"0","2514260","python3.11","127.0.0.1","add_kernel","1","7","(128, 1, 1)","(97, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.35% above the average, while the minimum instance value is 9.51% below the average.","global","8.689"
