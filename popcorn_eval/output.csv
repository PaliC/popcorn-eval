==PROF== Connected to process 1584410 (/home/sahanp/.conda/envs/parity-bench/bin/python3.11)
âœ… Triton and Torch match
==PROF== Disconnected from process 1584410
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description"
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,738,619.68",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,333,210,008.26",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,556",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.79",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.01",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,528",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.73",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.50",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","887.90",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.43",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"0","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,637,298.09",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,334,517,827.64",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,570",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.74",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.01",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,528",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","51.66",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.62",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","909.61",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.44",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"1","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,583,148.56",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,331,858,023.84",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,411",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.78",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.03",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,432",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.63",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.50",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","889.57",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.58",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.43",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"2","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,588,105,726.87",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,335,619,149.23",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,589",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.77",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.01",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,528",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.26",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.21",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","896.62",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.43",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"3","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,589,970,501.47",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,332,878,180.31",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,469",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.76",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.02",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,464",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.73",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.27",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","887.98",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.57",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.43",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"4","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,714,285.71",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,333,576,579.67",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,606",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.77",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.00",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,560",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","51.68",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.18",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","906.73",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.44",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"5","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,179,487.18",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,333,516,483.52",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,603",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.75",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.00",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,560",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.01",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.18",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","902.38",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.43",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"6","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,615,384.62",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,334,040,178.57",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19,606",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.76",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.01",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","14,560",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.01",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","6.60",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","901.69",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","2.56",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Block Size","","256",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Grid Size","","8",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","214",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","147,456",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","1",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","1",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","8",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","12.44",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","7.96",
"7","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(256, 1, 1)","(8, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,588,414,634.15",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,243,295,064.79",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,079",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.65",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.16",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,496",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.87",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.56",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,179.33",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.80",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"8","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,594,191,919.19",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,243,110,795.45",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,157",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.59",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.12",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,560",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.24",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.37",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,173.45",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.78",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"9","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,349,397.59",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,240,293,204.07",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,213",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.58",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.11",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,624",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.13",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.71",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,203.18",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.77",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"10","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,822,085.89",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,604,773.77",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,966",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.73",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.20",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,432",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.67",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.51",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,163.06",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.84",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"11","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,585,585.59",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,241,823,855.11",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,262",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.54",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.11",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,656",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.34",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.39",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,174.32",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.75",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"12","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,580,838.32",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,444,985.03",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,262",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.50",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.10",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,688",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.19",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.27",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,172.83",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.75",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"13","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,590,361,445.78",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,243,046,404.37",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,233",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.54",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.11",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,624",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.13",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.44",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,179.01",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.76",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"14","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,798,816.57",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,241,840,791.42",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13,473",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.44",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.04",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","10,816",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.47",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.39",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,162.36",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.70",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","200",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"15","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,592,178.77",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,244,129,713.69",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,278",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.84",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.82",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,456",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.42",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.16",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,278.39",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.48",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"16","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,523,277.47",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,244,544,343.58",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,283",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.83",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.81",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,456",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","42.66",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.07",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,300.89",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.48",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"17","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,592,696,629.21",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,244,436,885.53",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,200",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.88",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.82",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,392",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.41",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.10",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,277.70",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.50",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"18","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,592,178.77",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,242,274,790.50",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,253",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.89",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.82",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,456",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","42.83",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.10",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,295.55",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.49",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"19","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,633,053.22",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,241,662,289.92",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,219",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.86",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.84",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,424",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.14",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.09",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,278.95",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.50",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"20","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,593,484,419.26",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,243,316,218.13",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,067",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.93",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.85",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,296",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.17",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.30",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,283.86",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.54",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"21","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,391,713.75",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,241,028,866.53",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,098",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.95",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.87",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,328",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.16",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.12",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,284.27",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.53",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"22","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,269,662.92",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,240,662,306.88",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14,163",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.88",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.84",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","11,392",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","42.84",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.13",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,293.65",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","3.51",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Grid Size","","16",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","190",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Threads","thread","2,048",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","3",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"23","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(16, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,081,081.08",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,612,542.23",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,720",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.56",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.61",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,472",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.18",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.26",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,041.06",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.33",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"24","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,980,920.31",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,233,875,210.44",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,751",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.48",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.58",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,504",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.97",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.21",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,996.58",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.31",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"25","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,644,144.14",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,235,509,923.99",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,752",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.55",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.61",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,472",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.55",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.15",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,027.70",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.32",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"26","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,775,086.51",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,470,047.58",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,469",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.70",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.71",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,248",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","37.51",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.49",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,981.51",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.42",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"27","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,576,111,111.11",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,231,992,187.50",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,845",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.35",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.57",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,600",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","38.02",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.25",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,973.47",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.28",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"28","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,030,716.72",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,948,272.18",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,615",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.54",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.64",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,376",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.87",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.37",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,017.37",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.37",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"29","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,502,768.55",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,235,517,026.58",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,932",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.44",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.53",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,632",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.60",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.21",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,035.35",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.25",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"30","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,896,396.40",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,203,441.72",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,728",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.49",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.60",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,472",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","37.69",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.35",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,974.64",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.33",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","116",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"31","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,578,735,632.18",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,232,650,862.07",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,479",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.42",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.71",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,280",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.02",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.30",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,869.46",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.38",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"32","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,206,896.55",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,106,142.24",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,521",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.35",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.69",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,280",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.87",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.13",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,000.33",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.37",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"33","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,380,116.96",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,230,537,280.70",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,262",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.59",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.78",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,120",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.08",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.94",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,924.09",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.47",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"34","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,588,435,374.15",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,667,304.42",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,646",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.27",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.62",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,408",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.70",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.03",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,947.47",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.32",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"35","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,063,437.14",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,233,307,201.56",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,439",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.37",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.71",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,248",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.77",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.25",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,995.45",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.40",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"36","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,590,935,672.51",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,047,149.12",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,300",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.52",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.76",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,120",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.92",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.45",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,924.68",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.45",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"37","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,714,285.71",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,179,687.50",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,087",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.74",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.86",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,960",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.74",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.03",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,943.94",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.54",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"38","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,589,041,095.89",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,233,558,968.32",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,567",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.35",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.65",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,344",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.05",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.36",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,910.52",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.35",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","122",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","200,704",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","49,152",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.06",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","4",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","25",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"39","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (25.0%) is limited by the number of required registers. This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The difference between calculated theoretical (25.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,595,995,288.57",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,229,530,145.76",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,176",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.98",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.77",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,056",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.47",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.98",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,869.24",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.03",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"40","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,365,853.66",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,225,160,605.40",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,291",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.27",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.74",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,184",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.59",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.27",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,894.33",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.95",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"41","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,192,439.86",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,227,676,653.78",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,471",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.70",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.67",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,312",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","29.99",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.70",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,942.90",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.82",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"42","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,578,735,632.18",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,228,030,711.21",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,439",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.58",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.71",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,280",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.00",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.58",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,930.24",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.85",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"43","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,899,543.38",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,227,191,245.72",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,495",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.97",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.67",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,344",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.57",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.97",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,755.90",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.80",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"44","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,589,041,095.89",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,230,522,260.27",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,534",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.77",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.64",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,344",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.29",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.77",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,779.86",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.78",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"45","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,192,439.86",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,230,334,514.60",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,492",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.75",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.68",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,312",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","29.34",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.75",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","4,043.86",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.80",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"46","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,314,878.89",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,227,143,706.75",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,387",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.54",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.72",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","9,248",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.69",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.54",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3,890.33",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.88",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","64",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","40,960",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.10",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","8",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","5",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","20",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","31.25",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"47","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (31.2%) is limited by the required amount of shared memory. The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,820,512.82",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,225,826,322.12",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,238",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.68",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.22",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,320",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.09",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.68",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,418.30",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.53",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"48","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,749,049.43",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,224,691,064.64",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,347",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.58",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.18",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,416",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.65",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.58",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,555.37",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.43",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"49","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,590,038,314.18",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,224,317,528.74",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,280",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.63",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.20",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,352",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.50",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.63",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,595.40",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.51",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"50","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,589,987,325.73",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,223,057,271.86",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,329",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.47",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.16",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,416",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.83",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.47",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,531.04",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.44",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"51","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,615,384.62",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,221,965,144.23",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,228",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.81",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.24",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,320",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.69",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.81",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,519.64",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.56",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"52","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,121,212.12",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,223,174,124.05",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,374",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.96",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.15",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,448",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.81",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.96",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,554.90",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.41",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"53","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,591,408,268.73",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,226,017,441.86",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,171",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.95",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.25",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,256",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.52",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.95",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,601.82",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.60",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"54","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,452,471.48",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,226,844,700.57",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,384",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.93",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.17",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,416",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.66",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.93",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,540.62",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.42",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"55","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,419,753.09",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,226,374,421.30",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,644",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.24",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.04",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,640",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.60",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.93",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,841.60",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.13",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"56","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,419,753.09",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,228,182,870.37",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,657",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.19",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.04",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,640",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.62",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.67",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,825.58",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.11",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"57","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,089,552.24",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,222,350,163.25",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,525",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.39",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.09",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,576",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.87",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.96",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,756.07",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.24",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"58","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,360,594.80",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,225,749,302.97",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,594",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.27",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.06",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,608",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.87",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.21",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,747.29",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.17",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"59","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,009,803.92",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,223,733,340.99",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,697",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.14",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.00",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,704",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.48",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.68",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,865.76",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.07",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"60","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,360,594.80",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,223,382,318.77",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,564",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.35",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.06",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,608",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.95",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.90",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,749.27",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.19",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"61","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,333,333.33",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,227,976,329.29",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,573",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.30",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.09",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,576",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.79",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.80",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,771.52",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.19",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"62","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,590,458,488.23",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,227,651,603.16",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,614",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.25",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.05",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,608",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.96",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.04",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,724.70",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.15",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","64",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","93",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","24,576",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.11",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","10",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","9",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","32",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","18",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","28.12",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","3.12",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"63","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(64, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (28.1%) is limited by the required amount of shared memory. The difference between calculated theoretical (28.1%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,417,582.42",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,775,641.03",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,821",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.66",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.01",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,736",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.32",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.85",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,792.98",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.60",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"64","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,733,812.95",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,318,794.96",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,013",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.53",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.92",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,896",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.43",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.61",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,821.05",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.52",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"65","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,591,240,875.91",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,241,930,885.04",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,930",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.57",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.96",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,768",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.84",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.40",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,758.46",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.56",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"66","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,333,333.33",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,323,709.24",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,968",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.54",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.95",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,832",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.18",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.34",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,794.81",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.54",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"67","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,577,617,328.52",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,235,883,912.45",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,989",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.52",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.95",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,864",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","45.17",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.30",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,751.69",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.53",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"68","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,417,582.42",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,246,222.53",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,818",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.68",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.01",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,736",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.23",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.62",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,790.29",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.60",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"69","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,733,812.95",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,159,509.89",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11,032",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.48",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.92",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,896",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.34",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.43",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,793.97",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.51",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"70","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,563,345.63",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,396,563.65",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,769",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.69",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.04",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,672",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.61",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.69",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,782.40",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.62",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"71","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,588,014,981.27",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,306,179.78",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,599",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.43",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.11",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,544",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","39.75",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.97",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,701.71",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.70",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"72","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,749,049.43",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,235,845,413.50",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,431",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.57",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.21",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,416",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.96",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.00",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,653.82",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.77",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"73","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,584,917,617.24",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,237,078,184.41",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,446",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.60",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.20",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,416",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.62",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.85",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,671.29",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.77",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"74","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,579,545,454.55",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,120,975.38",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,470",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.56",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.19",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,448",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.55",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.89",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,668.60",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.75",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"75","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,579,545,454.55",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,233,368,844.70",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,462",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.58",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.19",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,448",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.42",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.13",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,672.88",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.77",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"76","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,872,586.87",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,234,842,543.44",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,271",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.67",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.27",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,288",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.39",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.11",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,680.62",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.85",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"77","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,582,706,766.92",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,236,181,273.50",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,561",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.47",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.15",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,512",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","39.25",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.04",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,716.15",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.72",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"78","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,590,733,590.73",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,238,296,332.05",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,295",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.67",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.26",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,288",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","40.45",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.25",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,672.77",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.84",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"79","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,673,469.39",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,231,473,214.29",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,687",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.44",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.72",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,840",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","45.64",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.30",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,843.29",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.99",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"80","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,591,220,850.48",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,232,494,212.96",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,620",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.39",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.59",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,776",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","45.13",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.55",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,843.99",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.05",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"81","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,587,500,000",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,231,787,109.38",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,497",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.33",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.67",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,680",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.72",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.73",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,883.44",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.15",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"82","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,580,246,913.58",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,230,999,228.40",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,626",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.29",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.62",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,776",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.37",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.60",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,932.39",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.06",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"83","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,586,497,890.30",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,231,144,514.77",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,374",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.71",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.74",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,584",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.05",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.74",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,905.33",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.26",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"84","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,581,955,922.87",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,232,728,564.05",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,584",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.07",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.64",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,744",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.73",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.32",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,852.86",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.08",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"85","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,333,333.33",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,229,839,777.54",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,317",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.84",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.78",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,552",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","45.23",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.78",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,840.71",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.31",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"86","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,583,333,333.33",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,230,859,375",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,479",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.50",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.68",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","7,680",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","43.96",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.59",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2,903.05",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.16",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Grid Size","","64",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","96",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","81,920",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Threads","thread","8,192",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.24",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","5",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.22",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"87","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(64, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1,585,454,545.45",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1,239,389,204.55",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,946",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.61",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.96",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","nsecond","8,800",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","44.86",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","12.30",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1,768.42",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","4.55",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferShared",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Grid Size","","32",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","128",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","byte","233,472",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","98,304",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Threads","thread","4,096",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.12",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","4",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","2",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","12.50",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","6.23",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"88","1584410","python3.11","127.0.0.1","matmul_kernel","1","7","(128, 1, 1)","(32, 1, 1)","0","9.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
