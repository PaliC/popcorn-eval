[[prompts]]
name = "matmul_kernel"
skip = true
template_file = "code_templates/matmul_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs matrix multiplication on matrices. The kernel should have the function signature:
```python
def matmul_kernel(
        # Pointers to matrices
        a_ptr, b_ptr, c_ptr,
        # Matrix dimensions
        M, N, K,
        # The stride variables represent how much to increase the ptr by when moving by 1
        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
        # by to get the element one row down (A has M rows).
        stride_am, stride_ak,  #
        stride_bk, stride_bn,  #
        stride_cm, stride_cn,
        # Meta-parameters
        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #
        GROUP_SIZE_M: tl.constexpr,  #
        ACTIVATION: tl.constexpr  #
):
```

"""
# taken from triton documentation
reference_kernel = """
```python
@triton.jit
def matmul_kernel(
        # Pointers to matrices
        a_ptr, b_ptr, c_ptr,
        # Matrix dimensions
        M, N, K,
        # The stride variables represent how much to increase the ptr by when moving by 1
        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`
        # by to get the element one row down (A has M rows).
        stride_am, stride_ak,  #
        stride_bk, stride_bn,  #
        stride_cm, stride_cn,
        # Meta-parameters
        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #
        GROUP_SIZE_M: tl.constexpr,  #
        ACTIVATION: tl.constexpr  #
):
    # -----------------------------------------------------------
    # Map program ids `pid` to the block of C it should compute.
    # This is done in a grouped ordering to promote L2 data reuse.
    # See above `L2 Cache Optimizations` section for details.
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B.
    # We will advance this pointer as we move in the K direction
    # and accumulate
    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
    # See above `Pointer Arithmetic` section for details
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    # -----------------------------------------------------------
    # Iterate to compute a block of the C matrix.
    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
    # of fp32 values for higher accuracy.
    # `accumulator` will be converted back to fp16 after the loop.
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Load the next block of A and B, generate a mask by checking the K dimension.
        # If it is out of bounds, set it to 0.
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        # We accumulate along the K dimension.
        accumulator = tl.dot(a, b, accumulator)
        # Advance the ptrs to the next K block.
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
    # You can fuse arbitrary activation functions here
    # while the accumulator is still in FP32!
    if ACTIVATION == "leaky_relu":
        accumulator = leaky_relu(accumulator)
    c = accumulator.to(tl.float16)

    # -----------------------------------------------------------
    # Write back the block of the output matrix C with masks.
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)


# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `matmul_kernel`.
@triton.jit
def leaky_relu(x):
    return tl.where(x >= 0, x, 0.01 * x)
```
"""


[[prompts]]
# taken from triton documentation
name = "softmax_kernel"
template_file = "code_templates/softmax_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs a softmax operation on a given input matrix. The kernel should have the function signature:
```python
def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                   num_stages: tl.constexpr):
```
"""
# taken from triton documentation
reference_kernel = """
```python
@triton.jit
def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                   num_stages: tl.constexpr):
    # starting row of the program
    row_start = tl.program_id(0)
    row_step = tl.num_programs(0)
    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
        # The stride represents how much we need to increase the pointer to advance 1 row
        row_start_ptr = input_ptr + row_idx * input_row_stride
        # The block size is the next power of two greater than n_cols, so we can fit each
        # row in a single block
        col_offsets = tl.arange(0, BLOCK_SIZE)
        input_ptrs = row_start_ptr + col_offsets
        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
        mask = col_offsets < n_cols
        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
        # Subtract maximum for numerical stability
        row_minus_max = row - tl.max(row, axis=0)
        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
        numerator = tl.exp(row_minus_max)
        denominator = tl.sum(numerator, axis=0)
        softmax_output = numerator / denominator
        # Write back output to DRAM
        output_row_start_ptr = output_ptr + row_idx * output_row_stride
        output_ptrs = output_row_start_ptr + col_offsets
        tl.store(output_ptrs, softmax_output, mask=mask)
```

"""
[[prompts]]
# taken from triton documentation
name = "add_kernel"
template_file = "code_templates/add_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs an elementwise addition operation given two input vectors. The kernel should have the function signature:
```python
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
```

"""
# taken from triton documentation
reference_kernel = """
```python
@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)
```
"""

[[prompts]]
name = "_layer_norm_fwd_fused"
template_file = "code_templates/_layer_norm_fwd_fused_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs the forward pass of a layer normalization operation given an input matrix. The kernel should have the function signature:
```python
def _layer_norm_fwd_fused(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride,  # how much to increase the pointer when moving by 1 row
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    BLOCK_SIZE: tl.constexpr,
):
"""
reference_kernel = """
```python
@triton.jit
def _layer_norm_fwd_fused(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride,  # how much to increase the pointer when moving by 1 row
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    BLOCK_SIZE: tl.constexpr,
):
    # Map the program id to the row of X and Y it should compute.
    row = tl.program_id(0)
    Y += row * stride
    X += row * stride
    # Compute mean
    mean = 0
    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)
        _mean += a
    mean = tl.sum(_mean, axis=0) / N
    # Compute variance
    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)
        x = tl.where(cols < N, x - mean, 0.)
        _var += x * x
    var = tl.sum(_var, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    # Write mean / rstd
    tl.store(Mean + row, mean)
    tl.store(Rstd + row, rstd)
    # Normalize and apply linear transformation
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < N
        w = tl.load(W + cols, mask=mask)
        b = tl.load(B + cols, mask=mask)
        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)
        x_hat = (x - mean) * rstd
        y = x_hat * w + b
        # Write output
        tl.store(Y + cols, y, mask=mask)
```

"""

[[prompts]]
name = "_layer_norm_bwd_dx_fused"
template_file = "code_templates/_layer_norm_bwd_dx_fused_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs the backward pass of a layer normalization operation, specifically computing the gradient with respect to the input (DX) and accumulating gradients for the weights (DW) and biases (DB). The kernel should have the function signature:
```python
def _layer_norm_bwd_dx_fused(DX,  # pointer to the input gradient
                             DY,  # pointer to the output gradient
                             DW,  # pointer to the partial sum of weights gradient
                             DB,  # pointer to the partial sum of biases gradient
                             X,  # pointer to the input
                             W,  # pointer to the weights
                             Mean,  # pointer to the mean
                             Rstd,  # pointer to the 1/std
                             Lock,  # pointer to the lock
                             stride,  # how much to increase the pointer when moving by 1 row
                             N,  # number of columns in X
                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def _layer_norm_bwd_dx_fused(DX,  # pointer to the input gradient
                             DY,  # pointer to the output gradient
                             DW,  # pointer to the partial sum of weights gradient
                             DB,  # pointer to the partial sum of biases gradient
                             X,  # pointer to the input
                             W,  # pointer to the weights
                             Mean,  # pointer to the mean
                             Rstd,  # pointer to the 1/std
                             Lock,  # pointer to the lock
                             stride,  # how much to increase the pointer when moving by 1 row
                             N,  # number of columns in X
                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):
    # Map the program id to the elements of X, DX, and DY it should compute.
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_SIZE_N)
    mask = cols < N
    X += row * stride
    DY += row * stride
    DX += row * stride
    # Offset locks and weights/biases gradient pointer for parallel reduction
    lock_id = row % GROUP_SIZE_M
    Lock += lock_id
    Count = Lock + GROUP_SIZE_M
    DW = DW + lock_id * N + cols
    DB = DB + lock_id * N + cols
    # Load data to SRAM
    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    mean = tl.load(Mean + row)
    rstd = tl.load(Rstd + row)
    # Compute dx
    xhat = (x - mean) * rstd
    wdy = w * dy
    xhat = tl.where(mask, xhat, 0.)
    wdy = tl.where(mask, wdy, 0.)
    c1 = tl.sum(xhat * wdy, axis=0) / N
    c2 = tl.sum(wdy, axis=0) / N
    dx = (wdy - (xhat * c1 + c2)) * rstd
    # Write dx
    tl.store(DX + cols, dx, mask=mask)
    # Accumulate partial sums for dw/db
    partial_dw = (dy * xhat).to(w.dtype)
    partial_db = (dy).to(w.dtype)
    while tl.atomic_cas(Lock, 0, 1) == 1:
        pass
    count = tl.load(Count)
    # First store doesn't accumulate
    if count == 0:
        tl.atomic_xchg(Count, 1)
    else:
        partial_dw += tl.load(DW, mask=mask)
        partial_db += tl.load(DB, mask=mask)
    tl.store(DW, partial_dw, mask=mask)
    tl.store(DB, partial_db, mask=mask)
    # Release the lock
    tl.atomic_xchg(Lock, 0)
```
"""

[[prompts]]
name = "_layer_norm_bwd_dwdb"
template_file = "code_templates/_layer_norm_bwd_dwdb_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs the backward pass of a layer normalization operation, specifically accumating the gradients of the weights (DW) and biases (DB). The kernel should have the function signature:
```python
def _layer_norm_bwd_dwdb(DW,  # pointer to the partial sum of weights gradient
                         DB,  # pointer to the partial sum of biases gradient
                         FINAL_DW,  # pointer to the weights gradient
                         FINAL_DB,  # pointer to the biases gradient
                         M,  # GROUP_SIZE_M
                         N,  # number of columns
                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def _layer_norm_bwd_dwdb(DW,  # pointer to the partial sum of weights gradient
                         DB,  # pointer to the partial sum of biases gradient
                         FINAL_DW,  # pointer to the weights gradient
                         FINAL_DB,  # pointer to the biases gradient
                         M,  # GROUP_SIZE_M
                         N,  # number of columns
                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):
    # Map the program id to the elements of DW and DB it should compute.
    pid = tl.program_id(0)
    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    # Iterate through the rows of DW and DB to sum the partial sums.
    for i in range(0, M, BLOCK_SIZE_M):
        rows = i + tl.arange(0, BLOCK_SIZE_M)
        mask = (rows[:, None] < M) & (cols[None, :] < N)
        offs = rows[:, None] * N + cols[None, :]
        dw += tl.load(DW + offs, mask=mask, other=0.)
        db += tl.load(DB + offs, mask=mask, other=0.)
    # Write the final sum to the output.
    sum_dw = tl.sum(dw, axis=0)
    sum_db = tl.sum(db, axis=0)
    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)
    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)
```
"""

[[prompts]]
name = "constant_add_kernel"
template_file = "code_templates/constant_add_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs an elementwise addition operation given one input vector. It should add a constant value of 10 to each element of the input vector. The kernel should have the function signature:
```python
def constant_add_kernel(x_ptr, z_ptr, N0, B0: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def constant_add_kernel(x_ptr, z_ptr, N0, B0: tl.constexpr):
    range = tl.arange(0, B0)
    x = tl.load(x_ptr + range)
    x += 10
    tl.store(z_ptr + range, x)
```
"""

[[prompts]]
name = "outer_vector_add_kernel"
template_file = "code_templates/outer_vector_add_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs an elementwise addition operation given two input vectors.

The triton kernel should have the same functionality as the following code which uses pytorch
```python
def outer_vector_add(x: Float32[Tensor, "32"], y: Float32[Tensor, "32"]) -> Float32[Tensor, "32 32"]:
    return x[None, :] + y[:, None]
```

Block size B0 is always the same as vector x length N0. Block size B1 is always the same as vector y length N1.

The kernel should have the function signature:
```python
def outer_vector_add_kernel(x_ptr, y_ptr, z_ptr, N0, B0: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def outer_vector_add_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):
    range0 = tl.arange(0, B0)
    range1 = tl.arange(0, B1)

    x = tl.load(x_ptr + range0)
    y = tl.load(y_ptr + range1)
    z = x[None, :] + y[:, None]

    range_block = range1[:, None] * N0 + range0[None, :]
    tl.store(z_ptr + range_block, z)
    return
```
"""

[[prompts]]
name = "outer_matmul_kernel"
template_file = "code_templates/outer_matmul_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that mulitplies a row vector to a column vector and take a relu. It should use two program block axes. Block size B0 is always less than the vector x length N0. Block size B1 is always less than vector y length N1.. The kernel should have the function signature:
```python
def outer_matmul_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):
```
"""

reference_kernel = """
```python
@triton.jit
def outer_matmul_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)

    range0 = tl.arange(0, B0) + pid_0 * B0
    range1 = tl.arange(0, B1) + pid_1 * B1

    x = tl.load(x_ptr + range0, mask=range0 < N0)
    y = tl.load(y_ptr + range1, mask=range1 < N1)

    z = tl.maximum(x[None, :] * y[:, None], 0)

    range_block = range1[:, None] * N0 + range0[None, :]
    mask = (range0[None, :] < N0) & (range1[:, None] < N1)
    tl.store(z_ptr + range_block, z, mask=mask)

    return

```
"""

[[prompts]]
name = "outer_matmul_bwd_kernel"
template_file = "code_templates/outer_matmul_bwd_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs the backward pass of the outer matmul operation.
It should use two program blocks. Block size B0 is always less than the vector x length N0. Block size B1 is always less than vector y length N1. Chain rule backward dz is of shape N0
The kernel should have the function signature:
```python
def outer_matmul_bwd_kernel(
    x_ptr, y_ptr, dz_ptr, dx_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr
):
```
"""
reference_kernel = """
```python

@triton.jit
def outer_matmul_bwd_kernel(
    x_ptr, y_ptr, dz_ptr, dx_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr
):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)

    # z = relu(x * y)
    # dL/dx = dL/dz * dz/dx
    #       = dL/dx * drelu(x*y)/d(x*y) * d(x*y)/dx
    #       = dL/dx * (x * y > 0) * y

    range_0 = tl.arange(0, B0) + pid_0 * B0
    range_1 = tl.arange(0, B1) + pid_1 * B1
    range_block = range_1[:, None] * N0 + range_0[None, :]
    mask_block = (range_0[None, :] < N0) & (range_1[:, None] < N1)

    x = tl.load(x_ptr + range_block, mask=mask_block)
    y = tl.load(y_ptr + range_1, mask=range_1 < N1)
    dz = tl.load(dz_ptr + range_block, mask=mask_block)

    dx = dz * (x * y[:, None] > 0) * y[:, None]

    tl.store(dx_ptr + range_block, dx, mask=mask_block)

    return
```
"""

[[prompts]]
name = "long_sum_kernel"
template_file = "code_templates/long_sum_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel that sums a batch of numbers. Block size B0 represents a range of batches of x of length N0. Each element is of length T. Process it B1 < T elements at a time.
The kernel should have the function signature:
```python
def long_sum_kernel(x_ptr, z_ptr, N0, T, B0: tl.constexpr, B1: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def long_sum_kernel(x_ptr, z_ptr, N0, N1, T, B0: tl.constexpr, B1: tl.constexpr):
    # Get program ID for the row dimension
    pid_0 = tl.program_id(0)

    # Calculate row offset and check if it's valid
    row_idx = pid_0 * B0 + tl.arange(0, B0)
    row_mask = row_idx < N0

    # Initialize accumulator for the sum
    accumulator = tl.zeros([B0], dtype=tl.float32)

    # Loop over columns in blocks
    for col_offset in range(0, T, B1):
        # Calculate column indices and mask for this block
        col_idx = col_offset + tl.arange(0, B1)
        col_mask = col_idx < T

        # Calculate the offsets into the input array
        offsets = row_idx[:, None] * T + col_idx[None, :]

        # Load values from input, masking invalid locations
        block = tl.load(
            x_ptr + offsets, mask=row_mask[:, None] & col_mask[None, :], other=0.0
        )

        # Add to accumulator
        accumulator += tl.sum(block, axis=1)

    # Store final results
    tl.store(z_ptr + row_idx, accumulator, mask=row_mask)
```
"""

[[prompts]]
name = "simple_fa_kernel"
skip = true
template_file = "code_templates/simple_fa_template.py"
system_prompt = """
You are a helpful assistant which writes kernels for a programming gpus. Specifically you write kernels in the triton python DSL. All of your reponses should only contain runnable python code with valid comments.
DO NOT INCLUDE ANY EXTRANEOUS OR EXPLANTION TEXT THAT IS NOT IN CODE COMMENTS. All comments should be brief and to the point.
Only include the kernel function and helper functions for the triton kernel. Assume there already exists a main function that runs the kernel with correct arguments. Assume that the file only has the following imports
```python
import torch
import triton
import triton.language as tl
```

If a function is given, respect the function signature.
"""
user_prompt = """
Write a triton kernel function that performs a simple feedforward operation specifically for a scaler version of flash attention.
Specifically, It should use zero programs. Block size B0 represents k of length N0. Block size B0 represents q of length N0.
Block size B0 represents v of length N0. Sequence length is T. Process it B1 < T elements at a time. The operation should only use 1 for loop.
Note that: "$$z_{i} = \\sum_{j} \\text{softmax}(q_1 k_1, \\ldots, q_T k_T)_j v_{j} \\text{ for } i = 1\\ldots N_0$$\n",

The kernel should have the function signature:
```python
def simple_fa_kernel(q_ptr, k_ptr, v_ptr, z_ptr, N0, N1, T, B0: tl.constexpr, B1: tl.constexpr):
```
"""
reference_kernel = """
```python
@triton.jit
def simple_fa_kernel(q_ptr, k_ptr, v_ptr, z_ptr, N0, T, B0: tl.constexpr):
    log2_e = 1.44269504

    # load q_block (B0,)
    # load k_block (B0,)
    # calculate outer product qk (B0, B0)
    # do max-sum

    for q_offset in tl.static_range(0, N0, B0):
        q_range = q_offset + tl.arange(0, B0)
        q_block = tl.load(q_ptr + q_range, mask=q_range < N0)

        _max = (tl.arange(0, B0) + 1) * (-1e10)
        _denom = tl.arange(0, B0) * 0
        z_block = tl.arange(0, B0) * 0

        for k_offset in tl.static_range(0, N0, B0):
            k_range = k_offset + tl.arange(0, B0)
            k_block = tl.load(k_ptr + k_range, mask=k_range < N0)

            qk_block = q_block[:, None] * k_block[None, :]

            # mask so that maximum is correct
            qk_block = qk_block * (k_range < N0) + (-1e10) * (k_range >= N0)

            new_max = tl.maximum(_max, tl.max(qk_block, axis=1))
            exp_correction = tl.exp2(_max - new_max)
            qk_block_exp = tl.exp2((qk_block - new_max[:, None]) * log2_e)

            new_denom = _denom * exp_correction + tl.sum(qk_block_exp, axis=1)
            qk_block_prob = qk_block_exp / new_denom[:, None]

            v_block = tl.load(v_ptr + k_range, mask=k_range < N0)
            z_block *= exp_correction * _denom / new_denom
            z_block += tl.sum(qk_block_prob * v_block[None, :], axis=1)

            _max = new_max
            _denom = new_denom

        tl.store(z_ptr + q_range, z_block, mask=q_range < N0)

    return
```
"""
