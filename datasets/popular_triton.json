[
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps=8)], key=['n_elements'],\n    restore_value=['p_ptr', 'exp_avg_ptr'])\n@triton.jit\ndef update_fn_kernel(p_ptr, grad_ptr, exp_avg_ptr, lr, wd, beta1, beta2,\n    n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n    p = p * (1 - lr * wd)\n    diff = exp_avg - grad\n    update = diff * beta1 + grad\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n    p = p + update_sign * can_update\n    exp_avg = diff * beta2 + grad\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef sumN(output_ptr, scaling_ptr, *inputs, BLOCK_SIZE: tl.constexpr):\n    offset = tl.arange(0, BLOCK_SIZE)\n    output = tl.zeros([BLOCK_SIZE], tl.float32)\n    scaling: 'VAR_ARGS_ARRAY'\n    for i in range(len(scaling)):\n        scaling[i] = tl.load(scaling_ptr + i)\n    for i in range(2):\n        for j in range(len(inputs)):\n            output = output + tl.load(inputs[j] + offset) * scaling[j]\n    tl.store(output_ptr + offset, output)\n"
    },
    {
      "input": "@triton.jit\ndef weighted_sumN(output_ptr, a_ptr: 'VAR_ARGS_ARRAY', b: 'VAR_ARGS_ARRAY',\n    BLOCK_SIZE: tl.constexpr):\n    offset = tl.arange(0, BLOCK_SIZE)\n    output = tl.zeros([BLOCK_SIZE], tl.float32)\n    for i in range(len(a_ptr)):\n        output = output + tl.load(a_ptr[i] + offset) * b[i]\n    tl.store(output_ptr + offset, output)\n"
    },
    {
      "input": "@triton.jit\ndef scaled_index_add_fwd_kernel(input_ptr, index_ptr, source_ptr,\n    scaling_ptr, alpha, num_inp_indices, num_src_indices, num_rows,\n    num_cols, stride0, stride1, stride2, BLOCK_SIZE_INDEX: tl.constexpr,\n    BLOCK_SIZE_ROW: tl.constexpr, BLOCK_SIZE_COL: tl.constexpr, HAS_SCALING:\n    tl.constexpr):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask).to(tl.float32)\n    input_indices = tl.load(index_ptr + source_indices, mask=source_indices <\n        num_src_indices)\n    input_offsets = input_ptr + input_indices[:, None, None] * stride0 + rows[\n        None, :, None] * stride1 + cols[None, None, :] * stride2\n    x = tl.load(input_offsets, mask=source_mask).to(tl.float32)\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols).to(tl.float32)\n        tl.store(input_offsets, x + alpha * scaling * source, mask=source_mask)\n    else:\n        tl.store(input_offsets, x + alpha * source, mask=source_mask)\n"
    },
    {
      "input": "@triton.jit\ndef scaled_index_add_bwd_kernel(grad_output_ptr, grad_source_ptr,\n    grad_scaling_ptr, source_ptr, scaling_ptr, index_ptr, alpha,\n    num_inp_indices, num_src_indices, num_rows, num_cols, stride0, stride1,\n    stride2, BLOCK_SIZE_INDEX: tl.constexpr, BLOCK_SIZE_ROW: tl.constexpr,\n    BLOCK_SIZE_COL: tl.constexpr, HAS_SCALING: tl.constexpr):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask).to(tl.float32)\n    grad_output_indices = tl.load(index_ptr + source_indices, mask=\n        source_indices < num_src_indices)\n    grad_output_offsets = (grad_output_ptr + grad_output_indices * stride0 +\n        rows[None, :, None] * stride1 + cols[None, None, :] * stride2)\n    grad_output = tl.load(grad_output_offsets, mask=source_mask).to(tl.float32)\n    grad_source_offsets = grad_source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols).to(tl.float32)\n        tl.store(grad_source_offsets, alpha * grad_output * scaling, mask=\n            source_mask)\n        grad_scaling_offsets = grad_scaling_ptr + source_indices[:, None, None\n            ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n            ] * stride2\n        tl.store(grad_scaling_offsets, alpha * grad_output * source, mask=\n            source_mask)\n    else:\n        tl.store(grad_source_offsets, alpha * grad_output, mask=source_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rope_padded_kernel(xq, xk, xv, out_q, cache_k, cache_v, seqstartq,\n    seqstartk, seqlenk, theta, linear_scale, use_dynamic_scaling: tl.\n    constexpr, dynamic_old_context_len: tl.constexpr, dynamic_scale_factor:\n    tl.constexpr, dynamic_low_freq_factor: tl.constexpr,\n    dynamic_high_freq_factor: tl.constexpr, first_seqpos, seqpos, k_start:\n    tl.constexpr, v_start: tl.constexpr, n_groups, dim: tl.constexpr,\n    stride_xqM, stride_xqG, stride_xqH, stride_xkM, stride_xkG, stride_xkH,\n    stride_xvM, stride_xvG, stride_xvH, stride_cachekM, stride_cachekG,\n    stride_cachekH, stride_cachevM, stride_cachevG, stride_cachevH,\n    stride_seqstartq, stride_seqstartk, stride_seqlenk, stride_outqM,\n    stride_outqG, stride_outqH, stride_seqpos, internal_dtype: tl.constexpr,\n    const_batch_strides: tl.constexpr, cache_padding_length, seqlenk_shift:\n    tl.constexpr, BLOCK_SIZE: tl.constexpr, adjacents: tl.constexpr):\n    \"\"\"\n    Each letter in this diagram is a whole row of length dim.\n\n     INPUT      xq        xk       xv\n\n        head_dim \u2500\u25ba\n\n      batch   qqqqqq      kk       vv\n        \u2502     qqqqqq      kk       vv\n        \u25bc     qqqqqq      kk       vv\n\n    head_idx:  (goes across all heads of all 3 inputs)\n              \u25b2     \u25b2     \u25b2 \u25b2      \u25b2 \u25b2\n              \u2502     \u2502     \u2502 \u2502      \u2502 \u2502\n                          \u2502        \u2502\n              0  k_start  \u2502v_start \u2502n_total_heads\n                          \u2502        \u2502\n                          \u2502        \u2502\n                      k_start    v_start\n\n    Output is to out_q (same shape as xq), an xk-shaped part\n    of cache_k and an xv-shaped part of cache_v\n    \"\"\"\n    query_pos_in_batch_elt = tl.program_id(0)\n    batch_elt = tl.program_id(1)\n    group_head_idx = tl.program_id(2)\n    group_idx = group_head_idx % n_groups\n    head_idx = group_head_idx // n_groups\n    if internal_dtype == 'f32':\n        theta = theta.to(tl.float32)\n    elif internal_dtype == 'f64':\n        theta = theta.to(tl.float64)\n    if const_batch_strides:\n        query_pos = query_pos_in_batch_elt + tl.num_programs(1) * batch_elt\n        end_query_pos = tl.num_programs(1) * (batch_elt + 1)\n    else:\n        query_pos = query_pos_in_batch_elt + tl.load(seqstartq + batch_elt *\n            stride_seqstartq)\n        end_query_pos = tl.load(seqstartq + (batch_elt + 1) * stride_seqstartq)\n        if query_pos >= end_query_pos:\n            return\n    is_q = head_idx < k_start\n    is_v = head_idx >= v_start\n    xq += (query_pos * stride_xqM + head_idx * stride_xqH + group_idx *\n        stride_xqG)\n    out_q += (query_pos * stride_outqM + head_idx * stride_outqH + \n        group_idx * stride_outqG)\n    if const_batch_strides:\n        cache_start = cache_padding_length * batch_elt\n    else:\n        cache_start = tl.load(seqstartk + batch_elt * stride_seqstartk)\n    end_of_batch_elt_cache = cache_start + tl.load(seqlenk + batch_elt *\n        stride_seqlenk) + seqlenk_shift\n    cache_pos = end_of_batch_elt_cache - (end_query_pos - query_pos)\n    if seqpos is not None:\n        seq_pos = tl.load(seqpos + query_pos * stride_seqpos)\n    else:\n        seq_pos = cache_pos - cache_start\n        if first_seqpos is not None:\n            seq_pos += tl.load(first_seqpos + batch_elt * stride_seqpos)\n    cache_k += ((head_idx - k_start) * stride_cachekH + cache_pos *\n        stride_cachekM + group_idx * stride_cachekG)\n    xk += query_pos * stride_xkM + (head_idx - k_start\n        ) * stride_xkH + group_idx * stride_xkG\n    in_qk = tl.where(is_q, xq, xk)\n    out_qk = tl.where(is_q, out_q, cache_k)\n    cache_v += ((head_idx - v_start) * stride_cachevH + cache_pos *\n        stride_cachevM + group_idx * stride_cachevG)\n    xv += query_pos * stride_xvM + (head_idx - v_start\n        ) * stride_xvH + group_idx * stride_xvG\n    out = tl.where(is_v, cache_v, out_qk)\n    x_in = tl.where(is_v, xv, in_qk)\n    for offset in range(0, dim // 2, BLOCK_SIZE // 2):\n        c = tl.arange(0, BLOCK_SIZE // 2)\n        powers = (offset + c) * 2.0\n        if adjacents:\n            cols_re = (offset + c) * 2\n            cols_im = cols_re + 1\n        else:\n            cols_re = offset + c\n            cols_im = cols_re + dim // 2\n        mask = cols_im < dim\n        re_x = tl.load(x_in + cols_re, mask=mask)\n        im_x = tl.load(x_in + cols_im, mask=mask)\n        freqs = pow(theta, powers / -dim)\n        if use_dynamic_scaling:\n            lo_freq_wavelen = dynamic_old_context_len / dynamic_low_freq_factor\n            hi_freq_wavelen = (dynamic_old_context_len /\n                dynamic_high_freq_factor)\n            wavelens = 6.28318530718 / freqs\n            is_low_freq = wavelens > lo_freq_wavelen\n            freqs = tl.where(is_low_freq, freqs / dynamic_scale_factor, freqs)\n            is_mid_freq = (hi_freq_wavelen < wavelens and wavelens <=\n                lo_freq_wavelen)\n            smooth = (dynamic_old_context_len / wavelens -\n                dynamic_low_freq_factor) / (dynamic_high_freq_factor -\n                dynamic_low_freq_factor)\n            freqs = tl.where(is_mid_freq, (1 - smooth) * freqs /\n                dynamic_scale_factor + smooth * freqs, freqs)\n        freqs = seq_pos * freqs / linear_scale\n        sines = tl.sin(freqs)\n        cosines = tl.cos(freqs)\n        re_out = re_x * cosines - im_x * sines\n        im_out = im_x * cosines + re_x * sines\n        re_out_ = tl.where(is_v, re_x, re_out)\n        im_out_ = tl.where(is_v, im_x, im_out)\n        if internal_dtype == 'f64':\n            if re_x.dtype == tl.bfloat16:\n                re_out_ = re_out_.to(tl.float32)\n                im_out_ = im_out_.to(tl.float32)\n        tl.store(out + cols_re, re_out_, mask=mask)\n        tl.store(out + cols_im, im_out_, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef index_select_cat_fwd_kernel(output_ptr, source_ptr, index_ptr,\n    num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX: tl.constexpr,\n    BLOCK_SIZE_COL: tl.constexpr):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    rows = tl.load(index_ptr + indices, mask=indices < num_indices)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_offsets = source_ptr + rows[:, None] * stride0 + cols[None, :\n        ] * stride1\n    mask = (indices[:, None] < num_indices) & (cols[None, :] < num_cols)\n    output = tl.load(source_offsets, mask=mask)\n    output_offsets = output_ptr + indices[:, None] * stride0 + cols[None, :\n        ] * stride1\n    tl.store(output_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef index_select_cat_bwd_kernel(grad_source_ptr, index_ptr, grad_output_ptr,\n    num_rows, num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX: tl\n    .constexpr, BLOCK_SIZE_COL: tl.constexpr):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    grad_output_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0,\n        BLOCK_SIZE_INDEX)\n    grad_output_offsets = grad_output_ptr + grad_output_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    grad_output_mask = (grad_output_indices[:, None] < num_indices) & (cols\n        [None, :] < num_cols)\n    grad_output = tl.load(grad_output_offsets, mask=grad_output_mask).to(tl\n        .float32)\n    grad_source_indices = tl.load(index_ptr + grad_output_indices, mask=\n        grad_output_indices < num_indices)\n    grad_source_offsets = grad_source_ptr + grad_source_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    tl.store(grad_source_offsets, grad_output, mask=grad_output_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_kernel(x_ptr, h1_ptr, w_ptr, eps, stride, N_COLS: tl.\n    constexpr, BLOCK_SIZE: tl.constexpr, INCLUDE_WEIGHT: tl.constexpr):\n    row = tl.program_id(0).to(tl.int64)\n    x_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(x_ptr + cols, mask=cols < N_COLS, other=0.0,\n            eviction_policy='evict_last').to(tl.float32)\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first').to(tl.float32)\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_add_kernel(x_ptr, y_ptr, h1_ptr, w_ptr, eps, stride, N_COLS:\n    tl.constexpr, BLOCK_SIZE: tl.constexpr, INCLUDE_WEIGHT: tl.constexpr):\n    row = tl.program_id(0)\n    x_ptr += row * stride\n    y_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        ax = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_last').to(tl.float32)\n        ay = tl.load(y_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first').to(tl.float32)\n        a = ax + ay\n        tl.store(x_ptr + cols, a, mask=mask)\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first').to(tl.float32)\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_splitK(Q, K, V, sm_scale, Out_splitK, LSE_splitk,\n    block_tables, Seq_len, Seq_starts_k, Seq_starts_q,\n    Seq_starts_q_multiplier, additive_bias, K_fp8_scale_shift,\n    V_fp8_scale_shift, stride_qz, stride_qm, stride_qg, stride_qh,\n    stride_qk, stride_kz, stride_kn, stride_kg, stride_kh, stride_kk,\n    stride_vz, stride_vn, stride_vg, stride_vh, stride_vk, stride_osk_z,\n    stride_osk_g, stride_osk_h, stride_osk_s, stride_osk_m, stride_osk_k,\n    stride_lsek_z, stride_lsek_g, stride_lsek_h, stride_lsek_s,\n    stride_lsek_m, stride_blocktablesz, stride_blocktablesl, stride_bias_b,\n    stride_bias_g, stride_bias_h, stride_bias_qm, stride_bias_km,\n    stride_k_fp8_scale_shift_z: tl.constexpr, stride_k_fp8_scale_shift_n:\n    tl.constexpr, stride_k_fp8_scale_shift_g: tl.constexpr,\n    stride_k_fp8_scale_shift_h: tl.constexpr, stride_v_fp8_scale_shift_z:\n    tl.constexpr, stride_v_fp8_scale_shift_n: tl.constexpr,\n    stride_v_fp8_scale_shift_g: tl.constexpr, stride_v_fp8_scale_shift_h:\n    tl.constexpr, kv_cache_blocks_per_row: tl.constexpr, Z: tl.constexpr,\n    N_CTX_Q: tl.constexpr, N_CTX_K: tl.constexpr, BLOCK_N_PER_SPLIT: tl.\n    constexpr, H: tl.constexpr, G: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    USE_SEQ_LEN: tl.constexpr, PACKED_PER_VAL: tl.constexpr, N_GROUPS: tl.\n    constexpr, BOUNDS_CHECKS_N: tl.constexpr, BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr, IS_SPLITK: tl.constexpr, SPLIT_K_EARLY_EXIT: tl.\n    constexpr, IS_CAUSAL: tl.constexpr, NUM_QUERIES_CAUSAL: tl.constexpr,\n    USE_PAGED_ATTENTION: tl.constexpr, PAGE_SIZE: tl.constexpr, WRITE_LSE:\n    tl.constexpr, HAS_ADDITIVE_BIAS: tl.constexpr):\n    \"\"\"This kernel can accept non-quantized or int4-quantized keys/values.\n    PACKED_PER_VAL determines the quantization type:\n        - PACKED_PER_VAL == 1 means no quantization\n        - PACKED_PER_VAL == 8 means 4-bit quantization (8 packed quantized values inside one int32)\n    For the quantized case K/V should be int32 tensors.\n    Quantization can be row-wise (when N_GROUPS = 1) or group-wise with N_GROUPS = 2, 4, or 8.\n    Quantization coefficients are stored at the beginning of the row along the last dimension of K/V\n    So K[B, H, M, :] has a form\n    [   quant_coef0, quant_coef1, ...|\n        group0_quant_value0, group0_quant_value1,... |\n        group1_quant_value0, group1_quant_value1,...]\n    where each quant_coef is an int32 which should be interpreted as 2 packed float16: scale and offset.\n\n    Note: this kernel needs to be processed by xformers.triton.vararg_kernel.unroll_varargs\n    before compilation. That will unroll variables marked with \"VAR_ARGS_ARRAY\" into lists.\n    See how FwOp.apply does it below.\n\n    Set IS_SPLITK=False to indicate the MHA result should be written directly.\n    No metadata will be written.\n    \"\"\"\n    internal_dtype = (tl.float64 if Out_splitK.dtype.element_ty is tl.\n        float64 else tl.float32)\n    tl.static_assert(PACKED_PER_VAL == 1 and tl.constexpr(K.dtype.\n        element_ty != tl.int32) or (PACKED_PER_VAL == 4 or PACKED_PER_VAL ==\n        8) and tl.constexpr(K.dtype.element_ty == tl.int32),\n        f'Only int4 and fp8 quantization is supported, K/V should have dtype int32 in the quantized case: PACKED_PER_VAL={PACKED_PER_VAL!r} tl.constexpr(K.dtype)={tl.constexpr(K.dtype)!r} tl.constexpr(K.dtype.element_ty)={tl.constexpr(K.dtype.element_ty)!r}'\n        )\n    tl.static_assert(((N_GROUPS == 1 or N_GROUPS == 2) or N_GROUPS == 4) or\n        N_GROUPS == 8,\n        'Number of quantization groups can be 1 (row-wise quantization), 2, 4, or 8.'\n        )\n    tl.static_assert(N_GROUPS == 1 or K_fp8_scale_shift is None,\n        f'Only row-wise fp8 quantization is supported, but got N_GROUPS={N_GROUPS!r} > 1.'\n        )\n    FP8_QUANTIZED: tl.constexpr = K_fp8_scale_shift is not None\n    INT4_QUANTIZED: tl.constexpr = PACKED_PER_VAL > 1 and not FP8_QUANTIZED\n    PACKED_D_PER_GROUP: tl.constexpr = (BLOCK_DMODEL // PACKED_PER_VAL //\n        N_GROUPS)\n    D_PER_GROUP: tl.constexpr = BLOCK_DMODEL // N_GROUPS\n    start_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_hg = off_zhg % (H * G)\n    off_h = off_hg // G\n    off_g = off_hg % G\n    splitk_idx = tl.program_id(2)\n    if USE_SEQ_LEN:\n        kv_len = tl.load(Seq_len + off_z)\n        if SPLIT_K_EARLY_EXIT and kv_len == 0:\n            return\n    else:\n        kv_len = N_CTX_K\n    if Seq_starts_k is None:\n        start_kv_idx = 0\n    else:\n        start_kv_idx = tl.load(Seq_starts_k + off_z)\n    if Seq_starts_q is None:\n        q_len = N_CTX_Q\n        queries_use_batch_dim = 1\n        off_m = 0\n    else:\n        queries_use_batch_dim = 0\n        off_m = tl.load(Seq_starts_q + off_z) * Seq_starts_q_multiplier\n        q_len = tl.load(Seq_starts_q + off_z + 1\n            ) * Seq_starts_q_multiplier - off_m\n        if q_len == 0:\n            return\n    k_base = K + off_h * stride_kh + off_g * stride_kg\n    v_base = V + off_h * stride_vh + off_g * stride_vg\n    if FP8_QUANTIZED:\n        k_fp8_scale_shift_base = (K_fp8_scale_shift + off_h *\n            stride_k_fp8_scale_shift_h + off_g * stride_k_fp8_scale_shift_g)\n        v_fp8_scale_shift_base = (V_fp8_scale_shift + off_h *\n            stride_v_fp8_scale_shift_h + off_g * stride_v_fp8_scale_shift_g)\n    else:\n        k_fp8_scale_shift_base = None\n        v_fp8_scale_shift_base = None\n    chunk_hi = (splitk_idx + 1) * BLOCK_N_PER_SPLIT\n    chunk_lo = splitk_idx * BLOCK_N_PER_SPLIT\n    ignore_in_first_block = 0\n    if PAGE_SIZE > 0:\n        BLOCKS_IN_PAGE: tl.constexpr = PAGE_SIZE // BLOCK_N\n        is_last_chunk = splitk_idx == tl.num_programs(2) - 1\n        shift = BLOCK_N - 1 if is_last_chunk else 0\n        lo = tl.maximum(chunk_lo, start_kv_idx) // BLOCK_N * BLOCK_N\n        ignore_in_first_block = tl.maximum(0, start_kv_idx - lo)\n        hi = (chunk_hi + shift) // BLOCK_N * BLOCK_N\n        hi = tl.minimum(hi, kv_len + start_kv_idx)\n        block_table = block_tables + stride_blocktablesz * off_z\n        logical_block_idx = lo // BLOCK_N\n    else:\n        lo = chunk_lo\n        hi = tl.minimum(chunk_hi, kv_len)\n        if Seq_starts_k is not None:\n            k_base += start_kv_idx * stride_kn\n            v_base += start_kv_idx * stride_vn\n        else:\n            k_base += off_z * stride_kz\n            v_base += off_z * stride_vz\n        K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n            INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, hi),\n            strides=(stride_kk, stride_kn), offsets=(0, lo), block_shape=(\n            PACKED_D_PER_GROUP, BLOCK_N), order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n            INT4_QUANTIZED * N_GROUPS, shape=(hi, PACKED_D_PER_GROUP),\n            strides=(stride_vn, stride_vk), offsets=(lo, 0), block_shape=(\n            BLOCK_N, PACKED_D_PER_GROUP), order=(1, 0))\n        if INT4_QUANTIZED:\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base, shape=\n                (1, hi), strides=(stride_kk, stride_kn), offsets=(0, lo),\n                block_shape=(1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base, shape=\n                (hi, 1), strides=(stride_vn, stride_vk), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        elif FP8_QUANTIZED:\n            if Seq_starts_k is not None:\n                k_fp8_scale_shift_base += (start_kv_idx *\n                    stride_k_fp8_scale_shift_n)\n                v_fp8_scale_shift_base += (start_kv_idx *\n                    stride_v_fp8_scale_shift_n)\n            else:\n                k_fp8_scale_shift_base += off_z * stride_k_fp8_scale_shift_z\n                v_fp8_scale_shift_base += off_z * stride_v_fp8_scale_shift_z\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                k_fp8_scale_shift_base, shape=(1, hi), strides=(1,\n                stride_k_fp8_scale_shift_n), offsets=(0, lo), block_shape=(\n                1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                v_fp8_scale_shift_base, shape=(hi, 1), strides=(\n                stride_v_fp8_scale_shift_n, 1), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        else:\n            K_scale_shift_block_ptr = None\n            V_scale_shift_block_ptr = None\n        if HAS_ADDITIVE_BIAS:\n            additive_bias_block_ptr = tl.make_block_ptr(base=additive_bias +\n                off_z * stride_bias_b + off_g * stride_bias_g + off_h *\n                stride_bias_h, shape=(N_CTX_Q, hi), strides=(stride_bias_qm,\n                stride_bias_km), offsets=(start_m * BLOCK_M, lo),\n                block_shape=(BLOCK_M, BLOCK_N), order=(0, 1))\n    if SPLIT_K_EARLY_EXIT and lo >= hi:\n        return\n    Q_block_ptr = tl.make_block_ptr(base=Q + off_m * stride_qm + off_h *\n        stride_qh + off_z * stride_qz * queries_use_batch_dim + off_g *\n        stride_qg, shape=(q_len, BLOCK_DMODEL), strides=(stride_qm,\n        stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M,\n        D_PER_GROUP), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc: 'VAR_ARGS_ARRAY'\n    for i in range(len(acc)):\n        acc[i] = tl.zeros([BLOCK_M, D_PER_GROUP], dtype=internal_dtype)\n    qk_scale = sm_scale * 1.44269504\n    q: 'VAR_ARGS_ARRAY'\n    for i in range(len(acc)):\n        q[i] = tl.load(tl.advance(Q_block_ptr, (0, i * D_PER_GROUP)),\n            boundary_check=(0,))\n    if IS_CAUSAL:\n        q_offset = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        diag_idx = q_offset[:, None] % NUM_QUERIES_CAUSAL - tl.arange(0,\n            BLOCK_N)[None, :]\n        diag_idx_shifted = tl.constexpr(diag_idx - NUM_QUERIES_CAUSAL + kv_len)\n    for start_n in range(lo, hi, BLOCK_N):\n        if PAGE_SIZE > 0:\n            block_offset_in_page = logical_block_idx % BLOCKS_IN_PAGE\n            logical_page_idx = logical_block_idx // BLOCKS_IN_PAGE\n            physical_page_idx = tl.load(block_table + stride_blocktablesl *\n                logical_page_idx).to(tl.int32)\n            offset = (physical_page_idx * PAGE_SIZE + block_offset_in_page *\n                BLOCK_N)\n            current_block_size = min(hi - start_n, BLOCK_N)\n            K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n                INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, \n                offset + current_block_size), strides=(stride_kk, stride_kn\n                ), offsets=(0, offset), block_shape=(PACKED_D_PER_GROUP,\n                BLOCK_N), order=(0, 1))\n            V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n                INT4_QUANTIZED * N_GROUPS, shape=(offset +\n                current_block_size, PACKED_D_PER_GROUP), strides=(stride_vn,\n                stride_vk), offsets=(offset, 0), block_shape=(BLOCK_N,\n                PACKED_D_PER_GROUP), order=(1, 0))\n            if INT4_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base,\n                    shape=(1, offset + current_block_size), strides=(\n                    stride_kk, stride_kn), offsets=(0, offset), block_shape\n                    =(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base,\n                    shape=(offset + current_block_size, 1), strides=(\n                    stride_vn, stride_vk), offsets=(offset, 0), block_shape\n                    =(BLOCK_N, 1), order=(1, 0))\n            elif FP8_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    k_fp8_scale_shift_base, shape=(1, offset +\n                    current_block_size), strides=(1,\n                    stride_k_fp8_scale_shift_n), offsets=(0, offset),\n                    block_shape=(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    v_fp8_scale_shift_base, shape=(offset +\n                    current_block_size, 1), strides=(\n                    stride_v_fp8_scale_shift_n, 1), offsets=(offset, 0),\n                    block_shape=(BLOCK_N, 1), order=(1, 0))\n            else:\n                K_scale_shift_block_ptr = None\n                V_scale_shift_block_ptr = None\n            logical_block_idx += 1\n        k: 'VAR_ARGS_ARRAY'\n        v: 'VAR_ARGS_ARRAY'\n        for i in range(len(acc)):\n            k[i], v[i] = load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n                K_scale_shift_block_ptr, V_scale_shift_block_ptr,\n                BOUNDS_CHECKS_N, PACKED_PER_VAL, PACKED_D_PER_GROUP,\n                FP8_QUANTIZED, Q.dtype.element_ty, i)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for i in range(len(acc)):\n            qk += tl.dot(q[i], k[i])\n        qk *= qk_scale\n        if start_n == lo and ignore_in_first_block > 0:\n            qk = tl.where(tl.arange(0, BLOCK_N) < ignore_in_first_block,\n                float('-inf'), qk)\n        if HAS_ADDITIVE_BIAS:\n            loaded_bias = tl.load(additive_bias_block_ptr, boundary_check=(\n                0, 1) if BOUNDS_CHECKS_N else (0,))\n            qk += loaded_bias * 1.44269504\n            additive_bias_block_ptr = tl.advance(additive_bias_block_ptr, (\n                0, BLOCK_N))\n        if BOUNDS_CHECKS_N:\n            qk = tl.where(tl.arange(0, BLOCK_N) < hi - start_n, qk, float(\n                '-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(diag_idx_shifted >= start_n, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        if HAS_ADDITIVE_BIAS or IS_CAUSAL:\n            alpha = tl.where(m_i_new == float('-inf'), 0, alpha)\n            p = tl.where(m_i_new[:, None] == float('-inf'), 0, p)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        p = p.to(Q.dtype.element_ty)\n        for i in range(len(acc)):\n            acc[i] *= alpha[:, None]\n            acc[i] += tl.dot(p, v[i])\n        if not PAGE_SIZE:\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n            if PACKED_PER_VAL > 1:\n                K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr,\n                    (0, BLOCK_N))\n                V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr,\n                    (BLOCK_N, 0))\n    O_block_ptr = tl.make_block_ptr(base=Out_splitK + off_z.to(tl.int64) *\n        stride_osk_z * queries_use_batch_dim + off_m * stride_osk_m + off_g *\n        stride_osk_g + off_h * stride_osk_h + splitk_idx * stride_osk_s,\n        shape=(q_len, D_PER_GROUP), strides=(stride_osk_m, 1), offsets=(\n        start_m * BLOCK_M, 0), block_shape=(BLOCK_M, D_PER_GROUP), order=(1, 0)\n        )\n    for i in range(len(acc)):\n        attn_out = tl.where(l_i[:, None] == 0, 0.0, acc[i] / l_i[:, None])\n        tl.store(tl.advance(O_block_ptr, (0, i * D_PER_GROUP)), attn_out.to\n            (Out_splitK.dtype.element_ty), boundary_check=(0,))\n    if WRITE_LSE:\n        LSE_splitk_ptr = (LSE_splitk + off_z * stride_lsek_z *\n            queries_use_batch_dim + off_m * stride_lsek_m + off_g *\n            stride_lsek_g + off_h * stride_lsek_h + splitk_idx *\n            stride_lsek_s + (start_m * BLOCK_M + tl.arange(0, BLOCK_M)) *\n            stride_lsek_m)\n        mask = start_m * BLOCK_M + tl.arange(0, BLOCK_M) < q_len\n        lse_dtype = LSE_splitk.dtype.element_ty\n        tl.store(LSE_splitk_ptr, (tl.math.log2(l_i.to(lse_dtype)) + m_i.to(\n            lse_dtype)) / 1.44269504, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n    K_scale_shift_block_ptr, V_scale_shift_block_ptr, BOUNDS_CHECKS_N: tl.\n    constexpr, PACKED_PER_VAL: tl.constexpr, PACKED_D_PER_GROUP: tl.\n    constexpr, FP8_QUANTIZED: tl.constexpr, dtype: tl.constexpr, group_id:\n    tl.constexpr):\n    \"\"\"Load K/V for a given block. In case of int4/fp8-quantized K/V, dequantize them after loading.\n    If quantization is group-wise, use group_id to advance the pointers to the current group.\n    \"\"\"\n    K_block_ptr = tl.advance(K_block_ptr, (PACKED_D_PER_GROUP * group_id, 0))\n    V_block_ptr = tl.advance(V_block_ptr, (0, PACKED_D_PER_GROUP * group_id))\n    k = tl.load(K_block_ptr, boundary_check=(1,) if BOUNDS_CHECKS_N else ())\n    v = tl.load(V_block_ptr, boundary_check=(0,) if BOUNDS_CHECKS_N else ())\n    if FP8_QUANTIZED:\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL).to(dtype)\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL).to(dtype)\n        k = tl.trans(k_t)\n    elif PACKED_PER_VAL > 1:\n        K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr, (\n            group_id, 0))\n        V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr, (0,\n            group_id))\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL).to(dtype)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL).to(dtype)\n        k = tl.trans(k_t)\n    return k, v\n"
    },
    {
      "input": "@triton.jit\ndef cast_uint32_to_half2(scale_shift):\n    \"\"\"Extract two float16 packed into one int32\"\"\"\n    scale = scale_shift & 65535\n    shift = scale_shift >> 16\n    scale = scale.to(tl.uint16).to(tl.float16, bitcast=True)\n    shift = shift.to(tl.uint16).to(tl.float16, bitcast=True)\n    return scale, shift\n"
    },
    {
      "input": "@triton.jit\ndef dequantize(x_, scale, shift, PACKED_PER_VAL: tl.constexpr):\n    \"\"\"PACKED_PER_VAL is the number of values packed into each element x_.\n    For example, for int4 quantization and x_ of type int32, PACKED_PER_VAL is 8.\n    \"\"\"\n    BLOCK_N: tl.constexpr = x_.shape[0]\n    BLOCK_DMODEL_PACKED: tl.constexpr = x_.shape[1]\n    offsets = tl.arange(0, PACKED_PER_VAL) * (32 // PACKED_PER_VAL)\n    quant_offset = x_[:, :, None, :] >> offsets\n    quant_offset = tl.reshape(quant_offset, (BLOCK_N, BLOCK_DMODEL_PACKED *\n        PACKED_PER_VAL))\n    if PACKED_PER_VAL == 4:\n        fp8_type = (tl.float8e4b8 if torch.version.hip is not None else tl.\n            float8e4nv)\n        dequant = quant_offset.to(tl.uint8).to(fp8_type, bitcast=True).to(scale\n            .dtype) * scale + shift\n    else:\n        quant_offset = (quant_offset & 15).to(tl.uint16).to(tl.float16,\n            bitcast=True)\n        quant_offset = (quant_offset * 32768.0).to(tl.float16)\n        scale_512 = scale * 512\n        dequant = quant_offset * scale_512 + shift\n    return dequant\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce(Out_splitK, LSE_splitK, Out, LSE, split_k: tl.constexpr,\n    splitK_pow2: tl.constexpr, stride_osk_z: tl.constexpr, stride_osk_g: tl\n    .constexpr, stride_osk_h: tl.constexpr, stride_osk_s: tl.constexpr,\n    stride_osk_m: tl.constexpr, stride_osk_k: tl.constexpr, stride_lsek_z:\n    tl.constexpr, stride_lsek_g: tl.constexpr, stride_lsek_h: tl.constexpr,\n    stride_lsek_s: tl.constexpr, stride_lsek_m: tl.constexpr, stride_oz: tl\n    .constexpr, stride_og: tl.constexpr, stride_oh: tl.constexpr, stride_om:\n    tl.constexpr, stride_ok: tl.constexpr, stride_lse_z: tl.constexpr,\n    stride_lse_g: tl.constexpr, stride_lse_h: tl.constexpr, stride_lse_m:\n    tl.constexpr, BLOCK_SIZE: tl.constexpr, H: tl.constexpr, G: tl.\n    constexpr, WRITE_LSE: tl.constexpr):\n    off_m = tl.program_id(0).to(tl.int64)\n    off_zhg = tl.program_id(1).to(tl.int64)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    Out_splitK_ptr = (Out_splitK + stride_osk_z * off_z + stride_osk_g *\n        off_g + stride_osk_h * off_h + stride_osk_m * off_m + tl.arange(0,\n        BLOCK_SIZE)[None, :] + stride_osk_s * tl.arange(0, splitK_pow2)[:,\n        None])\n    LSE_splitK_ptr0 = (LSE_splitK + stride_lsek_z * off_z + stride_lsek_g *\n        off_g + stride_lsek_h * off_h + stride_lsek_m * off_m + \n        stride_lsek_s * tl.arange(0, splitK_pow2))\n    if splitK_pow2 > split_k:\n        mask_1d = tl.arange(0, splitK_pow2) < split_k\n        mask_2d = mask_1d[:, None]\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr, mask=mask_2d, other=0)\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n    else:\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr)\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n    sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max).to(tl.\n        float32) * 1.44269504)\n    sumexp_normalized = tl.sum(sumexp_normalized_splitk, axis=0)\n    numerator_normalized = tl.sum(out_splitk * sumexp_normalized_splitk[:,\n        None], axis=0)\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc.to(tl.float32)\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce_varargs(Out_splitK: 'VAR_ARGS_ARRAY', LSE_splitK:\n    'VAR_ARGS_ARRAY', Out, LSE, stride_osk_z: 'VAR_ARGS_ARRAY',\n    stride_osk_g: 'VAR_ARGS_ARRAY', stride_osk_h: 'VAR_ARGS_ARRAY',\n    stride_osk_m: 'VAR_ARGS_ARRAY', stride_osk_k: 'VAR_ARGS_ARRAY',\n    stride_lsek_z: 'VAR_ARGS_ARRAY', stride_lsek_g: 'VAR_ARGS_ARRAY',\n    stride_lsek_h: 'VAR_ARGS_ARRAY', stride_lsek_m: 'VAR_ARGS_ARRAY',\n    stride_oz, stride_og, stride_oh, stride_om, stride_ok, stride_lse_z,\n    stride_lse_g, stride_lse_h, stride_lse_m, BLOCK_SIZE: tl.constexpr, H:\n    tl.constexpr, G: tl.constexpr, WRITE_LSE: tl.constexpr):\n    \"\"\"\n    This version of reduce kernel takes attention and LSE of chunks as lists of tensors,\n    as opposed to _splitK_reduce, which takes each as a stacked tensor.\n    \"\"\"\n    off_m = tl.program_id(0).to(tl.int64)\n    off_zhg = tl.program_id(1).to(tl.int64)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: 'VAR_ARGS_ARRAY'\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: 'VAR_ARGS_ARRAY'\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    sumexp_normalized = 0.0\n    numerator_normalized = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max).to(\n            tl.float32) * 1.44269504)\n        sumexp_normalized += sumexp_normalized_splitk\n        numerator_normalized += out_splitk * sumexp_normalized_splitk\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc.to(tl.float32)\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce_varargs_backward(Out_splitK: 'VAR_ARGS_ARRAY',\n    LSE_splitK: 'VAR_ARGS_ARRAY', Dout_splitK: 'VAR_ARGS_ARRAY',\n    DLSE_splitK: 'VAR_ARGS_ARRAY', Out, LSE, DOut, DLSE, stride_osk_z:\n    'VAR_ARGS_ARRAY', stride_osk_g: 'VAR_ARGS_ARRAY', stride_osk_h:\n    'VAR_ARGS_ARRAY', stride_osk_m: 'VAR_ARGS_ARRAY', stride_osk_k:\n    'VAR_ARGS_ARRAY', stride_lsek_z: 'VAR_ARGS_ARRAY', stride_lsek_g:\n    'VAR_ARGS_ARRAY', stride_lsek_h: 'VAR_ARGS_ARRAY', stride_lsek_m:\n    'VAR_ARGS_ARRAY', stride_oz, stride_og, stride_oh, stride_om, stride_ok,\n    stride_lse_z, stride_lse_g, stride_lse_h, stride_lse_m, stride_doz,\n    stride_dog, stride_doh, stride_dom, stride_dok, stride_dlse_z,\n    stride_dlse_g, stride_dlse_h, stride_dlse_m, BLOCK_SIZE: tl.constexpr,\n    H: tl.constexpr, G: tl.constexpr):\n    \"\"\"\n    Backward for _splitK_reduce_varargs. Similar to forward, it takes\n    attention and LSE of chunks as lists of tensors,\n    and outputs the corresponding gradients in the same format.\n    \"\"\"\n    off_m = tl.program_id(0).to(tl.int64)\n    off_zhg = tl.program_id(1).to(tl.int64)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: 'VAR_ARGS_ARRAY'\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: 'VAR_ARGS_ARRAY'\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    offset_out = (stride_oz * off_z + stride_oh * off_h + stride_og * off_g +\n        stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    offset_dout = (stride_doz * off_z + stride_doh * off_h + stride_dog *\n        off_g + stride_dom * off_m + tl.arange(0, BLOCK_SIZE))\n    out = tl.load(Out + offset_out)\n    dattn = tl.load(DOut + offset_dout)\n    offset_lse = (stride_lse_z * off_z + stride_lse_h * off_h + \n        stride_lse_g * off_g + stride_lse_m * off_m)\n    offset_dlse = (stride_dlse_z * off_z + stride_dlse_h * off_h + \n        stride_dlse_g * off_g + stride_dlse_m * off_m)\n    lse = tl.load(LSE + offset_lse)\n    dlse = tl.load(DLSE + offset_dlse)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        dout_splitk_ptr = Dout_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx]\n        dlse_splitk_ptr = DLSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        dattn_dattn_i = tl.exp(lse_splitk - lse_max) / tl.exp(lse - lse_max)\n        dX_dattn_i = dattn_dattn_i * dattn\n        tl.store(dout_splitk_ptr, dX_dattn_i)\n        dattn_dlse_i = (out_splitk - out) * dattn_dattn_i\n        dlse_dlse_i = dattn_dattn_i\n        dX_dlse_i = dlse_dlse_i * dlse + tl.sum(dattn_dlse_i * dattn)\n        tl.store(dlse_splitk_ptr, dX_dlse_i)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'UNUSED': 1}, num_stages=\n    num_stages, num_warps=num_warps) for num_stages in (1, 2, 3, 4, 5) for\n    num_warps in (1, 2, 4, 8)], key=['in_features', 'out_features',\n    'num_codebooks', 'codebook_size', 'out_group_size', 'in_group_size',\n    'num_input_groups', 'num_input_groups_next_power_of_2',\n    'compute_in_fp32', 'has_output_scale', 'has_bias'])\n@triton.jit\ndef _aqlm_gemv_simple(input_vec_ptr, output_vec_ptr, codes_ptr,\n    codebooks_ptr, scales_ptr, bias_ptr, in_features: tl.constexpr,\n    out_features: tl.constexpr, num_codebooks: tl.constexpr, codebook_size:\n    tl.constexpr, out_group_size: tl.constexpr, in_group_size: tl.constexpr,\n    num_input_groups: tl.constexpr, num_input_groups_next_power_of_2: tl.\n    constexpr, compute_in_fp32: tl.constexpr, has_output_scale: tl.\n    constexpr, has_bias: tl.constexpr, UNUSED: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    input_vec = tl.load(input_vec_ptr + tl.arange(0,\n        num_input_groups_next_power_of_2)[:, None, None, None] *\n        in_group_size + tl.arange(0, in_group_size)[None, None, None, :],\n        mask=tl.arange(0, num_input_groups_next_power_of_2)[:, None, None,\n        None] < num_input_groups)\n    dtype = input_vec.dtype\n    codes_i_ptrs = (codes_ptr + pid * num_input_groups * num_codebooks + tl\n        .arange(0, num_input_groups_next_power_of_2)[:, None] *\n        num_codebooks + tl.arange(0, num_codebooks)[None, :])\n    codes_i_mask_1d = tl.arange(0, num_input_groups_next_power_of_2\n        ) < num_input_groups\n    codes_i = tl.load(codes_i_ptrs, mask=codes_i_mask_1d[:, None])\n    codes_i = codes_i.to(tl.int32)\n    codes_i = codes_i + (codes_i < 0) * codebook_size\n    codes_i += tl.arange(0, num_codebooks)[None, :] * codebook_size\n    out_group_ix = tl.arange(0, out_group_size)[None, None, :, None]\n    in_group_ix = tl.arange(0, in_group_size)[None, None, None, :]\n    weight_i_ptrs = (codebooks_ptr + codes_i[:, :, None, None] *\n        out_group_size * in_group_size + out_group_ix * in_group_size +\n        in_group_ix)\n    weights_i = tl.load(weight_i_ptrs, mask=codes_i_mask_1d[:, None, None,\n        None], other=0)\n    if compute_in_fp32:\n        weights_i = weights_i.to(tl.float32)\n        input_vec = input_vec.to(tl.float32)\n    output_i = weights_i * input_vec\n    if out_group_size == 1:\n        output_i = tl.sum(output_i)\n    else:\n        output_i = tl.sum(output_i, axis=1)\n        output_i = tl.sum(output_i, axis=2)\n        output_i = tl.sum(output_i, axis=0)\n    if has_output_scale:\n        output_i *= tl.load(scales_ptr + pid).to(weights_i.dtype)\n    if has_bias:\n        output_i += tl.load(bias_ptr + pid).to(weights_i.dtype)\n    if out_group_size == 1:\n        tl.store(output_vec_ptr + pid, output_i.to(dtype))\n    else:\n        tl.store(output_vec_ptr + pid * out_group_size + tl.arange(0,\n            out_group_size), output_i.to(dtype))\n"
    },
    {
      "input": "@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_count, block_offset, column_count, column_index, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, NUM_ROWS, NNZ_S, NNZ_V,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.\n    constexpr, dtype: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_blks = tl.load(block_count + off_hz * NUM_ROWS + start_m)\n    blks_ptr = block_offset + (off_hz * NUM_ROWS + start_m) * NNZ_S\n    num_cols = tl.load(column_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = column_index + (off_hz * NUM_ROWS + start_m) * NNZ_V\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = (q * qk_scale).to(dtype)\n    m_mask = offs_m[:, None] < seqlen\n    for block_index in range(num_blks):\n        start_n = tl.load(blks_ptr + block_index)\n        cols = start_n + offs_n\n        n_mask = cols < seqlen\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(0, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=0)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc.to(dtype), mask=m_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=1, num_warps=4),\n    triton.Config({}, num_stages=1, num_warps=8), triton.Config({},\n    num_stages=2, num_warps=4), triton.Config({}, num_stages=2, num_warps=8\n    ), triton.Config({}, num_stages=3, num_warps=4), triton.Config({},\n    num_stages=3, num_warps=8), triton.Config({}, num_stages=4, num_warps=4\n    ), triton.Config({}, num_stages=4, num_warps=8), triton.Config({},\n    num_stages=5, num_warps=4), triton.Config({}, num_stages=5, num_warps=8\n    )], key=['N_CTX'])\n@triton.jit\ndef triton_sparse_fwd_kernel(Q, K, V, seqlens, sm_scale, col_count,\n    col_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_COLS_PRE_ROW, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, dtype: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_cols = tl.load(col_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = col_index + (off_hz * NUM_ROWS + start_m) * MAX_COLS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = (q * qk_scale).to(dtype)\n    m_mask = offs_m[:, None] < seqlen\n    split = tl.maximum(num_cols - BLOCK_N, 0) & ~(BLOCK_N - 1)\n    for start_n in range(0, split, BLOCK_N):\n        cols = tl.load(cols_ptr + start_n + offs_n)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(split, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=\n            N_CTX - 1)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    tl.store(o_ptrs, acc.to(dtype), mask=m_mask)\n"
    },
    {
      "input": "@triton.jit\ndef triton_dense_fwd_kernel(Q, K, V, seqlens, sm_scale, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, dtype: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vn, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(dtype)\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M\n    m_mask = offs_m[:, None] < seqlen\n    for start_n in range(lo, hi, BLOCK_N):\n        n_mask = start_n + offs_n[None, :] <= offs_m[:, None]\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    O_block_ptr = tl.make_block_ptr(base=Out + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_ok), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc.to(dtype), mask=m_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, N_CTX, sliding_window_offset, sliding_window_size, BLOCK_M:\n    tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr,\n    SLIDING_WINDOW: tl.constexpr, IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.\n    constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr):\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = ((start_m + 1) * BLOCK_M + sliding_window_offset -\n                sliding_window_size + BLOCK_N - 1) // BLOCK_N * BLOCK_N\n        else:\n            lo = (start_m * BLOCK_M + sliding_window_offset -\n                sliding_window_size + 1) // BLOCK_N * BLOCK_N\n            hi = ((start_m + 1) * BLOCK_M - 1 + sliding_window_offset + BLOCK_N\n                ) // BLOCK_N * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m * BLOCK_M - start_n + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n            qk = tl.where(mask, qk, float('-inf'))\n        if not IS_EVEN_N:\n            qk = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :\n                ], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :],\n                p, 0)\n        l_ij = tl.sum(p, 1)\n        tmp = m_i - m_ij\n        alpha_mask = tmp != tmp\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1.0, alpha)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        acc += tl.dot(p.to(v.dtype), v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset,\n    sliding_window_size, IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.\n    constexpr, END: tl.constexpr, INIT: tl.constexpr, SLIDING_WINDOW: tl.\n    constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64\n        ) * stride_kh\n    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64\n        ) * stride_vh\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        m_i = tl.load(m_ptrs).to(tl.float32)\n        l_i = tl.load(l_ptrs).to(tl.float32)\n        acc = tl.load(O_block_ptr).to(tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n        V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset,\n        sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW,\n        IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz,\n    stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX,\n    sliding_window_offset, sliding_window_size, SLIDING_WINDOW: tl.\n    constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr, IS_EVEN_M: tl.\n    constexpr, IS_EVEN_N: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DMODEL:\n    tl.constexpr, BLOCK_N: tl.constexpr):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64\n        ) * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n *\n        BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        m = tl.load(m_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None],\n                p, 0)\n        o += tl.sum(p, axis=0)\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o.to(Out.type.element_ty), mask=o_range < NKV_CTX)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_block_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_BLOCKS_PRE_ROW, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, dtype: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    blocks_ptr = block_index + (off_hz * NUM_ROWS + start_m\n        ) * MAX_BLOCKS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = (q * qk_scale).to(dtype)\n    m_mask = offs_m[:, None] < seqlen\n    block_count = tl.minimum((start_m + 1) * BLOCK_M // BLOCK_N,\n        MAX_BLOCKS_PRE_ROW)\n    for sparse_block_idx in range(block_count):\n        real_block_idx = tl.load(blocks_ptr + sparse_block_idx)\n        start_n = real_block_idx * BLOCK_N\n        cols = start_n + offs_n\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc.to(dtype), mask=m_mask)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.\n    constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M:\n    tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M:\n    tl.constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0).to(tl.float32)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0).to(tl.float32)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n"
    },
    {
      "input": "@custom_autotune.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256,\n    'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N', 'K'],\n    nearest_power_of_two=True, prune_configs_by={'early_config_prune':\n    custom_autotune.matmul248_kernel_config_pruner, 'perf_model': None,\n    'top_k': None})\n@triton.jit\ndef quant_fused_matmul_248_kernel(a_ptr, c_ptr, b1_ptr, scales1_ptr,\n    zeros1_ptr, g1_ptr, b2_ptr, scales2_ptr, zeros2_ptr, g2_ptr, M, N, K,\n    bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm,\n    stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\n    tl.constexpr):\n    \"\"\"\n        Computes: C = silu(A * B1) * (A * B2)\n        A is of shape (M, K) float16\n        B is of shape (K//8, N) int32\n        C is of shape (M, N) float16\n        scales is of shape (1, N) float16\n        zeros is of shape (1, N//8) int32\n        \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b1_ptrs = b1_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    b2_ptrs = b2_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    g1_ptrs = g1_ptr + offs_k\n    g2_ptrs = g2_ptr + offs_k\n    scales1_ptrs = scales1_ptr + offs_bn[None, :]\n    scales2_ptrs = scales2_ptr + offs_bn[None, :]\n    zeros1_ptrs = zeros1_ptr + offs_bn[None, :] // infearure_per_bits\n    zeros2_ptrs = zeros2_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    accumulator2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g1_idx = tl.load(g1_ptrs)\n        g2_idx = tl.load(g2_ptrs)\n        scales1 = tl.load(scales1_ptrs + g1_idx[:, None] * stride_scales)\n        scales2 = tl.load(scales2_ptrs + g2_idx[:, None] * stride_scales)\n        zeros1 = tl.load(zeros1_ptrs + g1_idx[:, None] * stride_zeros)\n        zeros1 = zeros1 >> zeros_shifter[None, :] & maxq\n        zeros1 = zeros1 + 1\n        zeros2 = tl.load(zeros2_ptrs + g2_idx[:, None] * stride_zeros)\n        zeros2 = zeros2 >> zeros_shifter[None, :] & maxq\n        zeros2 = zeros2 + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b1 = tl.load(b1_ptrs)\n        b2 = tl.load(b2_ptrs)\n        b1 = b1 >> shifter[:, None] & maxq\n        b1 = (b1 - zeros1) * scales1\n        accumulator1 += tl.dot(a, b1)\n        b2 = b2 >> shifter[:, None] & maxq\n        b2 = (b2 - zeros2) * scales2\n        accumulator2 += tl.dot(a, b2)\n        a_ptrs += BLOCK_SIZE_K\n        b1_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        b2_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g1_ptrs += BLOCK_SIZE_K\n        g2_ptrs += BLOCK_SIZE_K\n    accumulator1 = silu(accumulator1)\n    c = accumulator1 * accumulator2\n    c = c.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@custom_autotune.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': \n    128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8)], key=['M', 'N', 'K'],\n    nearest_power_of_two=True, prune_configs_by={'early_config_prune':\n    custom_autotune.matmul248_kernel_config_pruner, 'perf_model': None,\n    'top_k': None})\n@triton.jit\ndef quant_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_k\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@custom_autotune.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': \n    32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': \n    128, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8)], key=['M', 'N', 'K'],\n    nearest_power_of_two=True)\n@triton.jit\ndef transpose_quant_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr,\n    zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, stride_scales, stride_zeros,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K:\n    tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_k = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_bk[:, None] // infearure_per_bits * stride_bk + \n        offs_n[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + offs_n[None, :] // infearure_per_bits + g_idx[\n        :, None] * stride_zeros\n    shifter = offs_bk % infearure_per_bits * bits\n    zeros_shifter = offs_n % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(0, num_pid_n):\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = tl.trans(b)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += BLOCK_SIZE_N // infearure_per_bits\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: tl.constexpr, bits: tl.constexpr, outfeatures:\n    tl.constexpr, num_groups: tl.constexpr, X_BLOCK: tl.constexpr):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: tl.constexpr = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None).to(tl\n        .float32)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: tl.constexpr = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    zeros = zeros + 1\n    weights = weights - zeros\n    weights = weights.to(tl.float32)\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n"
    },
    {
      "input": "@triton.jit\ndef _fp4_packed_to_bf16(x_packed, sign_mask_f4, mantissa_mask_f4,\n    mbits_f4_e2m1, ebits_f4_e2m1, f4_e2m1_exp_bias, mbits_f32, ebits_f32,\n    f32_exp_bias, zero_bits_f32, zero_point_five_bits_f32):\n    \"\"\"\n        Input: a tensor of packed fp4 values\n        Output: a tensor of bfloat16 values\n        \"\"\"\n    x_low_bits = x_packed >> 4\n    x_high_bits = x_packed & 15\n    x = tl.interleave(x_low_bits, x_high_bits)\n    sign_f4 = x & sign_mask_f4\n    x_pos = x ^ sign_f4\n    zero_mask = x_pos == 0\n    denormal_mask = x_pos == 1\n    exp_biased_f4 = x_pos >> mbits_f4_e2m1\n    exp_biased_f32 = exp_biased_f4 - f4_e2m1_exp_bias + f32_exp_bias\n    exp_biased_f32 = exp_biased_f32.to(tl.int32) << mbits_f32\n    mantissa_f4 = x_pos & mantissa_mask_f4\n    mantissa_f32 = mantissa_f4.to(tl.int32) << mbits_f32 - mbits_f4_e2m1\n    output = mantissa_f32\n    result = exp_biased_f32 | mantissa_f32\n    result = tl.where(zero_mask, zero_bits_f32, result)\n    result = tl.where(denormal_mask, zero_point_five_bits_f32, result)\n    sign_f32 = sign_f4.to(tl.int32\n        ) << mbits_f32 - mbits_f4_e2m1 + ebits_f32 - ebits_f4_e2m1\n    result = result | sign_f32\n    output = result.to(tl.float32, bitcast=True)\n    output = output.to(tl.bfloat16)\n    return output\n"
    },
    {
      "input": "@triton.jit\ndef triton_f4_to_bf16_kernel(x_ptr, output_ptr, n_elements_in, sign_mask_f4:\n    tl.constexpr, mantissa_mask_f4: tl.constexpr, mbits_f4_e2m1: tl.\n    constexpr, ebits_f4_e2m1: tl.constexpr, f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr, ebits_f32: tl.constexpr, f32_exp_bias: tl.\n    constexpr, zero_bits_f32: tl.constexpr, zero_point_five_bits_f32: tl.\n    constexpr, BLOCK_SIZE_IN: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n    mask_in = offsets_in < n_elements_in\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(x_packed, sign_mask_f4, mantissa_mask_f4,\n        mbits_f4_e2m1, ebits_f4_e2m1, f4_e2m1_exp_bias, mbits_f32,\n        ebits_f32, f32_exp_bias, zero_bits_f32, zero_point_five_bits_f32)\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_IN': 128}), triton.\n    Config({'BLOCK_SIZE_IN': 256}), triton.Config({'BLOCK_SIZE_IN': 512}),\n    triton.Config({'BLOCK_SIZE_IN': 1024}), triton.Config({'BLOCK_SIZE_IN':\n    2048})], key=['n_elements_in'])\n@triton.jit\ndef triton_f4_to_scaled_bf16_kernel(x_ptr, s_ptr, output_ptr, n_elements_in,\n    mx_block_size: tl.constexpr, sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr, mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr, f4_e2m1_exp_bias: tl.constexpr, mbits_f32:\n    tl.constexpr, ebits_f32: tl.constexpr, f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr, zero_point_five_bits_f32: tl.constexpr,\n    e8m0_exponent_bias: tl.constexpr, e8m0_exponent_nan_val: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    n_elements_s = n_elements_out // 32\n    BLOCK_SIZE_S: tl.constexpr = BLOCK_SIZE_IN // 16\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n    mask_in = offsets_in < n_elements_in\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(x_packed, sign_mask_f4, mantissa_mask_f4,\n        mbits_f4_e2m1, ebits_f4_e2m1, f4_e2m1_exp_bias, mbits_f32,\n        ebits_f32, f32_exp_bias, zero_bits_f32, zero_point_five_bits_f32)\n    block_start_s = pid * BLOCK_SIZE_S\n    offsets_s = block_start_s + tl.arange(0, BLOCK_SIZE_S)\n    mask_s = offsets_s < n_elements_s\n    s = tl.load(s_ptr + offsets_s, mask=mask_s)\n    s_offset = s.to(tl.int16) - e8m0_exponent_bias\n    s_fp = libdevice.pow(2.0, s_offset).to(tl.bfloat16)\n    s_fp = tl.where(s != e8m0_exponent_nan_val, s_fp, float('nan'))\n    output = tl.reshape(output, (BLOCK_SIZE_OUT // mx_block_size,\n        mx_block_size))\n    s_fp = tl.reshape(s_fp, (BLOCK_SIZE_S // 1, 1))\n    output = output * s_fp\n    output = tl.reshape(output, (BLOCK_SIZE_OUT,))\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n"
    },
    {
      "input": "@autotune(get_compute_bound_configs() + get_configs_io_bound(), key=['M',\n    'N', 'K'], prune_configs_by={'early_config_prune': early_config_prune,\n    'perf_model': estimate_matmul_time, 'top_k': _AUTOTUNE_TOPK})\n@triton.heuristics({'EVEN_K': MATMUL_HEURISTICS['EVEN_K'], 'SPLIT_K':\n    MATMUL_HEURISTICS['SPLIT_K']})\n@triton.jit\ndef _matmul_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, acc_dtype: tl.constexpr,\n    input_precision: tl.constexpr, fp8_fast_accum: tl.constexpr, BLOCK_M:\n    tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl\n    .constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.\n    constexpr, SWIZZLE: tl.constexpr, EPILOGUE_ELEMENTWISE_ADD: tl.\n    constexpr=False, Epilogue_source=None, EPILOGUE_BROADCAST_SCALE: tl.\n    constexpr=False, Epilogue_scale=None):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    pid_m, pid_n = swizzle_tile(pid, M, N, BLOCK_M, BLOCK_N, GROUP_M, SWIZZLE)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if AB_DTYPE is not None:\n            a = a.to(AB_DTYPE)\n            b = b.to(AB_DTYPE)\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        else:\n            acc += tl.dot(a, b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc = acc.to(C.dtype.element_ty)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask_m = (rm < M)[:, None]\n    mask_n = (rn < N)[None, :]\n    if EPILOGUE_ELEMENTWISE_ADD:\n        Epilogue_source = Epilogue_source + (rm[:, None] * stride_cm + rn[\n            None, :] * stride_cn)\n        source = tl.load(Epilogue_source, mask=mask_m & mask_n)\n        acc += source\n    if EPILOGUE_BROADCAST_SCALE:\n        Epilogue_scale = Epilogue_scale + rn[None, :]\n        scale = tl.load(Epilogue_scale, mask=mask_n)\n        acc *= scale\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask_m & mask_n)\n    else:\n        tl.atomic_add(C, acc, mask=mask_m & mask_n)\n"
    },
    {
      "input": "@autotune(get_small_k_configs(), key=['M', 'N', 'K'], prune_configs_by={\n    'early_config_prune': small_k_early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': _AUTOTUNE_TOPK})\n@triton.jit\ndef _mm_small_k_kernel(A, B, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, acc_dtype: tl.constexpr, input_precision: tl.constexpr,\n    fp8_fast_accum: tl.constexpr, BLOCK_K: tl.constexpr, AB_DTYPE: tl.\n    constexpr, BLOCK_M: tl.constexpr=256, BLOCK_N: tl.constexpr=64, C=None,\n    stride_cm=None, stride_cn=None, Norm2=None, Source=None, stride_sourcem\n    =None, stride_sourcen=None, Magnitude=None, ADD_SOURCE: tl.constexpr=\n    False, EPILOGUE_NORM: tl.constexpr=False, EPILOGUE_MAGNITUDE: tl.\n    constexpr=False, STORE_ACC: tl.constexpr=False):\n    pid_m = tl.program_id(0)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    a = tl.load(A)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    rn = tl.arange(0, BLOCK_N)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    if STORE_ACC:\n        C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    if ADD_SOURCE:\n        Source = Source + (rm[:, None] * stride_sourcem + rn[None, :] *\n            stride_sourcen)\n    if EPILOGUE_NORM:\n        norm_vec = tl.zeros((BLOCK_M,), dtype=acc_dtype)\n    if EPILOGUE_MAGNITUDE:\n        Magnitude = Magnitude + ram\n    mask_m = rm < M\n    for n in range(0, tl.cdiv(N, BLOCK_N)):\n        b = tl.load(B)\n        if AB_DTYPE is not None:\n            a = a.to(AB_DTYPE)\n            b = b.to(AB_DTYPE)\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        else:\n            acc = tl.dot(a, b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        if ADD_SOURCE:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            source = tl.load(Source, mask=mask_m[:, None] & mask_n)\n            acc += source.to(acc_dtype)\n            Source += BLOCK_N * stride_sourcen\n        if EPILOGUE_NORM:\n            norm_vec += tl.sum(acc * acc, axis=1)\n        if STORE_ACC:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            tl.store(C, acc.to(C.dtype.element_ty), mask=mask_m[:, None] &\n                mask_n)\n            C += BLOCK_N * stride_cn\n        B += BLOCK_N * stride_bn\n    if EPILOGUE_NORM:\n        Norm2 = Norm2 + rm\n        norm_vec = tl.rsqrt(norm_vec).to(Norm2.dtype.element_ty)\n        if EPILOGUE_MAGNITUDE:\n            magnitude = tl.load(Magnitude, mask=mask_m)\n            norm_vec *= magnitude\n        tl.store(Norm2, norm_vec, mask=mask_m)\n"
    },
    {
      "input": "@triton.autotune(configs=configs, key=['M', 'N', 'K', 'stride_ak', 'stride_bk']\n    )\n@triton.jit\ndef _scaled_int8_mm_kernel(A_ptr, B_ptr, C_ptr, row_scale_ptr,\n    col_scale_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr=8, EVEN_K: tl.constexpr=\n    True, COL_SCALE_SCALAR: tl.constexpr=False):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n    row_scale = tl.load(row_scale_ptr + idx_m, mask=idx_m < M).to(tl.float32)\n    if COL_SCALE_SCALAR:\n        col_scale = tl.load(col_scale_ptr).to(tl.float32)\n    else:\n        col_scale = tl.load(col_scale_ptr + idx_n, mask=idx_n < N).to(tl.\n            float32)\n    acc = acc.to(tl.float32) * row_scale * col_scale\n    xindex = idx_m * stride_cm + idx_n * stride_cn\n    tl.store(C_ptr + tl.broadcast_to(xindex, mask.shape), acc, mask)\n"
    },
    {
      "input": "@triton.jit\ndef _matmul_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.\n    constexpr, GROUP_M: tl.constexpr, epilogue_alpha=None, epilogue_beta=\n    None, epilogue_source=None, acc_dtype: tl.constexpr=tl.float32,\n    allow_tf32: tl.constexpr=True, fp8_fast_accum: tl.constexpr=True,\n    AB_DTYPE: tl.constexpr=None, EPILOGUE: tl.constexpr=False):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if AB_DTYPE is not None:\n            a = a.to(AB_DTYPE)\n            b = b.to(AB_DTYPE)\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        else:\n            acc += tl.dot(a, b, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    if EPILOGUE:\n        if epilogue_alpha is not None:\n            acc = epilogue_alpha.to(acc_dtype) * acc\n        if epilogue_source is not None:\n            epilogue_src = tl.load(epilogue_source + rm[:, None] *\n                stride_cm + rn[None, :] * stride_cn)\n            if epilogue_beta is not None:\n                epilogue_src = epilogue_src.to(acc_dtype) * epilogue_beta.to(\n                    acc_dtype)\n            acc = acc + epilogue_src\n    acc = acc.to(C.dtype.element_ty)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fused_adam_mm_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, exp_avg_ptr, exp_avg2_ptr, store,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, GROUP_M: tl.constexpr,\n    BETA1: tl.constexpr=BETA1, BETA2: tl.constexpr=BETA2, EPS: tl.constexpr\n    =EPS, acc_dtype: tl.constexpr=tl.float32, allow_tf32: tl.constexpr=\n    False, fp8_fast_accum: tl.constexpr=False, AB_DTYPE: tl.constexpr=None):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if AB_DTYPE is not None:\n            a = a.to(AB_DTYPE)\n            b = b.to(AB_DTYPE)\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        else:\n            acc += tl.dot(a, b, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    epilogue_offsets = rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    exp_avg = tl.load(exp_avg_ptr + epilogue_offsets, mask=mask)\n    exp_avg2 = tl.load(exp_avg2_ptr + epilogue_offsets, mask=mask)\n    exp_avg = BETA1 * exp_avg.to(acc.dtype) + (1.0 - BETA1) * acc\n    exp_avg2 = BETA2 * exp_avg2.to(acc.dtype) + (1.0 - BETA2) * (acc * acc)\n    denom = tl.sqrt(exp_avg2) + EPS\n    norm_grad = exp_avg / denom\n    norm_grad = norm_grad.to(C.dtype.element_ty)\n    C = C + epilogue_offsets\n    if SPLIT_K == 1:\n        tl.store(C, norm_grad, mask=mask)\n    else:\n        tl.atomic_add(C, norm_grad, mask=mask)\n    if store:\n        tl.store(exp_avg_ptr + epilogue_offsets, exp_avg, mask=mask)\n        tl.store(exp_avg2_ptr + epilogue_offsets, exp_avg2, mask=mask)\n"
    },
    {
      "input": "@autotune(configs=get_configs_for_adam(), key=['numels'])\n@heuristics(get_adam_heuristics())\n@triton.jit\ndef _adam_update(avg_ptr, avg2_ptr, grad_ptr, numels, store, BLOCK_SIZE: tl\n    .constexpr, USE_MASK: tl.constexpr, BETA1: tl.constexpr=BETA1, BETA2:\n    tl.constexpr=BETA2, EPS: tl.constexpr=EPS):\n    pid_m = tl.program_id(0)\n    offset = pid_m * BLOCK_SIZE\n    offset = offset + tl.arange(0, BLOCK_SIZE)\n    load_idx = tl.max_contiguous(tl.multiple_of(offset, BLOCK_SIZE), BLOCK_SIZE\n        )\n    mask = None\n    if USE_MASK:\n        mask = load_idx < numels\n    avg = tl.load(avg_ptr + load_idx, mask=mask)\n    avg2 = tl.load(avg2_ptr + load_idx, mask=mask)\n    grad = tl.load(grad_ptr + load_idx, mask=mask)\n    avg = BETA1 * avg + (1.0 - BETA1) * grad\n    avg2 = BETA2 * avg2 + (1.0 - BETA2) * (grad * grad)\n    denom = sqrt(avg2) + EPS\n    norm_grad = avg / denom\n    if store:\n        tl.store(avg_ptr + load_idx, avg, mask=mask)\n        tl.store(avg2_ptr + load_idx, avg2, mask=mask)\n        tl.store(grad_ptr + load_idx, norm_grad, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _dequant_kernel(q_idx_ptr, absmax_ptr, qmap_ptr, dq_ptr, stride_qm,\n    stride_qn, M, N, GROUP_SIZE: tl.constexpr, BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offsets = rm[:, None] * stride_qm + rn[None, :] * stride_qn\n    mask = (rm[:, None] < M) & (rn[None, :] < N)\n    tl.static_print(offsets)\n    group_offsets = offsets // GROUP_SIZE\n    tl.static_print('group_offsets', group_offsets)\n    q_idx = tl.load(q_idx_ptr + offsets, mask=mask)\n    tl.static_print(q_idx)\n    q_vals = tl.load(qmap_ptr + q_idx.to(tl.int32))\n    absmax = tl.load(absmax_ptr + group_offsets, mask=group_offsets < M * N //\n        GROUP_SIZE)\n    dq = q_vals * absmax\n    tl.store(dq_ptr + offsets, dq, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics(values={'USE_MASK': lambda args: args['numels'] % args[\n    'BLOCK_SIZE'] != 0, 'NUM_GROUPS': lambda args: triton.cdiv(args[\n    'numels'], args['BLOCK_SIZE'])})\n@triton.jit\ndef _quantize_blockwise_kernel(t_ptr, cutoffs_ptr, q_ptr, absmax_ptr,\n    norm_ptr, numels, BLOCK_SIZE: tl.constexpr, NUM_BUCKETS: tl.constexpr,\n    USE_MASK: tl.constexpr, NUM_GROUPS: tl.constexpr, RETURN_NORM: tl.\n    constexpr=False):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = None\n    absmax_mask = None\n    if USE_MASK:\n        mask = offsets < numels\n        absmax_mask = pid < NUM_GROUPS\n    t = tl.load(t_ptr + offsets, mask=mask)\n    absmax = tl.max(tl.abs(t), axis=0)\n    normalized = t / absmax\n    cutoffs = tl.load(cutoffs_ptr + tl.arange(0, NUM_BUCKETS))\n    q = tl.reshape(normalized, (BLOCK_SIZE, 1)) > cutoffs\n    q = q.to(tl.uint8)\n    q = tl.sum(q, axis=1)\n    tl.store(q_ptr + offsets, q, mask=mask)\n    tl.store(absmax_ptr + pid, absmax, mask=absmax_mask)\n    if RETURN_NORM:\n        tl.store(norm_ptr + offsets, normalized, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef grouped_launch(pid, m, n, block_m: tl.constexpr, block_n: tl.constexpr,\n    group_m: tl.constexpr):\n    grid_m = tl.cdiv(m, block_m)\n    grid_n = tl.cdiv(n, block_n)\n    width = group_m * grid_n\n    group_id = pid // width\n    group_size = tl.minimum(grid_m - group_id * group_m, group_m)\n    pid_m = group_id * group_m + pid % group_size\n    pid_n = pid % width // group_size\n    return pid_m, pid_n\n"
    },
    {
      "input": "@triton.jit\ndef gemm_split_k_kernel(a_ptr, b_ptr, c_ptr, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, scale_a, scale_b, m, n, k,\n    block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr,\n    split_k: tl.constexpr, group_m: tl.constexpr):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = grouped_launch(pid, m, n, block_m, block_n, group_m)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc = scale_a * scale_b * acc\n    acc.to(tl.float16)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _mixed_mm_kernel(A, B, scales_ptr, zeros_ptr, C, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scale_k,\n    stride_scale_n, IS_BFLOAT16: tl.constexpr, QGROUP_SIZE: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, TRANSPOSED: tl.constexpr=\n    False, GROUP_M: tl.constexpr=8, acc_dtype: tl.constexpr=tl.float32,\n    input_precision: tl.constexpr='ieee', fp8_fast_accum: tl.constexpr=\n    False, DEBUG: tl.constexpr=False):\n    \"\"\"Mixed matmul kernel\n\n    A has shape (M, K) and is float16, bfloat16, or float32\n\n    B is i4 / s4 and has shape (K // 2, N) and is packed as uint8 / int8. See `packed_2xint4` for details.\n\n    Scales and zeros are of shape (NUM_GROUPS, N) and are same dtype as A, where NUM_GROUPS = (K // QGROUP_SIZE)\n    QGROUP_SIZE should be a multiple of BLOCK_K such that a vector of scales / zeros is loaded and broadcasted to block shape\n    per mainloop iteration.\n\n    In the transposed case, A is M x N and B is K x N, and we reduce along \"N\":\n    - TLDR: we are loading rows of A and B blocks at a time, dequantizing and transposing each block of B to achieve the overall\n    effect of a transposed matmul. This is necessary to perform a transposed matmul without unpacking and repacking the B matrix.\n        - Indexing remains the same for A (the reduction dim (BLK_K / K) corresponds to axis 1 of A -- \"N\" above)\n            - We load a BLK_M x BLK_K block of A\n        - Indexing for B is now flipped: N <-> K\n            - We load BLK_N x BLK_K block of B (remembering that the reduction dimension is axis 1 of B)\n            - We dequantize and transpose to BLK_K x BLK_N\n            - scale / zero indexing also change, since we are now iterating along the non-grouping dim within the mac loop and along\n            the grouping dim across blocks.\n        - Each mac loop calculates BLK_M x BLK_N -> M x \"N\"(= K)\n        - Within the mac loop for each block, we iterate along axis=1 for **both** A and B since axis = 1 is now the reduction dim for B.\n\n    NOTE: Assumes that the quantization grouping was done along the K dimension originally (i.e., QGROUP_SIZE consecutive elements\n    of original weight matrix in the K dimension were grouped together when calculating min / max scaling factors).\n    \"\"\"\n    if not TRANSPOSED:\n        tl.static_assert(QGROUP_SIZE % BLOCK_K == 0)\n    else:\n        tl.static_assert(QGROUP_SIZE % BLOCK_N == 0)\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    if not DEBUG:\n        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    else:\n        ram = rm\n    rak = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    if not TRANSPOSED:\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n        else:\n            rbn = rn\n        rbk = pid_z * BLOCK_K // 2 + tl.arange(0, BLOCK_K // 2)\n    else:\n        rn = (pid_n * BLOCK_N // 2 + tl.arange(0, BLOCK_N // 2)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N // 2), \n                BLOCK_N // 2)\n        else:\n            rbn = rn\n        rbk = rak\n    A = A + (ram[:, None] * stride_am + rak[None, :] * stride_ak)\n    if not TRANSPOSED:\n        B = B + (rbk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rbn[:, None] * stride_bk + rbk[None, :] * stride_bn)\n    if not TRANSPOSED:\n        offsets_scale_n = pid_n * stride_scale_n * BLOCK_N + tl.arange(0,\n            BLOCK_N) * stride_scale_n\n    else:\n        scale_offset_k = pid_n * BLOCK_N * stride_scale_k // QGROUP_SIZE\n        offsets_scale_n = tl.arange(0, BLOCK_K) * stride_scale_n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            qb = tl.load(B)\n        else:\n            k_remaining_a = K - k * (BLOCK_K * SPLIT_K)\n            if not TRANSPOSED:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K) // 2\n            else:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rak[None, :] < k_remaining_a, other=_0)\n            qb = tl.load(B, mask=rbk[:, None] < k_remaining_b, other=_0)\n        if not TRANSPOSED:\n            scale_offset_k = (k * BLOCK_K * SPLIT_K * stride_scale_k //\n                QGROUP_SIZE)\n        else:\n            offsets_scale_n = k * stride_scale_n * BLOCK_K + tl.arange(0,\n                BLOCK_K) * stride_scale_n\n        scales = tl.load(scales_ptr + offsets_scale_n + scale_offset_k)\n        zeros = tl.load(zeros_ptr + offsets_scale_n + scale_offset_k)\n        _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n        qb_lo = qb << _4_i8 >> _4_i8\n        qb_hi = qb >> _4_i8\n        if IS_BFLOAT16:\n            dq_b = tl.join(qb_lo.to(tl.float16).to(A.dtype.element_ty),\n                qb_hi.to(tl.float16).to(A.dtype.element_ty)).permute(0, 2, 1)\n        else:\n            dq_b = tl.join(qb_lo.to(A.dtype.element_ty), qb_hi.to(A.dtype.\n                element_ty)).permute(0, 2, 1)\n        if not TRANSPOSED:\n            dq_b = dq_b.reshape(BLOCK_K, BLOCK_N)\n        else:\n            dq_b = dq_b.reshape(BLOCK_N, BLOCK_K)\n        zeros = zeros[None, :]\n        scales = scales[None, :]\n        dq_b = (dq_b - zeros) * scales\n        if TRANSPOSED:\n            dq_b = tl.trans(dq_b)\n        if fp8_fast_accum:\n            acc = tl.dot(a, dq_b, acc, out_dtype=acc_dtype, input_precision\n                =input_precision)\n        else:\n            acc += tl.dot(a, dq_b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        if not TRANSPOSED:\n            B += BLOCK_K * SPLIT_K * stride_bk // 2\n        else:\n            B += BLOCK_K * SPLIT_K * stride_bn\n    acc = acc.to(C.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % GROUP_M\n    pid_n = pid % num_pid_in_group // GROUP_M\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(\n        stride_am, stride_ak), offsets=(pid_m * BLOCK_M, 0), block_shape=(\n        BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(\n        stride_bk, stride_bn), offsets=(0, pid_n * BLOCK_N), block_shape=(\n        BLOCK_K, BLOCK_N), order=(1, 0))\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n        accumulator += tl.dot(a, b)\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    c = accumulator\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(\n        stride_cm, stride_cn), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, s1_ptr, M,\n    N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_s1m, stride_s1n, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr=tl.int32):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = a_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = b_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n    xindex = idx_n + N * idx_m\n    tmp0 = tl.load(s1_ptr + tl.broadcast_to(idx_m, mask.shape), mask,\n        eviction_policy='evict_last')\n    tl.store(c_ptr + tl.broadcast_to(xindex, mask.shape), acc * tmp0, mask)\n"
    },
    {
      "input": "@triton.jit\ndef awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size,\n    result_ptr, num_cols, num_rows, BLOCK_SIZE_X: tl.constexpr,\n    BLOCK_SIZE_Y: tl.constexpr):\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n    offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offsets = num_cols * offsets_y[:, None] + offsets_x[None, :]\n    masks_y = offsets_y < num_rows\n    masks_x = offsets_x < num_cols\n    masks = masks_y[:, None] & masks_x[None, :]\n    result_offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    result_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8\n        )\n    result_offsets = 8 * num_cols * result_offsets_y[:, None\n        ] + result_offsets_x[None, :]\n    result_masks_y = result_offsets_y < num_rows\n    result_masks_x = result_offsets_x < num_cols * 8\n    result_masks = result_masks_y[:, None] & result_masks_x[None, :]\n    iweights = tl.load(qweight_ptr + offsets, masks)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_Y * BLOCK_SIZE_X, 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = iweights >> shifts & 15\n    zero_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    zero_offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    zero_offsets = num_cols * zero_offsets_y[:, None] + zero_offsets_x[None, :]\n    zero_masks_y = zero_offsets_y < num_rows // group_size\n    zero_masks_x = zero_offsets_x < num_cols\n    zero_masks = zero_masks_y[:, None] & zero_masks_x[None, :]\n    zeros = tl.load(zeros_ptr + zero_offsets, zero_masks)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    zeros = zeros >> shifts & 15\n    scale_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    scale_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8)\n    scale_offsets = num_cols * 8 * scale_offsets_y[:, None] + scale_offsets_x[\n        None, :]\n    scale_masks_y = scale_offsets_y < num_rows // group_size\n    scale_masks_x = scale_offsets_x < num_cols * 8\n    scale_masks = scale_masks_y[:, None] & scale_masks_x[None, :]\n    scales = tl.load(scales_ptr + scale_offsets, scale_masks)\n    scales = tl.broadcast_to(scales, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = (iweights - zeros) * scales\n    iweights = iweights.to(result_ptr.type.element_ty)\n    tl.store(result_ptr + result_offsets, iweights, result_masks)\n"
    },
    {
      "input": "@triton.jit\ndef awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K,\n    group_size, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr, SPLIT_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    pid_z = tl.program_id(1)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    accumulator_dtype = c_ptr.type.element_ty\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=\n        accumulator_dtype)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_K * (BLOCK_SIZE_N //\n        8), 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n    offsets_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    masks_am = offsets_am < M\n    offsets_bn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_bn = offsets_bn < N // 8\n    offsets_zn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_zn = offsets_zn < N // 8\n    offsets_sn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    masks_sn = offsets_sn < N\n    offsets_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offsets_a = K * offsets_am[:, None] + offsets_k[None, :]\n    offsets_b = N // 8 * offsets_k[:, None] + offsets_bn[None, :]\n    a_ptrs = a_ptr + offsets_a\n    b_ptrs = b_ptr + offsets_b\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        masks_k = offsets_k < K\n        masks_a = masks_am[:, None] & masks_k[None, :]\n        a = tl.load(a_ptrs, mask=masks_a)\n        masks_b = masks_k[:, None] & masks_bn[None, :]\n        b = tl.load(b_ptrs, mask=masks_b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        offsets_szk = (BLOCK_SIZE_K * SPLIT_K * k + pid_z * BLOCK_SIZE_K\n            ) // group_size + tl.arange(0, 1)\n        offsets_z = N // 8 * offsets_szk[:, None] + offsets_zn[None, :]\n        masks_zk = offsets_szk < K // group_size\n        masks_z = masks_zk[:, None] & masks_zn[None, :]\n        zeros_ptrs = zeros_ptr + offsets_z\n        zeros = tl.load(zeros_ptrs, mask=masks_z)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        offsets_s = N * offsets_szk[:, None] + offsets_sn[None, :]\n        masks_sk = offsets_szk < K // group_size\n        masks_s = masks_sk[:, None] & masks_sn[None, :]\n        scales_ptrs = scales_ptr + offsets_s\n        scales = tl.load(scales_ptrs, mask=masks_s)\n        scales = tl.broadcast_to(scales, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        b = b >> shifts & 15\n        zeros = zeros >> shifts & 15\n        b = (b - zeros) * scales\n        b = b.to(c_ptr.type.element_ty)\n        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n        offsets_k += BLOCK_SIZE_K * SPLIT_K\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * (N // 8)\n    c = accumulator.to(c_ptr.type.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + N * offs_cm[:, None] + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['HxW']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel(input_ptr, N,\n    C, HxW, groups, eps, mean_ptr, rstd_ptr, C_G, ROW_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    for off in range(0, HxW, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < HxW)[:, None] & (row[None, :] < C_G)\n        weight_ = mask.to(tl.float32)\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask).to(tl.\n            float32)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['cluster_size']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_1(input_ptr,\n    N, C, HxW, groups, cluster_size, cluster_num, cluster_mean_ptr,\n    cluster_m2_ptr, cluster_weight_ptr, C_G, ROW_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    group = tl.program_id(0)\n    cluster = tl.program_id(1)\n    pid_batch = tl.program_id(2)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    start = cluster * cluster_size\n    end = start + cluster_size\n    end = min(end, HxW)\n    for off in range(start, end, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < end)[:, None] & (row[None, :] < C_G)\n        weight_ = mask.to(tl.float32)\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask).to(tl.\n            float32)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    offset = pid_batch * groups * cluster_num + group * cluster_num + cluster\n    tl.store(cluster_mean_ptr + offset, mean)\n    tl.store(cluster_m2_ptr + offset, m2)\n    tl.store(cluster_weight_ptr + offset, weight)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['cluster_num']),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['BLOCK_SIZE'] // 128)),\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_2(\n    cluster_mean_ptr, cluster_m2_ptr, cluster_weight_ptr, N, groups,\n    cluster_num, eps, mean_ptr, rstd_ptr, BLOCK_SIZE: tl.constexpr):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    block = tl.arange(0, BLOCK_SIZE)\n    mask = block < cluster_num\n    offset = pid_batch * groups * cluster_num + group * cluster_num + block\n    cluster_mean = tl.load(cluster_mean_ptr + offset, mask=mask)\n    cluster_m2 = tl.load(cluster_m2_ptr + offset, mask=mask)\n    cluster_weight = tl.load(cluster_weight_ptr + offset, mask=mask)\n    mean, m2, weight = tl.reduce((cluster_mean, cluster_m2, cluster_weight),\n        0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(4096, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_2d_kernel(output_ptr, input_ptr, bs, size_inp_0, batch_stride_inp,\n    stride_inp_0, batch_stride_out, stride_out_0, BATCH_STRIDE_INP_IS_1: tl\n    .constexpr, STRIDE_INP_0_IS_1: tl.constexpr, BATCH_STRIDE_OUT_IS_1: tl.\n    constexpr, STRIDE_OUT_0_IS_1: tl.constexpr, BLOCK_M: tl.constexpr):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    pid_m = pid\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + rm * (1 if STRIDE_INP_0_IS_1 else stride_inp_0)\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + rm * (1 if STRIDE_OUT_0_IS_1 else stride_out_0)\n    mask = rm < size_inp_0\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_3d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    batch_stride_inp, stride_inp_0, stride_inp_1, batch_stride_out,\n    stride_out_0, stride_out_1, BATCH_STRIDE_INP_IS_1: tl.constexpr,\n    STRIDE_INP_0_IS_1: tl.constexpr, STRIDE_INP_1_IS_1: tl.constexpr,\n    BATCH_STRIDE_OUT_IS_1: tl.constexpr, STRIDE_OUT_0_IS_1: tl.constexpr,\n    STRIDE_OUT_1_IS_1: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid - pid_m * grid_n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1))\n    mask = (rm < size_inp_0)[:, None] & (rn < size_inp_1)[None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BLOCK_K': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_2'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'STRIDE_INP_2_IS_1': lambda kwargs: kwargs['stride_inp_2'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n    'STRIDE_OUT_2_IS_1': lambda kwargs: kwargs['stride_out_2'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] * kwargs['BLOCK_K'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_4d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    size_inp_2, batch_stride_inp, stride_inp_0, stride_inp_1, stride_inp_2,\n    batch_stride_out, stride_out_0, stride_out_1, stride_out_2,\n    BATCH_STRIDE_INP_IS_1: tl.constexpr, STRIDE_INP_0_IS_1: tl.constexpr,\n    STRIDE_INP_1_IS_1: tl.constexpr, STRIDE_INP_2_IS_1: tl.constexpr,\n    BATCH_STRIDE_OUT_IS_1: tl.constexpr, STRIDE_OUT_0_IS_1: tl.constexpr,\n    STRIDE_OUT_1_IS_1: tl.constexpr, STRIDE_OUT_2_IS_1: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    grid_k = tl.cdiv(size_inp_2, BLOCK_K)\n    pid_m = pid // (grid_n * grid_k)\n    pid_nk = pid - pid_m * (grid_n * grid_k)\n    pid_n = pid_nk // grid_k\n    pid_k = pid_nk - pid_n * grid_k\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = pid_k * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :, None] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1) + rk[None, None, :] * (1 if STRIDE_INP_2_IS_1 else\n        stride_inp_2))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :, None] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1) + rk[None, None, :] * (1 if STRIDE_OUT_2_IS_1 else\n        stride_out_2))\n    mask = (rm < size_inp_0)[:, None, None] & (rn < size_inp_1)[None, :, None\n        ] & (rk < size_inp_2)[None, None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef identity(x):\n    return x\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x.to(tl.float32)).to(x.dtype)\n"
    },
    {
      "input": "@triton.jit\ndef relu(x):\n    return tl.max(x, 0.0)\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    return 0.5 * x * (1.0 + tl.tanh(0.7978845608028654 * (x + 0.044715 * x *\n        x * x)))\n"
    },
    {
      "input": "@triton.jit\ndef welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n    delta = mean_2 - mean_1\n    new_weight = weight_1 + weight_2\n    w2_over_w = tl.where(new_weight == 0.0, 0.0, weight_2 / new_weight)\n    return (mean_1 + delta * w2_over_w, m2_1 + m2_2 + delta * delta *\n        weight_1 * w2_over_w, new_weight)\n"
    },
    {
      "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x_hwc(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_xh_ptr, delta_xw_ptr,\n    delta_xc_ptr, BATCH, IN_C, IN_H, IN_W, KERNEL_N, KERNEL_H, KERNEL_W,\n    OUT_H, OUT_W, stride_h, stride_w, padding_h, padding_w, dilation_h,\n    dilation_w, output_padding_h, output_padding_w, groups, ACC_TYPE: tl.\n    constexpr, CONV1X1_NHWC: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N:\n    tl.constexpr, BLOCK_K: tl.constexpr, GROUP_H: tl.constexpr, WITH_BIAS:\n    tl.constexpr):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_xh_ptrs = delta_xh_ptr + off_x_crs\n        delta_xw_ptrs = delta_xw_ptr + off_x_crs\n        delta_xc_ptrs = delta_xc_ptr + off_x_crs\n        delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n        off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw * stride_xw +\n            delta_xc * stride_xc)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n        delta_xh = 0\n        delta_xw = 0\n    mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n        off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n        delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None, :] >= 0\n        ) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n        if not CONV1X1_NHWC:\n            delta_xh_ptrs += BLOCK_K\n            delta_xw_ptrs += BLOCK_K\n            delta_xc_ptrs += BLOCK_K\n            delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n            off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw *\n                stride_xw + delta_xc * stride_xc)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            x_ptrs += BLOCK_K\n        mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n            off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n            delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None,\n            :] >= 0) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc.to(y.dtype.element_ty)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n"
    },
    {
      "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_x_ptr, BATCH, IN_C, IN_H, IN_W,\n    KERNEL_N, KERNEL_H, KERNEL_W, OUT_H, OUT_W, stride_h, stride_w,\n    padding_h, padding_w, dilation_h, dilation_w, output_padding_h,\n    output_padding_w, groups, ACC_TYPE: tl.constexpr, CONV1X1_NHWC: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.\n    constexpr, GROUP_H: tl.constexpr, WITH_BIAS: tl.constexpr):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_x_ptrs = delta_x_ptr + off_x_crs\n        off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n    mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n        off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[None, :]\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        if not CONV1X1_NHWC:\n            delta_x_ptrs += BLOCK_K\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS,\n                other=0)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            x_ptrs += BLOCK_K\n        mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n            off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[\n            None, :]\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc.to(y.dtype.element_ty)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, Mean, Rstd, stride: tl.constexpr, N:\n    tl.constexpr, eps, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N).to(tl.float32)\n        m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        weight_ = (cols < N).to(tl.float32)\n        _mean, _m2, _weight = x, m2_, weight_\n    else:\n        _mean = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _m2 = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _weight = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            x = tl.load(X + cols, mask=cols < N).to(tl.float32)\n            m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            weight_ = (cols < N).to(tl.float32)\n            if off == 0:\n                _mean, _m2, _weight = x, m2_, weight_\n            else:\n                _mean, _m2, _weight = welford_combine(_mean, _m2, _weight,\n                    x, m2_, weight_)\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1 / tl.sqrt(var + eps)\n    mean = mean.to(x.dtype)\n    rstd = rstd.to(x.dtype)\n    if Mean is not None:\n        tl.store(Mean + row, mean)\n    if Rstd is not None:\n        tl.store(Rstd + row, rstd)\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        if W is None:\n            w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n        else:\n            w = tl.load(W + cols, mask=mask)\n        if B is None:\n            b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n        else:\n            b = tl.load(B + cols, mask=mask)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n    else:\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            mask = cols < N\n            if W is None:\n                w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n            else:\n                w = tl.load(W + cols, mask=mask)\n            if B is None:\n                b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n            else:\n                b = tl.load(B + cols, mask=mask)\n            x = tl.load(X + cols, mask=mask)\n            x_hat = (x - mean) * rstd\n            y = x_hat * w + b\n            tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, Mean, Rstd, Lock,\n    stride: tl.constexpr, N: tl.constexpr, eps, GROUP_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = (dy * xhat).to(w.dtype)\n    partial_db = dy.to(w.dtype)\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M: tl\n    .constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr, IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr, CONJUGATE: tl.constexpr, BLOCK_M: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0).to(tl.float32)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n            ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, Rstd, stride, N, eps, BLOCK_SIZE: tl.constexpr\n    ):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_bwd_dx_fused(DX, DY, DW, X, W, Rstd, Lock, stride, N, eps,\n    GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    rstd = tl.load(Rstd + row)\n    xhat = x * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * c1) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = (dy * xhat).to(w.dtype)\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_bwd_dwdb(DW, FINAL_DW, M, N, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_aligned(Q, K, V, B0, sm_scale, Out, stride_qh, stride_qm,\n    stride_qk, stride_kh, stride_kn, stride_kk, stride_vh, stride_vk,\n    stride_vn, stride_oh, stride_om, stride_on, stride_b0h, stride_b0m, Z,\n    H, N_CTX, P_SEQ, OUT_DTYPE: tl.constexpr, BIAS_LAST_SIZE: tl.constexpr,\n    B0_NUMEL: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_M: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX + P_SEQ), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX +\n        P_SEQ, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0\n        ), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(OUT_DTYPE)\n    lo = 0\n    hi = N_CTX + P_SEQ\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = tl.arange(0, BLOCK_N) % BIAS_LAST_SIZE + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n        stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k)\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n            stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += (b0 + b1) * 1.44269504\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p.to(OUT_DTYPE), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    O_block_ptr = tl.make_block_ptr(base=Out + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc.to(OUT_DTYPE))\n"
    },
    {
      "input": "@triton.jit\ndef helper_id(p):\n    return p\n"
    },
    {
      "input": "@triton.jit\ndef helper_add_and_out(x, y, out_ptr):\n    return x + y, out_ptr\n"
    },
    {
      "input": "@triton.jit\ndef pass_kernel(kernel):\n    pass\n"
    },
    {
      "input": "@triton.jit\ndef add_one_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = x + 1\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128})], key=[])\n@triton.jit\ndef pass_kernel(out_ptr, n_elements, dummy_None, dummy_empty, dummy_float,\n    BLOCK_SIZE: 'tl.constexpr', RANDOM_SIZE: 'tl.constexpr'):\n    pass\n"
    },
    {
      "input": "@triton.jit\ndef mul2_and_add_and_zero_negatives_kernel(in_ptr0, in_ptr1, out_ptr,\n    n_elements, BLOCK_SIZE: 'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    indirection_kernel(in_ptr0, in_ptr0, n_elements, BLOCK_SIZE=BLOCK_SIZE,\n        ACTIVATION='mul2_inplace_kernel')\n    indirection_kernel(in_ptr1, in_ptr1, n_elements, BLOCK_SIZE=BLOCK_SIZE,\n        ACTIVATION='mul2_inplace_kernel')\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    if ACTIVATION == 'zero_negs':\n        output = zero_negs(output)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef mulC_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr',\n    CONSTANT_NAME: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    if CONSTANT_NAME == STRING_CONSTANT_C:\n        output = CONSTANT_C * x\n    if BOOL_CONSTANT_C:\n        output *= CONSTANT_C\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef ones_kernel(x_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = 1.0\n    tl.store(x_ptr + offsets, x, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    output = offsets\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, BLOCK_SIZE: 'tl.constexpr', out_ptr,\n    n_elements):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef square(in_ptr, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    output = x * x\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_half_n_elements(in_ptr0, in_ptr1, out_ptr, half_n_elements,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < half_n_elements * 2\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_imported_symbol(in_ptr, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    output = fast_dividef(x, 3.14)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_imported_symbol(in_ptr, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    output = my_fast_dividef(x, 3.14)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_dtype(in_ptr0, in_ptr1, out_ptr, dtype: 'tl.constexpr',\n    n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask).to(dtype)\n    y = tl.load(in_ptr1 + offsets, mask=mask).to(dtype)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}), triton.Config\n    ({'BLOCK_SIZE': 64})], key=['n_elements'])\n@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef special_params_kernel(in_ptr, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr', num_warps: 'tl.constexpr', num_stages: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    output = x * num_stages + num_warps\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, out_ptr2, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n    tl.store(out_ptr2 + offsets, output + 1, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef kernel(X):\n    return\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 16}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE': 32}, num_stages=3, num_warps\n    =8)], key=[], restore_value=['in_ptr0'])\n@triton.jit\ndef increment_kernel(in_ptr0, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = x + 1\n    tl.store(in_ptr0 + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef triton_add_noise_(x_ptr, y_ptr, seed, numel, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    x = tl.load(x_ptr + offsets, mask=offsets < numel)\n    rnd = tl.rand(seed, offsets)\n    res = x + rnd\n    tl.store(y_ptr + offsets, res, mask=offsets < numel)\n"
    },
    {
      "input": "@triton.jit\ndef triton_(x_ptr, y_ptr, NUMEL: tl.constexpr, IS_ODD: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = BLOCK_SIZE * pid + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < NUMEL\n    data = tl.load(x_ptr + offsets, mask)\n    result = data * data\n    if IS_ODD:\n        result = result + 1\n    tl.store(y_ptr + offsets, result, mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 32}, num_stages=5,\n    num_warps=2), triton.Config({'BLOCK_SIZE': 64}, num_stages=4, num_warps\n    =4)], key=['n_elements'])\n@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    if in_ptr0 is not None:\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n    else:\n        x = 0.0\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef triton_(in_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    x = tl.load(in_ptr + offsets)\n    output = x + FLOAT_CONSTANT_C\n    tl.store(out_ptr + offsets, output)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_out_of_order(in_ptr0, n_elements, in_ptr1, out_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_out_of_order_fn1(in_ptr0, n_elements, in_ptr1, out_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    add_kernel_out_of_order_fn2(in_ptr0, in_ptr1, n_elements, out_ptr,\n        BLOCK_SIZE=BLOCK_SIZE)\n"
    },
    {
      "input": "@triton.jit\ndef reduce_sum_kernel(a_ptr, c_ptr, stride_am, stride_an):\n    offs_am = tl.arange(0, 4)\n    offs_an = tl.arange(0, 4)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_an[None, :] *\n        stride_an)\n    a = tl.load(a_ptrs)\n    m = tl.sum(a, axis=1)\n    tl.store(c_ptr + tl.arange(0, 4), m)\n"
    },
    {
      "input": "@triton.jit\ndef argmax_kernel(a_ptr, c_ptr, stride_am, stride_an):\n    offs_am = tl.arange(0, 4)\n    offs_an = tl.arange(0, 4)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_an[None, :] *\n        stride_an)\n    a = tl.load(a_ptrs)\n    m = tl.argmax(a, axis=1)\n    tl.store(c_ptr + tl.arange(0, 4), m)\n"
    },
    {
      "input": "@triton.jit\ndef cumsum_kernel(in_ptr, out_ptr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n    rindex = tl.arange(0, RBLOCK)[None, :]\n    xindex = tl.arange(0, XBLOCK)[:, None]\n    data = tl.load(in_ptr + rindex)\n    scan = tl.cumsum(data, 1)\n    expected_max = tl.sum(data, 1)\n    tl.device_assert(scan <= expected_max)\n    tl.store(out_ptr + xindex * RBLOCK + rindex, scan)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_fn_call(in_ptr0, in_ptr1, n_elements, out_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    out = helper_id(out_ptr)\n    tl.store(out + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_fn_call(in_ptr0, in_ptr1, n_elements, out_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output, out = helper_add_and_out(x, y, out_ptr)\n    tl.store(out + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef nested_cond_op_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    if tl.program_id(0) == 0:\n        if tl.program_id(1) == 0:\n            output = x + y\n            tl.store(out_ptr + offsets, output, mask=mask)\n    else:\n        pass\n"
    },
    {
      "input": "@triton.jit\ndef add_4_times_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = tl.zeros((n_elements,), dtype=tl.float32)\n    for i in range(4):\n        output += x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_1_time_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    for i in range(0, BLOCK_SIZE):\n        i = tl.multiple_of(i, 1)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_4_times_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = tl.zeros((n_elements,), dtype=tl.float32)\n    for i in range(2):\n        for j in range(2):\n            output += x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_4_times_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output1 = tl.zeros((n_elements,), dtype=tl.float32)\n    output2 = tl.zeros((n_elements,), dtype=tl.float32)\n    for i in range(2):\n        for j in range(2):\n            output1 += y\n            output2 += x\n    output = output1 + output2\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef kernel_with_label(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    if pid > 1:\n        return\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef fwd_kernel(X_ptr, W1_ptr, b1_ptr, O_ptr, M: tl.constexpr, C1: tl.\n    constexpr, C2: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_C2:\n    tl.constexpr):\n    pid_m = tl.program_id(0)\n    offs_c1 = tl.arange(0, C1)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    x_block_ptr = X_ptr + offs_m[:, None] * C1 + offs_c1[None, :]\n    x = tl.load(x_block_ptr)\n    for c2 in range(0, tl.cdiv(C2, BLOCK_SIZE_C2)):\n        offs_c2 = c2 * BLOCK_SIZE_C2 + tl.arange(0, BLOCK_SIZE_C2)\n        o_block_ptr = O_ptr + offs_m[:, None] * C2 + offs_c2[None, :]\n        w1_block_ptr = W1_ptr + offs_c1[:, None] * C2 + offs_c2[None, :]\n        b1_block_ptr = b1_ptr + offs_c2\n        w = tl.load(w1_block_ptr)\n        b = tl.load(b1_block_ptr)\n        o = tl.dot(x, w, allow_tf32=False)\n        o += b[None, :]\n        tl.store(o_block_ptr, o)\n"
    },
    {
      "input": "@triton.jit\ndef fwd_kernel(x_ptr, o_ptr, M, N, stride_m, stride_n, BLOCK_B: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    X_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(M, N), strides=(\n        stride_m, stride_n), offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0))\n    O_block_ptr = tl.make_block_ptr(base=o_ptr, shape=(M, N), strides=(\n        stride_m, stride_n), offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0))\n    for _ in range(BLOCK_B):\n        x = tl.load(X_block_ptr)\n        tl.store(O_block_ptr, x)\n        X_block_ptr = tl.advance(X_block_ptr, (BLOCK_M, 0))\n        O_block_ptr = tl.advance(O_block_ptr, (BLOCK_M, 0))\n"
    },
    {
      "input": "@triton.jit\ndef fwd_kernel(x_ptr, o_ptr, M, N, stride_m, stride_n, BLOCK_B: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    X_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(M, N), strides=(\n        stride_m, stride_n), offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0))\n    O_block_ptr = tl.make_block_ptr(base=o_ptr, shape=(M, N), strides=(\n        stride_m, stride_n), offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0))\n    i = 0\n    while i < BLOCK_B:\n        x = tl.load(X_block_ptr)\n        tl.store(O_block_ptr, x)\n        X_block_ptr = tl.advance(X_block_ptr, (BLOCK_M, 0))\n        O_block_ptr = tl.advance(O_block_ptr, (BLOCK_M, 0))\n        i += 1\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=get_op_configs(), key=['N', 'K'])\n@triton.jit\ndef op_zeros(x_ptr, w_ptr, z_ptr, M, N, K, stride_xm, stride_xk, stride_wk,\n    stride_wn, stride_zm, stride_zn, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, ALLOW_TF32: tl\n    .constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (pid_m * BLOCK_M + offs_m)[:, None] < M\n    mask_n = (pid_n * BLOCK_N + offs_n)[None, :] < N\n    z_mask = mask_m & mask_n\n    z = 0.0\n    z_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_zm\n    z_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_zn\n    z_ptrs = z_ptr + stride_zm * offs_m[:, None] + stride_zn * offs_n[None, :]\n    tl.store(z_ptrs, z, mask=z_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 1024}, num_warps=4,\n    num_stages=2, pre_hook=init_to_zero('output_ptr'))], pre_hook=\n    init_to_zero('output_ptr'), post_hook=init_to_zero('output_ptr'), key=[\n    'n_elements'])\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.atomic_add(output_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE': 32}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE': 16}, num_stages=3, num_warps=8)], key\n    =[], reset_to_zero=['increment_ptr'])\n@triton.jit\ndef increment_kernel(in_ptr0, increment_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    in_ptr_vals = tl.load(in_ptr0 + offsets, mask=mask)\n    increment_val = tl.load(increment_ptr + offsets, mask=mask)\n    tl.store(in_ptr0 + offsets, in_ptr_vals + increment_val, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef pow2_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = x * x\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef pass_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    tl.store(out_ptr + offsets, x, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef pass_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    tl.store(out_ptr + offsets, x, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_kernel(out_ptr, numel, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = BLOCK_SIZE * pid + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < numel\n    ones = tl.full((BLOCK_SIZE,), 1, tl.float16)\n    tl.store(out_ptr + offsets, ones, mask)\n"
    },
    {
      "input": "@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _sin_kernel(in_ptr0, out_ptr, out2_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n    tl.store(out2_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef pass_kernel(x, num):\n    pass\n"
    },
    {
      "input": "@triton.jit\ndef pass_kernel(x, y):\n    pass\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 32}, num_stages=5,\n    num_warps=2), triton.Config({'BLOCK_SIZE': 64}, num_stages=4, num_warps\n    =4)], key=['n_elements'])\n@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, EQ_1_ARG, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    if in_ptr0 is not None:\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n    else:\n        x = 0.0\n    output = tl.sin(x) + EQ_1_ARG\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0, xmask)\n    tmp1 = tl_math.cos(tmp0)\n    tl.store(out_ptr0 + x0, tmp1, xmask)\n"
    },
    {
      "input": "@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@autotune(configs=[triton.Config({'XBLOCK': 1}), triton.Config({'XBLOCK': 2\n    })], meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32',\n    'xnumel': 'i32'}, 'device': DeviceProperties.create(torch.device('cuda'\n    )), 'configs': [AttrsDescriptorWrapper(divisible_by_16=(0, 1),\n    equal_to_1=())], 'constants': {}})\n@triton.jit\ndef kernel(in_out_ptr0, in_ptr0, xnumel, XBLOCK: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * XBLOCK\n    offsets = block_start + tl.arange(0, XBLOCK)\n    mask = offsets < xnumel\n    x = tl.load(in_out_ptr0 + offsets, mask=mask, other=0.0)\n    y = tl.load(in_ptr0 + offsets, mask=mask, other=0.0)\n    output = x + y\n    tl.store(in_out_ptr0 + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef relu_kernel_(inp_ptr, out_ptr, sz, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block = tl.arange(0, BLOCK_SIZE) + pid * BLOCK_SIZE\n    msk = block < sz\n    inp = tl.load(inp_ptr + block, mask=msk)\n    relu = tl.where(inp < 0, 0, inp)\n    tl.store(out_ptr + block, relu, mask=msk)\n"
    },
    {
      "input": "@triton.jit\ndef promote_to_tensor(x):\n    return x + tl.zeros((1,), tl.int1)\n"
    },
    {
      "input": "@triton.jit\ndef div_floor_integer(a, b):\n    quot = a // b\n    remainder = a % b\n    fixed = tl.where(remainder != 0, quot - 1, quot)\n    return tl.where((a < 0) != (b < 0), fixed, quot)\n"
    },
    {
      "input": "@triton.jit\ndef remainder_integer(a, b):\n    remainder = a % b\n    return tl.where(remainder != 0 and (a < 0) != (b < 0), remainder + b,\n        remainder)\n"
    },
    {
      "input": "@triton.jit\ndef is_floating(x):\n    return promote_to_tensor(x).dtype.is_floating()\n"
    },
    {
      "input": "@triton.jit\ndef _prod_accumulate(a, b):\n    return a * b\n"
    },
    {
      "input": "@triton.jit\ndef prod(input, axis):\n    return tl.reduce(input, axis, _prod_accumulate)\n"
    },
    {
      "input": "@triton.jit\ndef minimum(a, b):\n    mask = a < b\n    if is_floating(a):\n        mask |= a != a\n    return tl.where(mask, a, b)\n"
    },
    {
      "input": "@triton.jit\ndef maximum(a, b):\n    mask = a > b\n    if is_floating(a):\n        mask |= a != a\n    return tl.where(mask, a, b)\n"
    },
    {
      "input": "@triton.jit\ndef min2(a, dim):\n    return tl.reduce(a, dim, minimum)\n"
    },
    {
      "input": "@triton.jit\ndef max2(a, dim):\n    return tl.reduce(a, dim, maximum)\n"
    },
    {
      "input": "@triton.jit\ndef minimum_with_index(a_value, a_index, b_value, b_index):\n    mask = a_value < b_value\n    equal = a_value == b_value\n    if is_floating(a_value):\n        a_isnan = a_value != a_value\n        b_isnan = b_value != b_value\n        mask |= a_isnan and not b_isnan\n        equal |= a_isnan and b_isnan\n    mask |= equal & (a_index < b_index)\n    return tl.where(mask, a_value, b_value), tl.where(mask, a_index, b_index)\n"
    },
    {
      "input": "@triton.jit\ndef maximum_with_index(a_value, a_index, b_value, b_index):\n    mask = a_value > b_value\n    equal = a_value == b_value\n    if is_floating(a_value):\n        a_isnan = a_value != a_value\n        b_isnan = b_value != b_value\n        mask |= a_isnan and not b_isnan\n        equal |= a_isnan and b_isnan\n    mask |= equal & (a_index < b_index)\n    return tl.where(mask, a_value, b_value), tl.where(mask, a_index, b_index)\n"
    },
    {
      "input": "@triton.jit\ndef min_with_index(value, index, dim):\n    return tl.reduce((value, index), dim, minimum_with_index)\n"
    },
    {
      "input": "@triton.jit\ndef max_with_index(value, index, dim):\n    return tl.reduce((value, index), dim, maximum_with_index)\n"
    },
    {
      "input": "@triton.jit\ndef welford_reduce(value, mean, m2, weight, first_iteration):\n    if first_iteration:\n        new_weight = tl.full(weight.shape, 1, weight.dtype)\n        new_mean = value\n        new_m2 = tl.zeros_like(m2)\n    else:\n        delta = value - mean\n        new_weight = weight + 1\n        new_mean = mean + delta / new_weight\n        new_m2 = m2 + delta * (value - new_mean)\n    return new_mean, new_m2, new_weight\n"
    },
    {
      "input": "@triton.jit\ndef welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n    delta = mean_2 - mean_1\n    new_weight = weight_1 + weight_2\n    w2_over_w = tl.where(new_weight == 0.0, 0.0, weight_2 / new_weight)\n    return (mean_1 + delta * w2_over_w, m2_1 + m2_2 + delta * delta *\n        weight_1 * w2_over_w, new_weight)\n"
    },
    {
      "input": "@triton.jit\ndef welford(mean, m2, weight, dim):\n    return tl.reduce((mean, m2, weight), dim, welford_combine)\n"
    },
    {
      "input": "@triton.jit\ndef device_assert_then(cond, msg, r):\n    tl.device_assert(cond, msg)\n    return r\n"
    },
    {
      "input": "@triton.jit\ndef randint64(seed, offset, low, high):\n    r0, r1, r2, r3 = tl.randint4x(seed, offset)\n    r0 = r0.to(tl.uint64)\n    r1 = r1.to(tl.uint64)\n    result = r0 | r1 << 32\n    size = high - low\n    result = result % size.to(tl.uint64)\n    result = result.to(tl.int64) + low\n    return result\n"
    },
    {
      "input": "@triton.jit\ndef _any_combine(a, b):\n    return a | b\n"
    },
    {
      "input": "@triton.jit\ndef any(a, dim):\n    return tl.reduce(a, dim, _any_combine)\n"
    },
    {
      "input": "@triton.jit\ndef bucketize_binary_search(values: tl.tensor, boundaries_ptr: tl.tensor,\n    BOUNDARIES_SIZE: int, BOUNDARIES_UNDERLYING_NUMEL: int,\n    BOUNDARIES_STRIDE: int, boundary_indices: tl.tensor, indexing_dtype: tl\n    .dtype, right: 'bool', sorter_ptr: tl.tensor, SORTER_STRIDE: int,\n    sorter_indices: tl.tensor, BLOCK_SHAPE):\n    \"\"\"\n    See [Note: Inductor bucketize op]\n\n    Inputs:\n    -------\n    values: the values to bucketize.\n    boundaries_ptr: a pointer to the beginning of the boundaries tensor, in 1-D.\n    BOUNDARIES_SIZE: the length of the last dimension of the boundaries tensor (i.e. one\n    individual set of boundaries).\n    BOUNDARIES_UNDERLYING_NUMEL: the length of the boundaries tensor, in 1-D, ignoring\n    any striding.\n    BOUNDARIES_STRIDE: the stride of the last dimension of the boundaries tensor\n    boundary_indices: a tensor of the same size as \"values\"; each element is an index\n    into a 1-D, un-strided boundaries tensor, pointing to the first element in the set\n    of boundaries used for that value.\n    indexing_dtype: the dtype used for indexing into the boundaries tensor, and the\n    return dtype.\n    right: if true, use boundary intervals closed on the left; otherwise use intervals\n    closed on the right.\n    sorter_ptr: an optional pointer to a sorter tensor of the same shape as boundaries,\n    but potentially different striding.  If present, this allows us to treat boundaries\n    as sorted even if the elements of boundaries are unsorted.\n    SORTER_STRIDE: must be present if sorter_ptr is non-None; the stride of the last\n    dimension of the sorter tensor.\n    sorter_indices: must be present if sorter_ptr is non-None; see \"boundary_indices\".\n    BLOCK_SHAPE: the shape of the data block being processed.\n    \"\"\"\n    low = tl.zeros(BLOCK_SHAPE, dtype=indexing_dtype)\n    high = tl.full(BLOCK_SHAPE, BOUNDARIES_SIZE, dtype=indexing_dtype)\n    full_range = BOUNDARIES_SIZE + 1\n    while full_range > 1:\n        mid = (high + low) // 2\n        mask = (mid * BOUNDARIES_STRIDE + boundary_indices <\n            BOUNDARIES_UNDERLYING_NUMEL and mid < BOUNDARIES_SIZE)\n        mid_indices = (mid if sorter_ptr is None or SORTER_STRIDE is None else\n            tl.load(sorter_ptr + sorter_indices + SORTER_STRIDE * mid, mask\n            =mask, other=0))\n        bucket_upper_bound = tl.load(boundaries_ptr + boundary_indices + \n            BOUNDARIES_STRIDE * mid_indices, mask=mask, other=0)\n        if right:\n            is_above = values >= bucket_upper_bound\n        else:\n            is_above = values > bucket_upper_bound\n        low = tl.where(is_above & mask, mid + 1, low)\n        high = tl.where(is_above, high, mid)\n        full_range = (full_range + 1) // 2\n    return low\n"
    },
    {
      "input": "@triton.jit\ndef pack_value_flag(value, flag, DTYPE_VALUE_AS_UINT: tl.constexpr,\n    DTYPE_PACK: tl.constexpr):\n    DTYPE_VALUE_AS_UINT = tl.core._constexpr_to_value(DTYPE_VALUE_AS_UINT)\n    bitwidth = DTYPE_VALUE_AS_UINT.primitive_bitwidth\n    uv = value.to(DTYPE_VALUE_AS_UINT, bitcast=True).to(DTYPE_PACK)\n    return flag.to(DTYPE_PACK) | uv << bitwidth\n"
    },
    {
      "input": "@triton.jit\ndef unpack_value(pack, DTYPE_VALUE, DTYPE_VALUE_AS_UINT):\n    DTYPE_VALUE = tl.core._constexpr_to_value(DTYPE_VALUE)\n    DTYPE_VALUE_AS_UINT = tl.core._constexpr_to_value(DTYPE_VALUE_AS_UINT)\n    bitwidth = DTYPE_VALUE_AS_UINT.primitive_bitwidth\n    value_uint = (pack >> bitwidth).to(DTYPE_VALUE_AS_UINT)\n    return value_uint.to(DTYPE_VALUE, bitcast=True)\n"
    },
    {
      "input": "@triton.jit\ndef unpack_flag(pack, DTYPE_FLAG):\n    return pack.to(DTYPE_FLAG)\n"
    },
    {
      "input": "@triton.jit\ndef exclusive_scan_decoupled_lookback(scratch_base, block_value, index,\n    combine_fn, DTYPE_VALUE_AS_UINT: tl.constexpr, DTYPE_PACK: tl.constexpr):\n    \"\"\"Compute exclusive scan of a scalar value between blocks\n\n    Ref: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back\n\n    scratch_base: Pointer to scratch space in global memory\n    block_value: Scalar value for this block\n    index: Scalar index of this block relative to the current scan\n    combine_fn: Function ``(value, value) -> value`` which is scanned over\n    DTYPE_VALUE_AS_UINT: A tl.uint{n} type equal in size to ``block_value``\n    DTYPE_PACK: Unsigned type twice the width of block_value\n\n    NOTE: This function is limited to values which are 32-bits or less because\n    we need to pack (value, flag) into a single unsigned int.\n    \"\"\"\n    DTYPE_VALUE = block_value.dtype\n    pack = pack_value_flag(block_value, tl.full(block_value.shape, 1,\n        DTYPE_VALUE_AS_UINT), DTYPE_VALUE_AS_UINT, DTYPE_PACK)\n    if index > 0:\n        tl.atomic_xchg(scratch_base + index, pack, sem='relaxed')\n    exclusive_prefix = tl.zeros([], DTYPE_VALUE)\n    prefix_valid = False\n    test_target = index - 1\n    while test_target >= 0:\n        flag = tl.full([], 0, DTYPE_VALUE_AS_UINT)\n        while flag == 0:\n            pack = tl.atomic_add(scratch_base + test_target, 0, sem='relaxed')\n            flag = unpack_flag(pack, DTYPE_VALUE_AS_UINT)\n        value = unpack_value(pack, DTYPE_VALUE, DTYPE_VALUE_AS_UINT)\n        if prefix_valid:\n            exclusive_prefix = combine_fn(value, exclusive_prefix)\n        else:\n            exclusive_prefix = value\n            prefix_valid = True\n        if flag == 2:\n            test_target = -1\n        else:\n            test_target = test_target - 1\n    if prefix_valid:\n        inclusive_prefix = combine_fn(exclusive_prefix, block_value)\n    else:\n        inclusive_prefix = block_value\n    pack = pack_value_flag(inclusive_prefix, tl.full([], 2,\n        DTYPE_VALUE_AS_UINT), DTYPE_VALUE_AS_UINT, DTYPE_PACK)\n    tl.atomic_xchg(scratch_base + index, pack, sem='relaxed')\n    return exclusive_prefix\n"
    },
    {
      "input": "@triton.jit\ndef exclusive_scan_decoupled_lookback_64(scratch_base, block_value, index,\n    combine_fn):\n    \"\"\"Compute exclusive scan of a scalar value between blocks\n\n    Ref: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back\n\n    scratch_base: Pointer to scratch space in global memory\n    block_value: Scalar value for this block, must be 64-bits wide\n    index: Scalar index of this block relative to the current scan\n    combine_fn: Function ``(value, value) -> value`` which is scanned over\n    init: Scalar value equal to the identiy of combine_fn\n    \"\"\"\n    if index > 0:\n        block_value_u64 = block_value.to(tl.uint64, bitcast=True)\n        tl.store(scratch_base + 3 * index + 1, block_value_u64)\n        tl.debug_barrier()\n        flag_one = tl.full([], 1, tl.uint64)\n        tl.atomic_xchg(scratch_base + 3 * index + 0, flag_one, sem='release')\n    exclusive_prefix = tl.zeros([], block_value.dtype)\n    prefix_valid = False\n    test_target = index - 1\n    while test_target >= 0:\n        flag = tl.full([], 0, tl.uint64)\n        while flag == 0:\n            flag = tl.atomic_add(scratch_base + 3 * test_target + 0, 0, sem\n                ='acquire')\n        value_u64 = tl.load(scratch_base + 3 * test_target + flag.to(tl.int32))\n        value = value_u64.to(block_value.dtype, bitcast=True)\n        if prefix_valid:\n            exclusive_prefix = combine_fn(value, exclusive_prefix)\n        else:\n            exclusive_prefix = value\n            prefix_valid = True\n        if flag == 2:\n            test_target = -1\n        else:\n            test_target = test_target - 1\n    if prefix_valid:\n        inclusive_prefix = combine_fn(exclusive_prefix, block_value)\n    else:\n        inclusive_prefix = block_value\n    inclusive_prefix_u64 = inclusive_prefix.to(tl.uint64, bitcast=True)\n    tl.store(scratch_base + 3 * index + 2, inclusive_prefix_u64)\n    tl.debug_barrier()\n    flag_two = tl.full([], 2, tl.uint64)\n    tl.atomic_xchg(scratch_base + 3 * index + 0, flag_two, sem='release')\n    return exclusive_prefix\n"
    },
    {
      "input": "@triton.jit\ndef frexp(x):\n    y = libdevice.ilogb(x) + 1\n    exponent = tl.where(x == 0, 0, y)\n    mantissa = tl.where(x == 0, 0, libdevice.ldexp(x, -y))\n    return mantissa, exponent\n"
    },
    {
      "input": "@triton.jit\ndef _compare_and_swap_with_index(x, idxs, rnumel, flip, i: tl.constexpr,\n    n_dims: tl.constexpr, stable: tl.constexpr, descending: tl.constexpr):\n    n_outer: tl.constexpr = x.numel >> n_dims\n    shape: tl.constexpr = [n_outer * 2 ** i, 2, 2 ** (n_dims - i - 1)]\n    idtype = tl.core.get_int_dtype(bitwidth=x.dtype.primitive_bitwidth,\n        signed=True)\n    y = tl.reshape(x, shape)\n    iy = y.to(idtype, bitcast=True)\n    right_mask = tl.arange(0, 2)[None, :, None].to(idtype)\n    left_mask = (1 - right_mask).to(idtype)\n    ileft = tl.broadcast_to(tl.sum(iy * left_mask, 1)[:, None, :], shape)\n    iright = tl.broadcast_to(tl.sum(iy * right_mask, 1)[:, None, :], shape)\n    ileft = tl.reshape(ileft, x.shape)\n    iright = tl.reshape(iright, x.shape)\n    left = ileft.to(x.dtype, bitcast=True)\n    right = iright.to(x.dtype, bitcast=True)\n    y_idx = tl.reshape(idxs, shape)\n    left_idx = tl.broadcast_to(tl.sum(y_idx * left_mask.to(y_idx.dtype), 1)\n        [:, None, :], shape)\n    right_idx = tl.broadcast_to(tl.sum(y_idx * right_mask.to(y_idx.dtype), \n        1)[:, None, :], shape)\n    left_idx = tl.reshape(left_idx, x.shape)\n    right_idx = tl.reshape(right_idx, x.shape)\n    if rnumel is None:\n        left_valid_mask = tl.full(x.shape, True, tl.int1)\n        right_valid_mask = tl.full(x.shape, True, tl.int1)\n    else:\n        left_valid_mask = left_idx < rnumel\n        right_valid_mask = right_idx < rnumel\n    ix = x.to(idtype, bitcast=True)\n    if descending:\n        cond = left < right\n    else:\n        cond = left > right\n    if stable:\n        cond = cond | (left == right) & (left_idx > right_idx)\n    cond = (right_valid_mask > left_valid_mask) | (right_valid_mask ==\n        left_valid_mask) & cond\n    cond = (cond ^ flip).to(tl.int1)\n    ret = ix ^ tl.where(cond, ileft ^ iright, tl.zeros_like(ix))\n    new_idxs = idxs ^ tl.where(cond, left_idx ^ right_idx, tl.zeros_like(idxs))\n    return ret.to(x.dtype, bitcast=True), new_idxs\n"
    },
    {
      "input": "@triton.jit\ndef _bitonic_merge_with_index(x, idxs, rnumel, stage: tl.constexpr,\n    alternating: tl.constexpr, n_dims: tl.constexpr, stable: tl.constexpr,\n    descending: tl.constexpr):\n    n_outer: tl.constexpr = x.numel >> n_dims\n    tl.static_assert(stage <= n_dims)\n    if alternating:\n        shape: tl.constexpr = [n_outer * 2 ** (n_dims - 1 - stage), 2, 2 **\n            stage]\n        flip = tl.reshape(tl.broadcast_to(tl.arange(0, 2)[None, :, None],\n            shape), x.shape)\n    else:\n        flip = False\n    for i in tl.static_range(stage):\n        x, idxs = _compare_and_swap_with_index(x, idxs, rnumel, flip, i + (\n            n_dims - stage), n_dims, stable, descending)\n    return x, idxs\n"
    },
    {
      "input": "@triton.jit\ndef sort_with_index(x, idxs, rnumel, dim: tl.constexpr=None, stable: tl.\n    constexpr=tl.constexpr(False), descending: tl.constexpr=tl.constexpr(False)\n    ):\n    x, idxs = tl.broadcast(x, idxs)\n    _dim: tl.constexpr = len(x.shape) - 1 if dim is None else dim\n    tl.static_assert(_dim == len(x.shape) - 1,\n        'only minor dimension is currently supported')\n    n_dims: tl.constexpr = _log2(x.shape[_dim])\n    for i in tl.static_range(1, n_dims + 1):\n        x, idxs = _bitonic_merge_with_index(x, idxs, rnumel, i, alternating\n            =i < n_dims, n_dims=n_dims, stable=stable, descending=descending)\n    return x, idxs\n"
    },
    {
      "input": "@triton.jit\ndef select_one(x, mask, dim, keep_dims=False):\n    idtype = tl.core.get_int_dtype(x.dtype.primitive_bitwidth, signed=False)\n    ix = x.to(idtype, bitcast=True)\n    iy = tl.sum(ix * mask, dim, keep_dims=keep_dims)\n    return iy.to(x.dtype, bitcast=True)\n"
    },
    {
      "input": "@triton.jit\ndef x_grid_barrier(sem):\n    \"\"\"\n    Wait for all other thread blocks in grid sharing same y/z program_id\n    to reach this barrier before returning.\n\n    Args:\n        sem: an uint32 semaphores, zero or 0x80000000 initialized.  Must be unique to each y/z program ID.\n    \"\"\"\n    tl.debug_barrier()\n    one_i32 = 1\n    one_u32 = one_i32.to(tl.uint32)\n    expected = tl.num_programs(0).to(tl.uint32)\n    if tl.program_id(0) == 0:\n        nb = 2147483648 - (expected - one_u32)\n    else:\n        nb = one_u32\n    old_arrive = tl.atomic_add(sem, nb, sem='release')\n    bar_flipped = False\n    while not bar_flipped:\n        current_arrive = tl.atomic_add(sem, 0, sem='acquire')\n        bar_flipped = (old_arrive ^ current_arrive) & 2147483648 != 0\n    tl.debug_barrier()\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef sub_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x - y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_optional_param(in_ptr0, in_ptr1, out_ptr, n_elements,\n    ARGS_PASSED: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    if ARGS_PASSED == 'two':\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n    else:\n        output = x\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE': 128}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE': 64}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE': 64}, num_stages=4, num_warps=4)], key=[])\n@triton.jit\ndef add_kernel_autotuned(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 16}, num_stages=2,\n    num_warps=2)], key=[])\n@triton.jit\ndef add_kernel_autotuned_weird_param_order(in_ptr0, in_ptr1, n_elements,\n    BLOCK_SIZE: 'tl.constexpr', out_ptr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_X': 128,\n    'BLOCK_SIZE_Y': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_X': 128, 'BLOCK_SIZE_Y': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_X': 64, 'BLOCK_SIZE_Y': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_X': 64, 'BLOCK_SIZE_Y': 64},\n    num_stages=4, num_warps=4)], key=[])\n@triton.jit\ndef add_kernel_2d_autotuned(in_ptr0, in_ptr1, out_ptr, x_elements,\n    y_elements, BLOCK_SIZE_X: 'tl.constexpr', BLOCK_SIZE_Y: 'tl.constexpr'):\n    xoffset = tl.program_id(0) * BLOCK_SIZE_X\n    xindex = xoffset + tl.arange(0, BLOCK_SIZE_X)[:, None]\n    xmask = xindex < x_elements\n    yoffset = tl.program_id(1) * BLOCK_SIZE_Y\n    yindex = yoffset + tl.arange(0, BLOCK_SIZE_Y)[None, :]\n    ymask = yindex < y_elements\n    x1 = xindex\n    y0 = yindex\n    tmp0 = tl.load(in_ptr0 + (x1 + x_elements * y0), xmask & ymask)\n    tmp1 = tl.load(in_ptr0 + (y0 + y_elements * x1), xmask & ymask)\n    tmp2 = tmp0 + tmp1\n    tl.store(out_ptr + (x1 + x_elements * y0), tmp2, xmask & ymask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE': 64}, num_stages=4, num_warps\n    =4)], key=[], warmup=10, rep=20, prune_configs_by={'early_config_prune':\n    _dummy_early_config_prune})\n@triton.jit\ndef add_kernel_autotuned_with_unsupported_args(in_ptr0, in_ptr1, out_ptr,\n    n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_scaling(in_ptr0, in_ptr1, out_ptr, n_elements,\n    scaling_factor, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = (x + y) * scaling_factor\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_tma_1d(in_desc_ptr0, in_desc_ptr1, out_desc_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE\n    a = tl._experimental_descriptor_load(in_desc_ptr0, [offset], [\n        BLOCK_SIZE], tl.float32)\n    b = tl._experimental_descriptor_load(in_desc_ptr1, [offset], [\n        BLOCK_SIZE], tl.float32)\n    output = a + b\n    tl._experimental_descriptor_store(out_desc_ptr, output, [offset])\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_tma_2d(in_desc_ptr0, in_desc_ptr1, out_desc_ptr,\n    BLOCK_SIZE_X: 'tl.constexpr', BLOCK_SIZE_Y: 'tl.constexpr'):\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n    offset_x = pid_x * BLOCK_SIZE_X\n    offset_y = pid_y * BLOCK_SIZE_Y\n    x = tl._experimental_descriptor_load(in_desc_ptr0, [offset_x, offset_y],\n        [BLOCK_SIZE_X, BLOCK_SIZE_Y], tl.float32)\n    y = tl._experimental_descriptor_load(in_desc_ptr1, [offset_x, offset_y],\n        [BLOCK_SIZE_X, BLOCK_SIZE_Y], tl.float32)\n    output = x + y\n    tl._experimental_descriptor_store(out_desc_ptr, output, [offset_x,\n        offset_y])\n"
    },
    {
      "input": "@triton.jit\ndef mul2_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = 2 * x\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef mul2_inplace_kernel(ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(ptr + offsets, mask=mask)\n    output = 2 * x\n    tl.store(ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef zero_negs(x):\n    return tl.where(x >= 0, x, 0)\n"
    },
    {
      "input": "@triton.jit\ndef indirection_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    if ACTIVATION == 'mul2_inplace_kernel':\n        mul2_inplace_kernel(in_ptr0, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    elif ACTIVATION == 'add_kernel':\n        add_kernel(in_ptr0, in_ptr0, out_ptr, n_elements, BLOCK_SIZE=BLOCK_SIZE\n            )\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    tl.store(out_ptr + offsets, x, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef double_strided_kernel(in_ptr, out_ptr, in_y_stride, out_y_stride,\n    X_BLOCK_SIZE: 'tl.constexpr', Y_BLOCK_SIZE: 'tl.constexpr'):\n    xid = tl.program_id(axis=0)\n    yid = tl.program_id(axis=1)\n    x_start = xid * X_BLOCK_SIZE\n    y_start = yid * Y_BLOCK_SIZE\n    x_offsets = x_start + tl.arange(0, X_BLOCK_SIZE)\n    y_offsets = y_start + tl.arange(0, Y_BLOCK_SIZE)\n    src_offsets = y_offsets[:, None] * in_y_stride + x_offsets[None, :]\n    dst_offsets = y_offsets[:, None] * out_y_stride + x_offsets[None, :]\n    src = tl.load(in_ptr + src_offsets)\n    tl.store(out_ptr + dst_offsets, src * 2.0)\n"
    },
    {
      "input": "@triton.jit\ndef inline_asm_kernel(X, Y, Z, n: 'tl.constexpr', BLOCK: 'tl.constexpr'):\n    x = tl.load(X + tl.arange(0, BLOCK))\n    y = tl.load(Y + tl.arange(0, BLOCK))\n    s = tl.full([BLOCK], n, tl.int32)\n    z = tl.inline_asm_elementwise('shf.l.wrap.b32 $0, $1, $2, $3;',\n        '=r,r, r, r', [x, y, s], dtype=tl.int32, is_pure=True, pack=1)\n    tl.store(Z + tl.arange(0, BLOCK), z)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_block_ptr(x_ptr, y_ptr, output_ptr, n_elements,\n    BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    x = tl.load(tl.make_block_ptr(base=x_ptr, shape=[n_elements], strides=[\n        1], offsets=[block_start], block_shape=[BLOCK_SIZE], order=[0]),\n        boundary_check=[0])\n    y = tl.load(tl.make_block_ptr(base=y_ptr, shape=[n_elements], strides=[\n        1], offsets=[block_start], block_shape=[BLOCK_SIZE], order=[0]),\n        boundary_check=[0])\n    output = x + y\n    tl.store(tl.make_block_ptr(base=output_ptr, shape=[n_elements], strides\n        =[1], offsets=[block_start], block_shape=[BLOCK_SIZE], order=[0]),\n        output, boundary_check=[0])\n"
    },
    {
      "input": "@triton.jit\ndef kernel_with_block_ptr_2d(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.\n    constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    x = tl.load(tl.make_block_ptr(base=x_ptr, shape=[n_elements, 1],\n        strides=[1, 1], offsets=[block_start, 0], block_shape=[BLOCK_SIZE, \n        1], order=[1, 0]), boundary_check=[0])\n    output = x\n    tl.store(tl.make_block_ptr(base=output_ptr, shape=[n_elements, 1],\n        strides=[1, 1], offsets=[block_start, 0], block_shape=[BLOCK_SIZE, \n        1], order=[1, 0]), output, boundary_check=[0])\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_with_import(in_ptr0, in_ptr1, out_ptr, n_elements,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = load(in_ptr0 + offsets, mask=mask)\n    y = load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef cond_op_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    if tl.program_id(0) == 0:\n        output = x + y\n    else:\n        output = x * y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef atomic_add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.atomic_add(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_4_times_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    for i in range(2):\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n    i = 2\n    while i > 0:\n        i -= 1\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel_out_of_order_fn2(in_ptr0, in_ptr1, n_elements, out_ptr,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _sampled_addmm_kernel(alpha, beta, IS_BETA_ZERO: tl.constexpr,\n    BLOCKSIZE_ROW: tl.constexpr, BLOCKSIZE_COL: tl.constexpr, k, TILE_K: tl\n    .constexpr, values_ptr, values_batch_stride, values_nnz_stride,\n    values_row_block_stride, values_col_block_stride, crow_indices_ptr,\n    crow_indices_batch_stride, crow_indices_stride, col_indices_ptr,\n    col_indices_batch_stride, col_indices_stride, mat1_ptr,\n    mat1_batch_stride, mat1_tiled_row_stride, mat1_tiled_col_stride,\n    mat1_row_block_stride, mat1_col_block_stride, mat2_ptr,\n    mat2_batch_stride, mat2_tiled_row_stride, mat2_tiled_col_stride,\n    mat2_row_block_stride, mat2_col_block_stride, acc_dtype: tl.constexpr,\n    allow_tf32: tl.constexpr):\n    batch_pid = tl.program_id(axis=1)\n    row_block_pid = tl.program_id(axis=0)\n    crow_indices_offset_ptr = (crow_indices_ptr + crow_indices_batch_stride *\n        batch_pid + crow_indices_stride * row_block_pid)\n    nnz_offset = tl.load(crow_indices_offset_ptr)\n    nnz_offset_next = tl.load(crow_indices_offset_ptr + crow_indices_stride)\n    row_nnz = nnz_offset_next - nnz_offset\n    if row_nnz == 0:\n        return\n    row_block_arange = tl.arange(0, BLOCKSIZE_ROW)\n    col_block_arange = tl.arange(0, BLOCKSIZE_COL)\n    values_block_ptrs = (values_ptr + values_batch_stride * batch_pid + \n        values_nnz_stride * nnz_offset + values_row_block_stride *\n        row_block_arange[:, None] + values_col_block_stride *\n        col_block_arange[None, :])\n    col_index_nnz_ptr = (col_indices_ptr + col_indices_batch_stride *\n        batch_pid + col_indices_stride * nnz_offset)\n    mat1_block_ptrs = (mat1_ptr + mat1_batch_stride * batch_pid + \n        mat1_tiled_row_stride * row_block_pid + mat1_row_block_stride *\n        row_block_arange[:, None])\n    mat2_block_ptrs = (mat2_ptr + mat2_batch_stride * batch_pid + \n        mat2_col_block_stride * col_block_arange[None, :])\n    k_tile_arange = tl.arange(0, TILE_K)\n    for _ in range(row_nnz):\n        acc_block = tl.zeros((BLOCKSIZE_ROW, BLOCKSIZE_COL), dtype=acc_dtype)\n        col_block = tl.load(col_index_nnz_ptr)\n        for k_tile in range(0, k, TILE_K):\n            k_offsets = k_tile + k_tile_arange\n            mask_k = k_offsets < k\n            mat1_block = tl.load(mat1_block_ptrs + mat1_col_block_stride *\n                k_offsets[None, :], mask=mask_k[None, :], other=0.0)\n            mat2_block = tl.load(mat2_block_ptrs + mat2_tiled_col_stride *\n                col_block + mat2_row_block_stride * k_offsets[:, None],\n                mask=mask_k[:, None], other=0.0)\n            acc_block += tl.dot(mat1_block, mat2_block, allow_tf32=\n                allow_tf32, out_dtype=acc_dtype)\n        if IS_BETA_ZERO:\n            acc_block *= alpha\n        else:\n            acc_block = alpha * acc_block + beta * tl.load(values_block_ptrs)\n        tl.store(values_block_ptrs, acc_block.to(values_ptr.dtype.element_ty))\n        values_block_ptrs += values_nnz_stride\n        col_index_nnz_ptr += col_indices_stride\n"
    },
    {
      "input": "@triton.jit\ndef _bsr_strided_dense_rowspace_kernel(values_ptr, values_batch_stride,\n    values_nnz_stride, values_row_block_stride, values_col_block_stride,\n    crow_indices_ptr, crow_indices_batch_stride, crow_indices_stride,\n    col_indices_ptr, col_indices_batch_stride, col_indices_stride,\n    dense_ptr, dense_batch_stride, dense_tiled_row_stride,\n    dense_tiled_col_stride, dense_row_block_stride, dense_col_block_stride,\n    output_ptr, output_batch_stride, output_tiled_row_stride,\n    output_tiled_col_stride, output_row_block_stride,\n    output_col_block_stride, BLOCKSIZE_ROW: tl.constexpr, BLOCKSIZE_COL: tl\n    .constexpr, acc_dtype: tl.constexpr, allow_tf32: tl.constexpr,\n    GROUP_SIZE_ROW: tl.constexpr):\n    batch_pid = tl.program_id(axis=2)\n    row_block_pid = tl.program_id(axis=0)\n    col_block_pid = tl.program_id(axis=1)\n    n_block_rows = tl.num_programs(axis=0)\n    n_block_cols = tl.num_programs(axis=1)\n    row_block_pid, col_block_pid = tl.swizzle2d(row_block_pid,\n        col_block_pid, n_block_rows, n_block_cols, GROUP_SIZE_ROW)\n    crow_indices_offset_ptr = (crow_indices_ptr + crow_indices_batch_stride *\n        batch_pid + crow_indices_stride * row_block_pid)\n    nnz_offset = tl.load(crow_indices_offset_ptr)\n    nnz_offset_next = tl.load(crow_indices_offset_ptr + crow_indices_stride)\n    row_nnz = nnz_offset_next - nnz_offset\n    if row_nnz == 0:\n        return\n    row_block_arange = tl.arange(0, BLOCKSIZE_ROW)\n    col_block_arange = tl.arange(0, BLOCKSIZE_COL)\n    values_block_ptrs = (values_ptr + values_batch_stride * batch_pid + \n        values_nnz_stride * nnz_offset + values_row_block_stride *\n        row_block_arange[:, None] + values_col_block_stride *\n        col_block_arange[None, :])\n    dense_block_ptrs = (dense_ptr + dense_batch_stride * batch_pid + \n        dense_tiled_col_stride * col_block_pid + dense_row_block_stride *\n        col_block_arange[:, None] + dense_col_block_stride *\n        row_block_arange[None, :])\n    output_ptrs = (output_ptr + output_batch_stride * batch_pid + \n        output_tiled_row_stride * row_block_pid + output_tiled_col_stride *\n        col_block_pid + output_row_block_stride * row_block_arange[:, None] +\n        output_col_block_stride * row_block_arange[None, :])\n    col_index_nnz_ptr = (col_indices_ptr + col_indices_batch_stride *\n        batch_pid + col_indices_stride * nnz_offset)\n    output_acc_block = tl.zeros((BLOCKSIZE_ROW, BLOCKSIZE_COL), dtype=acc_dtype\n        )\n    for _ in range(row_nnz):\n        values_block = tl.load(values_block_ptrs)\n        dense_row_idx = tl.load(col_index_nnz_ptr)\n        dense_block = tl.load(dense_block_ptrs + dense_tiled_row_stride *\n            dense_row_idx)\n        output_acc_block += tl.dot(values_block, dense_block, allow_tf32=\n            allow_tf32, out_dtype=acc_dtype)\n        values_block_ptrs += values_nnz_stride\n        col_index_nnz_ptr += col_indices_stride\n    tl.store(output_ptrs, output_acc_block.to(output_ptr.dtype.element_ty))\n"
    },
    {
      "input": "@triton.jit\ndef _bsr_softmax_kernel(crow_indices_ptr, crow_indices_batch_stride,\n    crow_indices_stride, values_ptr, values_batch_stride,\n    values_row_block_stride, values_nnz_col_block_stride, row_block,\n    col_block, MAX_ROW_NNZ: tl.constexpr, TILE: tl.constexpr):\n    batch_pid = tl.program_id(axis=2)\n    row_block_offset_pid = tl.program_id(axis=1)\n    row_block_pid = tl.program_id(axis=0)\n    crow_indices_offset_ptr = (crow_indices_ptr + crow_indices_batch_stride *\n        batch_pid + crow_indices_stride * row_block_pid)\n    nnz_offset = tl.load(crow_indices_offset_ptr)\n    nnz_offset_next = tl.load(crow_indices_offset_ptr + crow_indices_stride)\n    row_nnz = nnz_offset_next - nnz_offset\n    if row_nnz == 0:\n        return\n    row_arange = tl.arange(0, TILE)\n    mask = row_arange < row_nnz * col_block\n    curr_row_values_ptrs = (values_ptr + values_batch_stride * batch_pid + \n        values_row_block_stride * row_block_offset_pid + nnz_offset * col_block\n        )\n    row_tile = tl.load(curr_row_values_ptrs + row_arange, mask=mask, other=\n        -float('inf')).to(tl.float32)\n    max_row_value = tl.max(row_tile, axis=0)\n    for _ in range(TILE, MAX_ROW_NNZ, TILE):\n        row_arange += TILE\n        mask = row_arange < row_nnz * col_block\n        row_tile = tl.load(curr_row_values_ptrs + row_arange, mask=mask,\n            other=-float('inf')).to(tl.float32)\n        curr_max_row_value = tl.max(row_tile, axis=0)\n        max_row_value = tl.where(max_row_value > curr_max_row_value,\n            max_row_value, curr_max_row_value)\n    num = tl.exp(row_tile - max_row_value)\n    denom = tl.sum(num, axis=0)\n    for _ in range(TILE, MAX_ROW_NNZ, TILE):\n        row_arange -= TILE\n        mask = row_arange < row_nnz * col_block\n        row_tile = tl.load(curr_row_values_ptrs + row_arange, mask=mask,\n            other=-float('inf')).to(tl.float32)\n        num = tl.exp(row_tile - max_row_value)\n        denom += tl.sum(num, axis=0)\n    tl.store(curr_row_values_ptrs + row_arange, (num / denom).to(values_ptr\n        .dtype.element_ty), mask=mask)\n    for _ in range(TILE, MAX_ROW_NNZ, TILE):\n        row_arange += TILE\n        mask = row_arange < row_nnz * col_block\n        row_tile = tl.load(curr_row_values_ptrs + row_arange, mask=mask,\n            other=-float('inf')).to(tl.float32)\n        num = tl.exp(row_tile - max_row_value)\n        tl.store(curr_row_values_ptrs + row_arange, (num / denom).to(\n            values_ptr.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _scatter_mm2_kernel(M: tl.constexpr, K: tl.constexpr, N: tl.constexpr,\n    blocks_ptr, blocks_stride_P, blocks_stride_M, blocks_stride_K,\n    others_ptr, others_stride_Q, others_stride_K, others_stride_N,\n    accumulators_ptr, accumulators_stride_R, accumulators_stride_M,\n    accumulators_stride_N, pq_offsets_ptr, pq_offsets_stride, pq_ptr,\n    pq_stride_T, pq_stride_1, dot_out_dtype: tl.constexpr, TILE_M: tl.\n    constexpr, TILE_N: tl.constexpr, allow_tf32: tl.constexpr):\n    Ms = M // TILE_M\n    pid_t = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // Ms\n    pid_n = pid % Ms\n    rm = pid_m * TILE_M + tl.arange(0, TILE_M)\n    rn = pid_n * TILE_N + tl.arange(0, TILE_N)\n    rk = tl.arange(0, K)\n    A_ptr = blocks_ptr + (rm[:, None] * blocks_stride_M + rk[None, :] *\n        blocks_stride_K)\n    B_ptr = others_ptr + (rk[:, None] * others_stride_K + rn[None, :] *\n        others_stride_N)\n    g0 = tl.load(pq_offsets_ptr + pid_t * pq_offsets_stride)\n    g1 = tl.load(pq_offsets_ptr + (pid_t + 1) * pq_offsets_stride)\n    if g0 == g1:\n        return\n    acc_block = tl.zeros((TILE_M, TILE_N), dtype=dot_out_dtype)\n    for i in range(g0, g1):\n        p = tl.load(pq_ptr + i * pq_stride_T)\n        q = tl.load(pq_ptr + i * pq_stride_T + pq_stride_1)\n        A = tl.load(A_ptr + p * blocks_stride_P)\n        B = tl.load(B_ptr + q * others_stride_Q)\n        acc_block += tl.dot(A, B, out_dtype=dot_out_dtype, allow_tf32=\n            allow_tf32)\n    C_ptr = accumulators_ptr + pid_t * accumulators_stride_R + (rm[:, None] *\n        accumulators_stride_M + rn[None, :] * accumulators_stride_N)\n    tl.store(C_ptr, acc_block.to(accumulators_ptr.dtype.element_ty))\n"
    },
    {
      "input": "@triton.jit\ndef _scatter_mm6_kernel(nbatches, Ms, Ks: tl.constexpr, N, blocks_ptr,\n    blocks_stride_P, blocks_stride_M, blocks_stride_K, others_ptr,\n    others_stride_B, others_stride_K, others_stride_N, accumulators_ptr,\n    accumulators_stride_B, accumulators_stride_M, accumulators_stride_N,\n    c_indices_ptr, r_offsets_ptr, p_offsets_ptr, q_offsets_ptr,\n    is_compressed: tl.constexpr, dot_out_dtype: tl.constexpr, SPLIT_N: tl.\n    constexpr, TILE_M: tl.constexpr, TILE_N: tl.constexpr, GROUP_SIZE: tl.\n    constexpr, allow_tf32: tl.constexpr):\n    Ns = N // SPLIT_N\n    BLOCKS_M = Ms // TILE_M\n    BLOCKS_N = Ns // TILE_N\n    pid_t_ = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_b = pid_t_ % nbatches\n    pid_t = pid_t_ // nbatches\n    num_pid_in_group = GROUP_SIZE * BLOCKS_N\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE\n    group_size_m = min(BLOCKS_M - first_pid_m, GROUP_SIZE)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    rm = pid_m * TILE_M + tl.arange(0, TILE_M)\n    rn = pid_n * TILE_N + tl.arange(0, TILE_N)\n    rk = tl.arange(0, Ks)\n    A_ptr = blocks_ptr + (rm[:, None] * blocks_stride_M + rk[None, :] *\n        blocks_stride_K)\n    B_ptr = others_ptr + pid_b * others_stride_B + (rk[:, None] *\n        others_stride_K + rn[None, :] * others_stride_N)\n    r = tl.load(r_offsets_ptr + pid_t)\n    if is_compressed:\n        m = r // N // Ms\n        n = r % N // Ns\n        r0 = tl.load(c_indices_ptr + m)\n        r1 = tl.load(c_indices_ptr + m + 1)\n        g0 = n * r1 + (SPLIT_N - n) * r0\n        nnz = r1 - r0\n    else:\n        g0 = tl.load(c_indices_ptr + pid_t)\n        g1 = tl.load(c_indices_ptr + pid_t + 1)\n        nnz = g1 - g0\n    q_ptr = q_offsets_ptr + g0\n    acc_block = tl.zeros((TILE_M, TILE_N), dtype=dot_out_dtype)\n    if is_compressed:\n        A_ptr += r0 * blocks_stride_P\n        for _ in range(nnz):\n            q = tl.load(q_ptr)\n            B = tl.load(B_ptr + q)\n            A = tl.load(A_ptr)\n            acc_block += tl.dot(A, B, out_dtype=dot_out_dtype, allow_tf32=\n                allow_tf32)\n            A_ptr += blocks_stride_P\n            q_ptr += 1\n    else:\n        p_ptr = p_offsets_ptr + g0\n        for _ in range(nnz):\n            q = tl.load(q_ptr)\n            B = tl.load(B_ptr + q)\n            p = tl.load(p_ptr)\n            A = tl.load(A_ptr + p * blocks_stride_P)\n            p_ptr += 1\n            q_ptr += 1\n            acc_block += tl.dot(A, B, out_dtype=dot_out_dtype, allow_tf32=\n                allow_tf32)\n    C_ptr = accumulators_ptr + r + pid_b * accumulators_stride_B + (rm[:,\n        None] * accumulators_stride_M + rn[None, :] * accumulators_stride_N)\n    tl.store(C_ptr, acc_block.to(accumulators_ptr.dtype.element_ty))\n"
    },
    {
      "input": "@triton.jit\ndef _bsr_strided_addmm_kernel(values_ptr, values_batch_stride,\n    values_nnz_stride, values_row_block_stride, values_col_block_stride,\n    crow_indices_ptr, crow_indices_batch_stride, crow_indices_stride,\n    col_indices_ptr, col_indices_batch_stride, col_indices_stride,\n    input_ptr, input_batch_stride, input_tiled_row_stride,\n    input_tiled_col_stride, input_row_block_stride, input_col_block_stride,\n    dense_ptr, dense_batch_stride, dense_tiled_row_stride,\n    dense_tiled_col_stride, dense_row_block_stride, dense_col_block_stride,\n    left_alpha_ptr, left_alpha_batch_stride, left_alpha_tiled_row_stride,\n    left_alpha_tiled_col_stride: tl.constexpr, left_alpha_row_block_stride,\n    left_alpha_col_block_stride: tl.constexpr, right_alpha_ptr,\n    right_alpha_batch_stride, right_alpha_tiled_row_stride: tl.constexpr,\n    right_alpha_tiled_col_stride, right_alpha_row_block_stride: tl.\n    constexpr, right_alpha_col_block_stride, output_ptr,\n    output_batch_stride, output_tiled_row_stride, output_tiled_col_stride,\n    output_row_block_stride, output_col_block_stride, beta, alpha,\n    beta_is_one: tl.constexpr, beta_is_nonzero: tl.constexpr, alpha_is_one:\n    tl.constexpr, left_alpha_is_one: tl.constexpr, right_alpha_is_one: tl.\n    constexpr, BLOCKSIZE_ROW: tl.constexpr, BLOCKSIZE_COL: tl.constexpr,\n    BLOCKSIZE_INNER: tl.constexpr, acc_dtype: tl.constexpr, allow_tf32: tl.\n    constexpr, GROUP_SIZE_ROW: tl.constexpr, SPLIT_N: tl.constexpr):\n    assert left_alpha_tiled_col_stride == 0\n    assert left_alpha_col_block_stride == 0\n    assert right_alpha_tiled_row_stride == 0\n    assert right_alpha_row_block_stride == 0\n    batch_pid = tl.program_id(axis=2)\n    row_block_pid = tl.program_id(axis=0)\n    col_block_pid = tl.program_id(axis=1)\n    n_block_rows = tl.num_programs(axis=0)\n    n_block_cols = tl.num_programs(axis=1)\n    row_block_pid, col_block_pid = tl.swizzle2d(row_block_pid,\n        col_block_pid, n_block_rows, n_block_cols, GROUP_SIZE_ROW)\n    crow_indices_offset_ptr = (crow_indices_ptr + crow_indices_batch_stride *\n        batch_pid + crow_indices_stride * row_block_pid)\n    nnz_offset = tl.load(crow_indices_offset_ptr)\n    nnz_offset_next = tl.load(crow_indices_offset_ptr + crow_indices_stride)\n    row_nnz = nnz_offset_next - nnz_offset\n    row_block_arange = tl.arange(0, BLOCKSIZE_ROW)\n    inner_block_arange = tl.arange(0, BLOCKSIZE_INNER)\n    col_block_arange = tl.arange(0, BLOCKSIZE_COL)\n    values_block_ptrs = (values_ptr + values_batch_stride * batch_pid + \n        values_nnz_stride * nnz_offset + values_row_block_stride *\n        row_block_arange[:, None] + values_col_block_stride *\n        inner_block_arange[None, :])\n    dense_block_ptrs = (dense_ptr + dense_batch_stride * batch_pid + \n        dense_tiled_col_stride * col_block_pid + dense_row_block_stride *\n        inner_block_arange[:, None] + dense_col_block_stride *\n        col_block_arange[None, :])\n    output_ptrs = (output_ptr + output_batch_stride * batch_pid + \n        output_tiled_row_stride * row_block_pid + output_tiled_col_stride *\n        col_block_pid + output_row_block_stride * row_block_arange[:, None] +\n        output_col_block_stride * col_block_arange[None, :])\n    col_index_nnz_ptr = (col_indices_ptr + col_indices_batch_stride *\n        batch_pid + col_indices_stride * nnz_offset)\n    output_acc_block = tl.zeros((BLOCKSIZE_ROW, BLOCKSIZE_COL), dtype=acc_dtype\n        )\n    for _ in range(row_nnz):\n        values_block = tl.load(values_block_ptrs)\n        dense_row_idx = tl.load(col_index_nnz_ptr)\n        dense_block = tl.load(dense_block_ptrs + dense_tiled_row_stride *\n            dense_row_idx)\n        output_acc_block += tl.dot(values_block, dense_block, allow_tf32=\n            allow_tf32, out_dtype=acc_dtype)\n        values_block_ptrs += values_nnz_stride\n        col_index_nnz_ptr += col_indices_stride\n    if not alpha_is_one:\n        output_acc_block *= alpha\n    if not left_alpha_is_one:\n        left_alpha_ptrs = (left_alpha_ptr + left_alpha_batch_stride *\n            batch_pid + left_alpha_tiled_row_stride * row_block_pid + \n            left_alpha_tiled_col_stride * col_block_pid + \n            left_alpha_row_block_stride * row_block_arange[:, None] + \n            left_alpha_col_block_stride * col_block_arange[None, :])\n        output_acc_block *= tl.load(left_alpha_ptrs)\n    if not right_alpha_is_one:\n        right_alpha_ptrs = (right_alpha_ptr + right_alpha_batch_stride *\n            batch_pid + right_alpha_tiled_row_stride * row_block_pid + \n            right_alpha_tiled_col_stride * col_block_pid + \n            right_alpha_row_block_stride * row_block_arange[:, None] + \n            right_alpha_col_block_stride * col_block_arange[None, :])\n        output_acc_block *= tl.load(right_alpha_ptrs)\n    if beta_is_nonzero:\n        input_ptrs = (input_ptr + input_batch_stride * batch_pid + \n            input_tiled_row_stride * row_block_pid + input_tiled_col_stride *\n            col_block_pid + input_row_block_stride * row_block_arange[:,\n            None] + input_col_block_stride * col_block_arange[None, :])\n        if beta_is_one:\n            output_acc_block += tl.load(input_ptrs)\n        else:\n            output_acc_block += beta * tl.load(input_ptrs)\n    tl.store(output_ptrs, output_acc_block.to(output_ptr.dtype.element_ty))\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, TMP, L, M, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh,\n    stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_DMODEL:\n    tl.constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + start_n * stride_kn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk += tl.where(offs_m[:, None] >= start_n + offs_n[None, :], 0,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + start_n * stride_vk)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, L, NewDO, Delta, BLOCK_M: tl.constexpr, D_HEAD:\n    tl.constexpr):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    denom = tl.load(L + off_m).to(tl.float32)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, k, trans_b=True)\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(p.to(do.dtype), do, trans_a=True)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, v, trans_b=True)\n            ds = p * dp * sm_scale\n            dk += tl.dot(ds.to(q.dtype), q, trans_a=True)\n            dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n            dq += tl.dot(ds.to(k.dtype), k)\n            tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.\n    constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M:\n    tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M:\n    tl.constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0).to(tl.float32)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0).to(tl.float32)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: tl.constexpr, BLOCK_HEADDIM: tl.\n    constexpr):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0).to(tl.float32)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0).to(tl.float32)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.\n    constexpr):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: tl.constexpr, BIAS_TYPE: tl.constexpr, IS_CAUSAL:\n    tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N:\n    tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0\n                        ).to(tl.float32)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs).to(tl.float32)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0\n                        ).to(tl.float32)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p.to(do.dtype), do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = (p * (dp - Di[:, None]) * softmax_scale).to(q.dtype)\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.constexpr,\n    IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, SEQUENCE_PARALLEL:\n    tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM:\n    tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride, BLOCK_SIZE:\n    tl.constexpr, HAS_SMOOTHING: tl.constexpr, SPLIT: tl.constexpr,\n    PRECOMPUTED_LSE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    sum_logits = 0.0\n    if not PRECOMPUTED_LSE:\n        m_i = -float('inf')\n        l_i = 0.0\n        for col_offset in range(0, n_cols, BLOCK_SIZE):\n            cols = col_offset + tl.arange(0, BLOCK_SIZE)\n            logits = tl.load(logits_ptr + cols, mask=cols < n_cols, other=-\n                float('inf')).to(tl.float32) * logit_scale\n            if HAS_SMOOTHING:\n                sum_logits += tl.sum(tl.where(cols < n_cols, logits, 0.0))\n            m_i_new = tl.maximum(m_i, tl.max(logits))\n            l_i = tl.exp(m_i - m_i_new) * l_i + tl.sum(tl.exp(logits - m_i_new)\n                )\n            m_i = m_i_new\n        lse = tl.log(l_i) + m_i\n        tl.store(lse_ptr + row_idx, lse)\n    else:\n        lse = tl.load(lse_ptr + row_idx)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= 0 and label_idx < n_cols:\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + row_idx, z_loss)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')).to(tl.float32) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - smooth_positive,\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_fwd(C, ACT_INPUT, A, B, bias, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bn, stride_bk,\n    BLOCK_M: tl.constexpr, GROUP_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n    A_ROWMAJOR: tl.constexpr, B_COLMAJOR: tl.constexpr, BIAS: tl.constexpr,\n    SAVE_ACT_INPUT: tl.constexpr, ACTIVATION: tl.constexpr):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    if A_ROWMAJOR:\n        A = A + (ram[:, None] * stride_am + rk[None, :])\n    else:\n        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    if B_COLMAJOR:\n        B = B + (rk[:, None] + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        if A_ROWMAJOR:\n            A += BLOCK_K\n        else:\n            A += BLOCK_K * stride_ak\n        if B_COLMAJOR:\n            B += BLOCK_K\n        else:\n            B += BLOCK_K * stride_bk\n    if BIAS:\n        bias = tl.load(bias + rn, mask=rn < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n    if SAVE_ACT_INPUT:\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        tl.store(act_in_ptrs, acc)\n    if ACTIVATION == 'gelu':\n        acc = gelu(acc)\n    elif ACTIVATION == 'gelu_approx':\n        acc = gelu_approx(acc)\n    elif ACTIVATION == 'squared_relu':\n        acc = squared_relu(acc)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_bwd(C, ACT_INPUT, A, B, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bk, stride_bn,\n    BLOCK_M: tl.constexpr, GROUP_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n    ACTIVATION: tl.constexpr):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    if ACTIVATION != 'id':\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        act_input = tl.load(act_in_ptrs).to(acc.dtype)\n    if ACTIVATION == 'gelu':\n        acc *= gelu_grad(act_input)\n    elif ACTIVATION == 'gelu_approx':\n        acc *= gelu_approx_grad(act_input)\n    elif ACTIVATION == 'squared_relu':\n        acc *= squared_relu_grad(act_input)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n"
    },
    {
      "input": "@triton.jit\ndef cosh(x):\n    exp_x = tl.exp(x)\n    return (exp_x + 1.0 / exp_x) * 0.5\n"
    },
    {
      "input": "@triton.jit\ndef relu(x):\n    \"\"\"\n    ReLU_ activation function\n\n    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n    \"\"\"\n    zero = 0.0\n    return tl.where(x >= 0, x, zero.to(x.dtype))\n"
    },
    {
      "input": "@triton.jit\ndef relu_grad(x):\n    zero = 0.0\n    one = 1.0\n    return tl.where(x >= 0, one.to(x.dtype), zero.to(x.dtype))\n"
    },
    {
      "input": "@triton.jit\ndef squared_relu(x):\n    \"\"\"\n    Squared ReLU activation, as proposed in the Primer_ paper.\n\n    .. _Primer: https://arxiv.org/abs/2109.08668\n    \"\"\"\n    x_ = relu(x)\n    return (x_ * x_).to(x.dtype)\n"
    },
    {
      "input": "@triton.jit\ndef squared_relu_grad(x):\n    return tl.where(x >= 0, 2.0 * x, 0.0)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu(x):\n    \"\"\"\n    LeakyReLU_ activation\n\n    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\n    \"\"\"\n    scale = 0.01 + 0.0\n    scale = scale.to(x.dtype)\n    return tl.where(x >= 0, x, scale * x)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu_grad(x):\n    min_grad = 0.01\n    max_grad = 1\n    min_grad = min_grad.to(x.dtype)\n    max_grad = max_grad.to(x.dtype)\n    return tl.where(x >= 0, max_grad, min_grad)\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n"
    },
    {
      "input": "@triton.jit\ndef gelu_grad(x):\n    cdf = 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n    pdf = tl.exp(-0.5 * x * x) * _gaussian_pdf_normalization\n    return cdf + x * pdf\n"
    },
    {
      "input": "@triton.jit\ndef gelu_approx(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit, with tanh approximation\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1.0 + tanh(_sqrt2pi * x * (1.0 + 0.044715 * x * x)))\n"
    },
    {
      "input": "@triton.jit\ndef gelu_approx_grad(x):\n    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + \n        0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr, IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr, CONJUGATE: tl.constexpr, BLOCK_M: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0).to(tl.float32)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n            ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})\n@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, X1, W1, B1, Y1,\n    RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row,\n    stride_y1_row, M, N, eps, dropout_p, IS_RMS_NORM: tl.constexpr, BLOCK_N:\n    tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.\n    constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_X1:\n    tl.constexpr, HAS_W1: tl.constexpr, HAS_B1: tl.constexpr):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols,\n            n_rounds=7) > dropout_p\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32),\n                cols, n_rounds=7) > dropout_p\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask,\n                    mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl\n            .float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])\n@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})\n@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})\n@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, W1, DY1,\n    DX1, DW1, DB1, DRESIDUAL_IN, ROWSCALE, SEEDS, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row,\n    stride_dy1_row, stride_dx1_row, stride_dres_in_row, M, N, eps,\n    dropout_p, rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.\n    constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr,\n    HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr, HAS_ROWSCALE: tl.\n    constexpr, HAS_DY1: tl.constexpr, HAS_DX1: tl.constexpr, HAS_B1: tl.\n    constexpr, RECOMPUTE_OUTPUT: tl.constexpr):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if HAS_DY1:\n        DY1 += row_start * stride_dy1_row\n    if HAS_DX1:\n        DX1 += row_start * stride_dx1_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_DY1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_DY1:\n        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n        if HAS_B1:\n            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if HAS_DY1:\n            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_DY1:\n            wdy += w1 * dy1\n            dw1 += dy1 * xhat\n            if HAS_B1:\n                db1 += dy1\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        if HAS_DX1:\n            if HAS_DROPOUT:\n                keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32),\n                    cols, n_rounds=7) > dropout_p\n                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n            else:\n                dx1 = dx\n            tl.store(DX1 + cols, dx1, mask=mask)\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols,\n                n_rounds=7) > dropout_p\n            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n            dx *= rowscale\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        if HAS_DY1:\n            DY1 += stride_dy1_row\n        if HAS_DX1:\n            DX1 += stride_dx1_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n    if HAS_DY1:\n        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n        if HAS_B1:\n            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _attention_core(Q, K, V, mask, bias, sm_scale, TMP, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_on, Z, H, N_CTX, BATCH, BLOCK_M: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, use_mask:\n    tl.constexpr, use_bias: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    if use_bias:\n        batch_2 = Z // BATCH\n        off_hz_bias = off_hz // (batch_2 * H) * H + off_hz % H\n        offs_base_bias = off_hz_bias * (N_CTX * N_CTX) + offs_m[:, None\n            ] * N_CTX + offs_n[None, :]\n    if use_mask:\n        off_hz_mask = off_hz // H\n        offs_base_mask = off_hz_mask * N_CTX\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q_load_mask = offs_m[:, None] < N_CTX\n    q = tl.load(q_ptrs, mask=q_load_mask, other=0.0)\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=load_mask, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= N_CTX, float('-1e20'), qk)\n        qk = tl.where((start_n + offs_n)[None, :] >= N_CTX, float('-1e20'), qk)\n        if use_bias:\n            bias_load_mask = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            bias_load_mask = tl.where(offs_m[:, None] >= N_CTX, 1.0,\n                bias_load_mask)\n            bias_load_mask = tl.where((start_n + offs_n)[None, :] >= N_CTX,\n                1.0, bias_load_mask)\n            bias_data = tl.load(bias + offs_base_bias + start_n, mask=\n                bias_load_mask == 0.0, other=0.0)\n            qk += bias_data\n        if use_mask:\n            mask_data = tl.load(mask + offs_base_mask + offs_n + start_n,\n                mask=start_n + offs_n < N_CTX, other=0.0)\n            qk = tl.where(mask_data[None, :] == 0.0, float('-1e20'), qk)\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale, mask=offs_m < N_CTX)\n        acc_scale = tl.load(TMP + off_hz * N_CTX + start_m * BLOCK_M + tl.\n            arange(0, BLOCK_M), mask=start_m * BLOCK_M + tl.arange(0,\n            BLOCK_M) < N_CTX, other=float(0.0))\n        acc = acc * acc_scale[:, None]\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=load_mask, other=0.0)\n        p = p.to(Q.dtype.element_ty)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    out_store_mask = offs_m[:, None] < N_CTX\n    tl.store(out_ptrs, acc, mask=out_store_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n    col_offsets, n_cols, use_mask: tl.constexpr, use_bias: tl.constexpr):\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf')\n        ).to(tl.float32)\n    if use_bias:\n        bias = tl.load(bias_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf')).to(tl.float32)\n        row += bias\n    if use_mask:\n        mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf')).to(tl.float32)\n        row = tl.where(mask == 0, float('-1e20'), row)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n    col_offsets, n_cols, is_bf16: tl.constexpr):\n    output_row = tl.load(output_ptrs, mask=col_offsets < n_cols, other=float(0)\n        )\n    d_output_row = tl.load(d_output_ptrs, mask=col_offsets < n_cols, other=\n        float(0))\n    if is_bf16:\n        output_row = output_row.to(tl.float32)\n        d_output_row = d_output_row.to(tl.float32)\n    row_sum = tl.sum(output_row * d_output_row, axis=0)\n    d_softmax_output = (d_output_row - row_sum) * output_row\n    tl.store(d_input_ptrs, d_softmax_output, mask=col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_mask_bias_kernel(output_ptr, input_ptr, mask_ptr, bias_ptr,\n    input_row_stride, output_row_stride, n_cols, n_heads, BLOCK_SIZE: tl.\n    constexpr, use_mask: tl.constexpr, use_bias: tl.constexpr):\n    row_idx = tl.program_id(0).to(tl.int64)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + row_idx * input_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_mask_bias_kernel_two_rows(output_ptr, input_ptr, mask_ptr,\n    bias_ptr, input_row_stride, output_row_stride, n_cols, n_heads,\n    BLOCK_SIZE: tl.constexpr, use_mask: tl.constexpr, use_bias: tl.constexpr):\n    row_idx = tl.program_id(0).to(tl.int64)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + 2 * row_idx * input_row_stride\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + 2 * row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + 2 * row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + (2 * row_idx + 1) // (n_heads * n_cols\n            ) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + (2 * row_idx + 1) % (n_heads * n_cols\n            ) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs + n_cols, output_ptrs + n_cols, mask_ptrs,\n        bias_ptrs, col_offsets, n_cols, use_mask, use_bias)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_grad_kernel(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr, is_bf16: tl.constexpr):\n    row_idx = tl.program_id(0).to(tl.int64)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_grad_kernel_two_rows(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr, is_bf16: tl.constexpr):\n    row_idx = tl.program_id(0).to(tl.int64)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + 2 * row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + 2 * row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n    _softmax_grad_core(output_ptrs + n_cols, d_output_ptrs + n_cols, \n        d_input_ptrs + n_cols, col_offsets, n_cols, is_bf16)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(Out, A, Weight, Bias, Mean, Rstd, stride, N, eps,\n    BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(A + cols, mask=mask, other=0.0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        tl.store(Out + cols, out, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(_DA, _DOut, _A, Weight, Mean, Rstd, stride,\n    NumRows, NumCols, eps, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    row = pid\n    A = _A + row * stride\n    DOut = _DOut + row * stride\n    DA = _DA + row * stride\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        _mean1 += a_hat * wdout\n        _mean2 += wdout\n    mean1 = tl.sum(_mean1, axis=0) / NumCols\n    mean2 = 0.0\n    mean2 = tl.sum(_mean2, axis=0) / NumCols\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        da = (wdout - (a_hat * mean1 + mean2)) * rstd\n        tl.store(DA + cols, da, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(A, DOut, Mean, Var, DW, DB, M, N, BLOCK_SIZE_M: tl\n    .constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    UNROLL: tl.constexpr = 4\n    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n        for j in range(UNROLL):\n            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            mask = (rows[:, None] < M) & (cols[None, :] < N)\n            offs = rows[:, None] * N + cols[None, :]\n            a = tl.load(A + offs, mask=mask, other=0.0).to(tl.float32)\n            dout = tl.load(DOut + offs, mask=mask, other=0.0).to(tl.float32)\n            mean = tl.load(Mean + rows, mask=rows < M, other=0.0)\n            rstd = tl.load(Var + rows, mask=rows < M, other=0.0)\n            a_hat = (a - mean[:, None]) * rstd[:, None]\n            dw += dout * a_hat\n            db += dout\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(DW + cols, sum_dw, mask=cols < N)\n    tl.store(DB + cols, sum_db, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_forward(y_ptr, y_stride, gt_ptr, gt_stride, loss_ptr,\n    loss_stride, n_cols, eps, BLOCK_SIZE: tl.constexpr, log_target: tl.\n    constexpr=False, reduction: tl.constexpr=_REDUCTION_MODE_BATCHMEAN):\n    pid = tl.program_id(0).to(tl.int64)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n    if reduction != _REDUCTION_MODE_NONE:\n        tl.store(loss_ptr, loss_sum)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_backward(target_ptr, target_stride, new_grads_ptr,\n    new_grads_stride, n_cols, BLOCK_SIZE: tl.constexpr, log_target: tl.\n    constexpr=False):\n    pid = tl.program_id(0).to(tl.int64)\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    program_id = tl.program_id(0).to(tl.int64)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    program_id = tl.program_id(0).to(tl.int64)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    z_loss_ptr, loss_stride, n_cols, n_non_ignore, ignore_index,\n    lse_square_scale: tl.constexpr, label_smoothing: tl.constexpr,\n    reduction: tl.constexpr, softcap, RETURN_Z_LOSS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr, HAS_SOFTCAPPING: tl.constexpr):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now. Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Parameters:\n    X_ptr: Pointer to input tensor.\n    X_stride (int): The stride of the input tensor.\n    Y_ptr: Pointer to target tensor.\n    Y_stride (int): The stride of the target tensor.\n    loss_ptr: Pointer to tensor to store the loss.\n    z_loss_ptr: Pointer to tensor to store the z loss. No operation if RETURN_Z_LOSS is 0.\n    loss_stride (int): The stride of the loss tensor.\n    n_cols (int): The number of columns in the input tensor.\n    n_non_ignore (int): The number of non-ignored elements in the batch.\n    ignore_index (int): The index to ignore in the target.\n    label_smoothing (float): The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n    lse_square_scale (float): The scaler of (logsumexp(_input)) ^ 2 adding to the loss for the stability of training.\n    RETURN_Z_LOSS (int): The boolean value to decide whether storing z loss to z_loss_ptr or not. It must be 0 or 1.\n    reduction (str): The string for the reduction to apply\n    softcap (float): The upper threshold for scaling logits to the range (-softcap, +softcap).\n    BLOCK_SIZE (int): The block size for Triton operations.\n    HAS_SOFTCAPPING (bool): The boolean value to determine whether applying soft-capping or not.\n    \"\"\"\n    program_id = tl.program_id(0).to(tl.int64)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    z_loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y).cast(tl.float32)\n    if HAS_SOFTCAPPING:\n        ori_X_y = softcap * tanh(ori_X_y / softcap)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf')).cast(tl.float32)\n        if HAS_SOFTCAPPING:\n            X_block = softcap * tanh(X_block / softcap)\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    lse = m + tl.log(d)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf')).cast(tl.float32)\n        if HAS_SOFTCAPPING:\n            intermediate = tanh(X_block / softcap)\n            X_block = softcap * intermediate\n        X_block = tl.exp(X_block - m) / d\n        X_block += 2 * lse_square_scale * lse * X_block\n        X_block += -eps\n        X_block = tl.where(X_offsets != y, X_block, X_block - (1 -\n            label_smoothing))\n        if reduction == 'mean':\n            X_block = X_block / n_non_ignore\n        if HAS_SOFTCAPPING:\n            X_block = X_block * (1 - intermediate * intermediate)\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = lse - ori_X_y\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * lse\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    z_loss = lse_square_scale * lse * lse\n    loss += z_loss\n    if reduction == 'mean':\n        z_loss = z_loss / n_non_ignore\n        loss = loss / n_non_ignore\n    tl.store(loss_ptr, loss)\n    if RETURN_Z_LOSS == _TRUE:\n        tl.store(z_loss_ptr, z_loss)\n"
    },
    {
      "input": "@triton.jit\ndef _jsd_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr, loss_stride,\n    dX_ptr, dX_stride, label_ptr, beta: tl.constexpr, n_non_ignore: int,\n    ignore_index: tl.constexpr, n_cols, BLOCK_SIZE: tl.constexpr, HAS_LABEL:\n    tl.constexpr):\n    pid = tl.program_id(0).to(tl.int64)\n    X_ptr += pid * X_stride\n    dX_ptr += pid * dX_stride\n    Y_ptr += pid * Y_stride\n    loss_ptr += pid * loss_stride\n    label_ptr += pid\n    if HAS_LABEL:\n        label = tl.load(label_ptr)\n        if label == ignore_index:\n            for i in range(0, n_cols, BLOCK_SIZE):\n                offsets = i + tl.arange(0, BLOCK_SIZE)\n                tl.store(dX_ptr + offsets, 0.0, mask=offsets < n_cols)\n            return\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        X = tl.load(X_ptr + offsets, mask=mask, other=float('-inf')).to(tl.\n            float32)\n        Y = tl.load(Y_ptr + offsets, mask=mask, other=float('-inf')).to(tl.\n            float32)\n        if beta == 0.0:\n            Y_prob = tl.exp(Y)\n            loss = Y_prob * (Y - X)\n            dX = -Y_prob\n        elif beta == 1.0:\n            X_prob = tl.exp(X)\n            loss = X_prob * (X - Y)\n            dX = loss + X_prob\n        else:\n            Q = tl.exp(X)\n            P = tl.exp(Y)\n            M = beta * P + (1 - beta) * Q\n            log_M = tl.log(M)\n            loss = beta * P * Y + (1 - beta) * Q * X - M * log_M\n            dX = (1 - beta) * Q * (X - log_M)\n        loss = loss / n_non_ignore\n        dX = dX / n_non_ignore\n        tl.store(loss_ptr + offsets, loss, mask=mask)\n        tl.store(dX_ptr + offsets, dX, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_qwen2vl_mrope(q_ptr, k_ptr, cos, sin, sl, n_qh: tl.constexpr,\n    n_kh: tl.constexpr, hd: tl.constexpr, pad_n_qh: tl.constexpr, pad_n_kh:\n    tl.constexpr, pad_hd: tl.constexpr, mrope_section_t: tl.constexpr,\n    mrope_section_h: tl.constexpr, BLOCK_SIZE: tl.constexpr, BACKWARD_PASS:\n    tl.constexpr=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * (n_qh * hd)\n    k_ptr = k_ptr + pid * (n_kh * hd)\n    t_end = mrope_section_t\n    h_end = t_end + mrope_section_h\n    cos_row_idx = pid % sl\n    t_cos = cos + cos_row_idx * hd\n    h_cos = t_cos + sl * hd\n    w_cos = h_cos + sl * hd\n    t_sin = sin + cos_row_idx * hd\n    h_sin = t_sin + sl * hd\n    w_sin = h_sin + sl * hd\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    t_mask = cos_offsets < t_end\n    h_mask = (t_end <= cos_offsets) & (cos_offsets < h_end)\n    w_mask = (h_end <= cos_offsets) & (cos_offsets < hd // 2)\n    t_cos_row = tl.load(t_cos + cos_offsets, mask=t_mask, other=0)\n    h_cos_row = tl.load(h_cos + cos_offsets, mask=h_mask, other=0)\n    w_cos_row = tl.load(w_cos + cos_offsets, mask=w_mask, other=0)\n    t_sin_row = tl.load(t_sin + cos_offsets, mask=t_mask, other=0)\n    h_sin_row = tl.load(h_sin + cos_offsets, mask=h_mask, other=0)\n    w_sin_row = tl.load(w_sin + cos_offsets, mask=w_mask, other=0)\n    cos_row = t_cos_row + h_cos_row + w_cos_row\n    sin_row = t_sin_row + h_sin_row + w_sin_row\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        ).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        ).to(sin_row.dtype)\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0).to(sin_row.dtype)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _group_norm_forward_kernel(Y_ptr, Y_row_stride, Y_col_stride, X_ptr,\n    X_row_stride, X_col_stride, Mean_ptr, Mean_row_stride, Mean_col_stride,\n    RSTD_ptr, RSTD_row_stride, RSTD_col_stride, W_ptr, B_ptr, hidden_size,\n    channels_per_group, eps, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    References:\n    https://nn.labml.ai/normalization/group_norm/index.html\n    \"\"\"\n    batch_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    X_ptr += batch_idx * X_row_stride + group_idx * X_col_stride\n    Y_ptr += batch_idx * Y_row_stride + group_idx * Y_col_stride\n    block_range = tl.arange(0, BLOCK_SIZE)\n    s = 0.0\n    squared_sum = 0.0\n    for i in tl.range(0, hidden_size, BLOCK_SIZE):\n        hidden_size_offsets = i + block_range\n        mask = hidden_size_offsets < hidden_size\n        X = tl.load(X_ptr + hidden_size_offsets, mask=mask, other=0.0)\n        s += tl.sum(X)\n        squared_sum += tl.sum(X * X)\n    m = s / hidden_size\n    variance = squared_sum / hidden_size - m * m\n    rstd = rsqrt(variance + eps)\n    hidden_size_per_channel = hidden_size // channels_per_group\n    for channel_idx in tl.range(group_idx * channels_per_group, (group_idx +\n        1) * channels_per_group):\n        W = tl.load(W_ptr + channel_idx)\n        B = tl.load(B_ptr + channel_idx)\n        for i in range(0, hidden_size_per_channel, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size_per_channel\n            X = tl.load(X_ptr + hidden_size_offsets, mask=mask, other=m)\n            Y = (X - m) * rstd * W + B\n            tl.store(Y_ptr + hidden_size_offsets, Y, mask=mask)\n        X_ptr += hidden_size_per_channel\n        Y_ptr += hidden_size_per_channel\n    tl.store(Mean_ptr + batch_idx * Mean_row_stride + group_idx *\n        Mean_col_stride, m)\n    tl.store(RSTD_ptr + batch_idx * RSTD_row_stride + group_idx *\n        RSTD_col_stride, rstd)\n"
    },
    {
      "input": "@triton.jit\ndef _group_norm_backward_kernel(X_ptr, X_row_stride, X_col_stride, W_ptr,\n    Mean_ptr, Mean_ptr_row_stride, Mean_ptr_col_stride, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, UPSTREAM_ptr, hidden_size: tl.constexpr,\n    channels_per_group: tl.constexpr, BLOCK_SIZE: tl.constexpr, dtype: tl.\n    constexpr):\n    \"\"\"\n    References:\n    https://nn.labml.ai/normalization/group_norm/index.html\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n\n    The backprop equations are the same for group_norm and layer_norm\n    the only difference here is that we load the Mean, Rstd corresponding to the\n    group we're computing gradients for and the mean and rstd are computed over n-channels\n    so the total number of elements we compute the mean over is num_channels_per_group * hidden_size\n\n    We also need to load the Weights corresponding to the current channel to compute the gradients.\n    \"\"\"\n    batch_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    X_ptr += batch_idx * X_row_stride\n    DX_ptr += batch_idx * X_row_stride\n    UPSTREAM_ptr += batch_idx * X_row_stride\n    mean = tl.load(Mean_ptr + batch_idx * Mean_ptr_row_stride + group_idx *\n        Mean_ptr_col_stride)\n    rstd = tl.load(RSTD_ptr + batch_idx * Mean_ptr_row_stride + group_idx *\n        Mean_ptr_col_stride)\n    c1 = 0.0\n    c2 = 0.0\n    block_range = tl.arange(0, BLOCK_SIZE)\n    for channel_idx in range(group_idx * channels_per_group, (group_idx + 1\n        ) * channels_per_group):\n        dW = 0.0\n        dB = 0.0\n        W = tl.load(W_ptr + channel_idx)\n        for i in tl.range(0, hidden_size, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size\n            X = tl.load(X_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, mask=mask, other=0.0)\n            UPSTREAM_grad = tl.load(UPSTREAM_ptr + channel_idx *\n                X_col_stride + hidden_size_offsets, mask=mask, other=0.0)\n            x_hat = (X - mean) * rstd\n            dW += tl.sum(UPSTREAM_grad * x_hat)\n            dB += tl.sum(UPSTREAM_grad)\n            wdy = W * UPSTREAM_grad\n            c1 += tl.sum(x_hat * wdy)\n            c2 += tl.sum(wdy)\n        tl.atomic_add(DW_ptr + channel_idx, dW.to(dtype))\n        tl.atomic_add(DB_ptr + channel_idx, dB.to(dtype))\n    N = hidden_size * channels_per_group\n    c1 = c1 / N\n    c2 = c2 / N\n    for channel_idx in tl.range(group_idx * channels_per_group, (group_idx +\n        1) * channels_per_group):\n        W = tl.load(W_ptr + channel_idx)\n        for i in range(0, hidden_size, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size\n            X = tl.load(X_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, mask=mask, other=0.0)\n            UPSTREAM_grad = tl.load(UPSTREAM_ptr + channel_idx *\n                X_col_stride + hidden_size_offsets, mask=mask, other=0.0)\n            x_hat = (X - mean) * rstd\n            wdy = W * UPSTREAM_grad\n            dx = (wdy - (x_hat * c1 + c2)) * rstd\n            tl.store(DX_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, dx, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    tl.constexpr):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by X_ptr with the value pointed by grad_output_ptr.\n    The multiplication is performed in-place on the tensor pointed by X_ptr.\n\n    Parameters:\n    X_ptr: Pointer to the input tensor.\n    X_stride (int): The stride of the input tensor.\n    grad_output_ptr: Pointer to the gradient output value.\n    n_cols (int): The number of columns in the input tensor.\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0).to(tl.int64)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: tl.constexpr, n_qh: tl.\n    constexpr, n_kh: tl.constexpr, hd: tl.constexpr, pad_n_qh: tl.constexpr,\n    pad_n_kh: tl.constexpr, pad_hd: tl.constexpr, BLOCK_SIZE: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        ).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        ).to(sin_row.dtype)\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0).to(sin_row.dtype)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols: tl.\n    constexpr, BLOCK_SIZE: tl.constexpr):\n    program_id = tl.program_id(0).to(tl.int64)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols: tl.\n    constexpr, BLOCK_SIZE: tl.constexpr):\n    program_id = tl.program_id(0).to(tl.int64)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride, n_cols, eps, offset,\n    casting_mode: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)\n\n    Reference:\n    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22\n    3. https://arxiv.org/pdf/1910.07467\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    X_row_dtype = X_row.dtype\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row.to(tl.float32)\n    if casting_mode == _CASTING_MODE_GEMMA:\n        W_row = W_row.to(tl.float32)\n        X_row = X_row.to(tl.float32)\n    if casting_mode == _CASTING_MODE_NONE:\n        eps = eps.to(X_row_dtype)\n        offset = offset.to(X_row_dtype)\n    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols\n    rstd = rsqrt(mean_square + eps)\n    tl.store(RSTD_ptr, rstd)\n    X_row = X_row * rstd\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row.to(X_row_dtype)\n    Y_row = X_row * (offset + W_row)\n    if casting_mode == _CASTING_MODE_GEMMA:\n        Y_row = Y_row.to(X_row_dtype)\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_backward_kernel(dY_ptr, dY_row_stride, dX_ptr, dX_row_stride,\n    X_ptr, X_row_stride, X_dtype: tl.constexpr, W_ptr, W_row_stride,\n    RSTD_ptr, RSTD_row_stride, dW_ptr, dW_row_stride, n_rows, n_cols,\n    offset, rows_per_program: tl.constexpr, casting_mode: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    dx = (1 / RMS) * [dy * (w + offset - (1 / N) * (1 / RMS^2) * ((dy * (w + offset)) dot x) * x]. * means element-wise multiplication, whileas dot means dot product\n    dw = sum(dy * (x / RMS)). summation over BxT dimension\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dW_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dY_ptr += row_start * dY_row_stride\n    dX_ptr += row_start * dX_row_stride\n    X_ptr += row_start * X_row_stride\n    RSTD_ptr += row_start\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)\n    W_row = W_row + offset\n    for _ in range(row_start, row_end):\n        dY_row = tl.load(dY_ptr + col_offsets, mask=mask, other=0.0)\n        X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0.0)\n        rstd_row = tl.load(RSTD_ptr)\n        X_row = X_row.to(tl.float32)\n        if casting_mode == _CASTING_MODE_LLAMA:\n            m = (dY_row * W_row).to(tl.float32)\n        elif casting_mode == _CASTING_MODE_GEMMA:\n            dY_row = dY_row.to(tl.float32)\n            m = dY_row * W_row\n        else:\n            m = dY_row * W_row\n        dX_row = rstd_row * m\n        dX_row += rstd_row * (-(1 / n_cols) * rstd_row * rstd_row * tl.sum(\n            m * X_row, axis=0) * X_row)\n        if casting_mode == _CASTING_MODE_LLAMA:\n            dW_row += dY_row * (X_row * rstd_row).to(X_dtype)\n        else:\n            dW_row += dY_row * (X_row * rstd_row)\n        tl.store(dX_ptr + col_offsets, dX_row.to(X_dtype), mask=mask)\n        dY_ptr += dY_row_stride\n        dX_ptr += dX_row_stride\n        X_ptr += X_row_stride\n        RSTD_ptr += RSTD_row_stride\n    tl.store(dW_ptr + row_block_id * dW_row_stride + col_offsets, dW_row,\n        mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, B_ptr, B_row_stride, Mean_ptr, Mean_row_stride,\n    RSTD_ptr, RSTD_row_stride, n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = rsqrt(var + eps)\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_backward_kernel(X_ptr, W_ptr, Mean_ptr, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, DY_ptr, stride_x, stride_dx, stride_dw, stride_db,\n    stride_dy, n_rows, n_cols, rows_per_program: tl.constexpr, BLOCK_SIZE:\n    tl.constexpr, dtype: tl.constexpr):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/ops/triton/layer_norm.py\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx.to(dtype), mask=mask)\n        dw_row += dy * x_hat\n        db_row += dy\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row.to(dtype),\n        mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row.to(dtype),\n        mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef embedding_forward_kernel(embeddings_ptr, indices_ptr, output_ptr,\n    n_elements, embedding_dim: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    embedding_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    embeddings = tl.load(embeddings_ptr + embedding_offsets, mask=mask_m[:,\n        None] & mask_n[None, :], other=0.0)\n    output_offsets = offsets_m[:, None] * embedding_dim + offsets_n[None, :]\n    tl.store(output_ptr + output_offsets, embeddings, mask=mask_m[:, None] &\n        mask_n[None, :])\n"
    },
    {
      "input": "@triton.jit\ndef embedding_backward_kernel(grad_output_ptr, grad_weight_ptr, indices_ptr,\n    n_elements, embedding_dim: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    grad_output = tl.load(grad_output_ptr + offsets_m[:, None] *\n        embedding_dim + offsets_n[None, :], mask=mask_m[:, None] & mask_n[\n        None, :], other=0.0)\n    grad_weight_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    tl.atomic_add(grad_weight_ptr + grad_weight_offsets, grad_output, mask=\n        mask_m[:, None] & mask_n[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K: tl.constexpr, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl\n    .constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr):\n    tl.static_assert(K % (4 * BLOCK_SIZE_K) == 0,\n        'K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K'\n        )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \"\"\"\n        This part of the code generates pointers to the specific blocks of matrices A and B that the current thread block will process.\n\n        As described in the PyTorch documentation, a stride refers to the step size needed to move from one element to the next along a given dimension:\n\n        For matrix A: stride_am = A.stride(0) = K (stride along the rows), and stride_ak = A.stride(1) = 1 (stride along the columns).\n        For matrix B: stride_bk = B.stride(0) = N (stride along the rows), and stride_bn = B.stride(1) = 1 (stride along the columns).\n        Now, let's break down the pointer generation:\n\n        offs_am[:, None] creates a column of shape [BLOCK_SIZE_M, 1], which represents the row indices of matrix A that this block is processing. It is multiplied by K (the number of columns in matrix A) since A is stored in row-major order. So, the element at position (i, j) in A is located at index i*K + j in memory.\n        offs_k[None, BLOCK_SIZE_K] creates a row vector representing the column indices of the block, i.e., a range from 0 to BLOCK_SIZE_K. This is used to compute the positions of the columns within the block.\n        When combined, the result has the shape [BLOCK_SIZE_M, BLOCK_SIZE_K], where each entry (i, j) points to the element in matrix A at position (i, j) for the current block.\n\n        The same logic is applied to matrix B, but the resulting shape is [BLOCK_SIZE_K, BLOCK_SIZE_N], representing the block of matrix B that the thread block will work on.\n    \"\"\"\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    \"\"\"\n        We split the loop into two layers. The outer loop runs 4 times, and each iteration focuses on a specific portion of matrix A.\n\n        For example, when i = 0, we\u2019re only concerned with the blocks of matrix A that cover the range from 0 to K // (4 * BLOCK_SIZE_K).\n        Since matrix B is packed, its first dimension is effectively divided by 4. So, while we process the first segment of matrix A,\n        we still iterate over the entire first dimension of matrix B.\n\n        In each of the 4 iterations of the outer loop, we go through the full blocks of matrix B, but what changes is the data we extract.\n        Matrix B elements contain 4 weights, all packed into an int8 format, and during each iteration of the outer loop,\n        we extract a different weight by using bitwise shifting operations. This way, we access a unique weight on each pass.\n    \"\"\"\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n            stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n                other=0)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << 2 * i\n            b = (b_uint8 & mask) >> 2 * i\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n            accumulator += tl.dot(a, b.to(tl.int8) - tensor_full, out_dtype\n                =tl.int32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_kernel(X, W, Y, stride_x_N, stride_x_hn, stride_x_hd,\n    stride_y_N, stride_y_hn, stride_y_hd, stride_w_hn, stride_w_hd, N, eps,\n    BLOCK_SIZE: tl.constexpr):\n    Seq = tl.program_id(0)\n    H = tl.program_id(1)\n    X += Seq * stride_x_N + H * stride_x_hn\n    Y += Seq * stride_y_N + H * stride_y_hn\n    W += H * stride_w_hn\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y.to(X.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, BLOCK_HEAD:\n    tl.constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2) * 2\n    dim_range1 = tl.arange(0, BLOCK_DMODEL // 2) * 2 + 1\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos0 - q1 * sin0\n    out1 = q0 * sin1 + q1 * cos1\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos0 - k1 * sin0\n    out_k1 = k0 * sin1 + k1 * cos1\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, head_dim,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim), other=0.0)\n    tl.store(o_ptrs, k, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num, head_dim,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=(offs_h[:, None] <\n        head_num) & (offs_d[None, :] < head_dim), other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.\n        element_ty)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=(offs_h[:, None] < head_num) & (\n        offs_d[None, :] < head_dim))\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, head_dim, BLOCK_SEQ:\n    tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q, mask=offs_d < head_dim, other=0.0)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp, mask=offs_d < head_dim)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, rot_dim,\n    head_dim, BLOCK_HEAD: tl.constexpr, BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL)\n    dim_range1 = rot_dim + tl.arange(0, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos - k1 * sin\n    out_k1 = k0 * sin + k1 * cos\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, head_dim, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL:\n    tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os, mask=\n            offs_d < head_dim, other=0.0)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp, mask=offs_d < head_dim)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out,\n    Req_to_tokens, B_req_idx, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    kv_group_num, b_prompt_cache_len, head_dim: tl.constexpr, BLOCK_M: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, \n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_k = kv_loc[None, :\n            ] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :] <\n            block_end_loc) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n,\n            acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        off_v = kv_loc[:, None\n            ] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None] <\n            block_end_loc) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_no_prompt_cache(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,\n    Out, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, head_dim, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=(start_n + offs_n[None, :] < cur_batch_seq_len\n            ) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=(start_n + offs_n[:, None] < cur_batch_seq_len\n            ) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, stride, N, eps, BLOCK_SIZE: tl.constexpr\n    ):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Alibi, B_Loc, B_Seqlen, max_input_len,\n    Out, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    stride_b_loc_b, stride_b_loc_s, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl\n    .constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    off_k = cur_head * stride_kh + offs_d[None, :] * stride_kd\n    off_v = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    q = tl.load(Q + off_q)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = -float('inf')\n    l_i = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    alibi_m = tl.load(Alibi + cur_head)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + k_index[:, None] * stride_kbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_N], dtype=tl.float32)\n        qk += tl.sum(q[None, :] * k, 1)\n        qk *= sm_scale\n        alibi_loc = cur_batch_seq_len - 1 - (start_n + offs_n)\n        qk -= alibi_loc * alibi_m\n        qk = tl.where(cur_batch_seq_len > start_n + offs_n, qk, float('-inf'))\n        m_ij = tl.max(qk, 0)\n        p = tl.exp(qk - m_ij)\n        l_ij = tl.sum(p, 0)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale\n        v_index = k_index\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p[:, None] * v, 0)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    stride_ph, stride_pbs, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_pbs, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, Alibi, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, att_stride_h, att_stride_bs, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_id = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        alibi_m = tl.load(Alibi + cur_head)\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_id + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value -= alibi_m * (cur_batch_seq_len - 1 - offs_n)\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Alibi, B_Start_Loc, B_Seqlen, Out,\n    Req_to_tokens, B_req_idx, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    b_ready_cache_len, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    ready_cache_len = tl.load(b_ready_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - ready_cache_len\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    alibi_m = tl.load(Alibi + cur_head)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + ready_cache_len, \n        cur_batch_seq_len + ready_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_k = kv_loc[None, :] * stride_kbs + cur_head * stride_kh + offs_d[\n            :, None] * stride_kd\n        k = tl.load(K + off_k, mask=start_n + offs_n[None, :] <\n            block_end_loc, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        alibi_loc = ready_cache_len + offs_m[:, None] - (start_n + offs_n[\n            None, :])\n        qk -= alibi_loc * alibi_m\n        qk = tl.where(offs_m[:, None] + ready_cache_len >= start_n + offs_n\n            [None, :], qk, -10000000.0)\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        off_v = kv_loc[:, None] * stride_vbs + cur_head * stride_vh + offs_d[\n            None, :] * stride_vd\n        v = tl.load(V + off_v, mask=start_n + offs_n[:, None] <\n            block_end_loc, other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, BLOCK_HEAD:\n    tl.constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2) * 2\n    dim_range1 = dim_range0 + 1\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    cos_range = tl.arange(0, BLOCK_DMODEL // 2)\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + cos_range[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + cos_range[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos - k1 * sin\n    out_k1 = k0 * sin + k1 * cos\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))\n"
    },
    {
      "input": "@triton.jit\ndef _gelu_and_mul_kernel(input_ptr, output_ptr, stride_input_m,\n    stride_input_n, stride_output_m, stride_output_n, size_m, size_n,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0).to(tl.\n        float32)\n    gate = gelu(gate)\n    gate = gate.to(input_ptr.dtype.element_ty)\n    tl.store(output_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage1(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: tl.constexpr, block_size: tl.constexpr, numel: tl.\n    constexpr, tokens_per_thread: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    start_idx = pid * tokens_per_thread\n    off_c = (pid + 1) * num_experts\n    for i in range(tokens_per_thread):\n        if start_idx + i < numel:\n            idx = tl.load(topk_ids_ptr + start_idx + i)\n            token_cnt = tl.load(tokens_cnts_ptr + off_c + idx)\n            tl.store(tokens_cnts_ptr + off_c + idx, token_cnt + 1)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage2(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: tl.constexpr, block_size: tl.constexpr, numel: tl.\n    constexpr, tokens_per_thread: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    last_cnt = 0\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + i * num_experts + pid)\n        last_cnt = last_cnt + token_cnt\n        tl.store(tokens_cnts_ptr + i * num_experts + pid, last_cnt)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage3(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: tl.constexpr, block_size: tl.constexpr, numel: tl.\n    constexpr, tokens_per_thread: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    last_cumsum = 0\n    off_cnt = num_experts * num_experts\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + off_cnt + i - 1)\n        last_cumsum = last_cumsum + tl.cdiv(token_cnt, block_size) * block_size\n        tl.store(cumsum_ptr + i, last_cumsum)\n    tl.store(total_tokens_post_pad_ptr, last_cumsum)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage4(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: tl.constexpr, block_size: tl.constexpr, numel: tl.\n    constexpr, tokens_per_thread: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    start_idx = tl.load(cumsum_ptr + pid)\n    end_idx = tl.load(cumsum_ptr + pid + 1)\n    for i in range(start_idx, end_idx, block_size):\n        tl.store(expert_ids_ptr + i // block_size, pid)\n    start_idx = pid * tokens_per_thread\n    off_t = pid * num_experts\n    for i in range(start_idx, tl.minimum(start_idx + tokens_per_thread, numel)\n        ):\n        expert_id = tl.load(topk_ids_ptr + i)\n        token_cnt = tl.load(tokens_cnts_ptr + off_t + expert_id)\n        rank_post_pad = token_cnt + tl.load(cumsum_ptr + expert_id)\n        tl.store(sorted_token_ids_ptr + rank_post_pad, i)\n        tl.store(tokens_cnts_ptr + off_t + expert_id, token_cnt + 1)\n"
    },
    {
      "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, a_scale_ptr, b_scale_ptr,\n    topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K:\n    tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.\n    constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, use_fp8: tl\n    .constexpr):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using\n    token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can\n        be any shape representing batches and K is the feature dimension of\n        each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is\n        the number of experts, K is the input feature dimension, and N is\n        the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the\n        total number of tokens post padding, topk is the number of times\n        each token is repeated, and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens,\n        repeated topk times and arranged by the expert index they are\n        assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each\n        block. It determines which expert matrix from B should be used for\n        each block in A.\n    This kernel performs the multiplication of a token by its corresponding\n    expert matrix as determined by `expert_ids`. The sorting of\n    `sorted_token_ids` by expert index and padding ensures divisibility by\n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix\n    multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    if use_fp8:\n        accumulator = (accumulator * a_scale * b_scale).to(compute_type)\n    else:\n        accumulator = accumulator.to(compute_type)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(KV_nope, KV_rope, Dest_loc, O_nope,\n    O_rope, stride_kv_nope_bs, stride_kv_nope_h, stride_kv_nope_d,\n    stride_kv_rope_bs, stride_kv_rope_h, stride_kv_rope_d, stride_o_nope_bs,\n    stride_o_nope_h, stride_o_nope_d, stride_o_rope_bs, stride_o_rope_h,\n    stride_o_rope_d, kv_nope_head_num, kv_rope_head_num, BLOCK_DMODEL_NOPE:\n    tl.constexpr, BLOCK_DMODEL_ROPE: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_d_nope = tl.arange(0, BLOCK_DMODEL_NOPE)\n    offs_d_rope = tl.arange(0, BLOCK_DMODEL_ROPE)\n    dest_index = tl.load(Dest_loc + cur_index)\n    kv_nope_ptrs = (KV_nope + cur_index * stride_kv_nope_bs + \n        stride_kv_nope_d * offs_d_nope[None, :])\n    kv_rope_ptrs = (KV_rope + cur_index * stride_kv_rope_bs + \n        stride_kv_rope_d * offs_d_rope[None, :])\n    o_nope_ptrs = (O_nope + dest_index * stride_o_nope_bs + stride_o_nope_d *\n        offs_d_nope[None, :])\n    o_rope_ptrs = (O_rope + dest_index * stride_o_rope_bs + stride_o_rope_d *\n        offs_d_rope[None, :])\n    kv_nope = tl.load(kv_nope_ptrs)\n    kv_rope = tl.load(kv_rope_ptrs)\n    tl.store(o_nope_ptrs, kv_nope)\n    tl.store(o_rope_ptrs, kv_rope)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q_nope, Q_rope, KV_nope, KV_rope,\n    sm_scale, Req_to_tokens, B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_q_bs, stride_q_h,\n    stride_q_d, stride_q_rope_bs, stride_q_rope_h, stride_q_rope_d,\n    stride_kv_bs, stride_kv_h, stride_kv_d, stride_kv_rope_bs,\n    stride_kv_rope_h, stride_kv_rope_d, stride_mid_ob, stride_mid_oh,\n    stride_mid_os, stride_mid_od, stride_mid_o_eb, stride_mid_o_eh,\n    stride_mid_o_es, Q_HEAD_NUM: tl.constexpr, BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_ROPE_DMODEL: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    seq_start_block = tl.program_id(0)\n    cur_q_head = tl.program_id(1)\n    cur_batch = tl.program_id(2)\n    cur_q_head_offs = tl.arange(0, Q_HEAD_NUM)\n    cur_q_head_range = cur_q_head * Q_HEAD_NUM + cur_q_head_offs\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_rope_d = tl.arange(0, BLOCK_ROPE_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_q_bs + cur_q_head_range[:, None\n        ] * stride_q_h + offs_d[None, :]\n    off_rope_q = cur_batch * stride_q_rope_bs + cur_q_head_range[:, None\n        ] * stride_q_rope_h + offs_rope_d[None, :]\n    q = tl.load(Q_nope + off_q)\n    q_rope = tl.load(Q_rope + off_rope_q)\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    sum_exp = tl.zeros([Q_HEAD_NUM], dtype=tl.float32)\n    max_logic = tl.zeros([Q_HEAD_NUM], dtype=tl.float32) - float('inf')\n    acc = tl.zeros([Q_HEAD_NUM, BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_kv = kv_loc[None, :] * stride_kv_bs + offs_d[:, None]\n        kv = tl.load(KV_nope + off_kv, mask=offs_n_new[None, :] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.dot(q, kv)\n        off_rope_kv = kv_loc[None, :] * stride_kv_rope_bs + offs_rope_d[:, None\n            ]\n        rope_kv = tl.load(KV_rope + off_rope_kv, mask=offs_n_new[None, :] <\n            cur_batch_end_index, other=0.0)\n        att_value += tl.dot(q_rope, rope_kv)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new[None, :] < cur_batch_end_index,\n            att_value, float('-inf'))\n        cur_max_logic = tl.max(att_value, axis=1)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic[:, None])\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale[:, None]\n        acc += tl.dot(exp_logic.to(kv.dtype), tl.trans(kv))\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=1)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = cur_batch * stride_mid_ob + cur_q_head_range[:, None\n            ] * stride_mid_oh + seq_start_block * stride_mid_os + offs_d[\n            None, :]\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + \n            cur_q_head_range * stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp[:, None])\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_head = tl.program_id(0)\n    cur_batch = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q_nope, Q_rope, KV_nope, KV_rope, sm_scale, B_Start_Loc,\n    B_Seqlen, Out, Req_to_tokens, B_req_idx, stride_q_bs, stride_q_h,\n    stride_q_d, stride_q_rope_bs, stride_q_rope_h, stride_q_rope_d,\n    stride_kv_bs, stride_kv_h, stride_kv_d, stride_kv_rope_bs,\n    stride_kv_rope_h, stride_kv_rope_d, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num,\n    b_prompt_cache_len, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_ROPE_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_rope_d = tl.arange(0, BLOCK_ROPE_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_q_bs + cur_head * stride_q_h + offs_d[None, :] * stride_q_d\n    off_q_rope = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_q_rope_bs + cur_head * stride_q_rope_h + offs_rope_d[None, :\n        ] * stride_q_rope_d\n    q = tl.load(Q_nope + off_q, mask=offs_m[:, None] < cur_batch_seq_len,\n        other=0.0)\n    q_rope = tl.load(Q_rope + off_q_rope, mask=offs_m[:, None] <\n        cur_batch_seq_len, other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, \n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_kv = kv_loc[None, :\n            ] * stride_kv_bs + cur_kv_head * stride_kv_h + offs_d[:, None\n            ] * stride_kv_d\n        off_kv_rope = kv_loc[None, :\n            ] * stride_kv_rope_bs + cur_kv_head * stride_kv_rope_h + offs_rope_d[\n            :, None] * stride_kv_rope_d\n        kv = tl.load(KV_nope + off_kv, mask=start_n + offs_n[None, :] <\n            block_end_loc, other=0.0)\n        kv_rope = tl.load(KV_rope + off_kv_rope, mask=start_n + offs_n[None,\n            :] < block_end_loc, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, kv)\n        qk += tl.dot(q_rope, kv_rope)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n,\n            acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        v = tl.trans(kv)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_no_prompt_cache(Q_nope, Q_rope, KV_nope, KV_rope, sm_scale,\n    B_Start_Loc, B_Seqlen, Out, stride_q_bs, stride_q_h, stride_q_d,\n    stride_q_rope_bs, stride_q_rope_h, stride_q_rope_d, stride_kv_bs,\n    stride_kv_h, stride_kv_d, stride_kv_rope_bs, stride_kv_rope_h,\n    stride_kv_rope_d, stride_obs, stride_oh, stride_od, kv_group_num,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_ROPE_DMODEL:\n    tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_kv_head = 0\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_rope_d = tl.arange(0, BLOCK_ROPE_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_q_bs + cur_head * stride_q_h + offs_d[None, :] * stride_q_d\n    off_rope_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_q_rope_bs + cur_head * stride_q_rope_h + offs_rope_d[None, :\n        ] * stride_q_rope_d\n    off_kv = offs_n[None, :\n        ] * stride_kv_bs + cur_kv_head * stride_kv_h + offs_d[:, None\n        ] * stride_kv_d\n    off_rope_kv = offs_n[None, :\n        ] * stride_kv_rope_bs + cur_kv_head * stride_kv_rope_h + offs_rope_d[\n        :, None] * stride_kv_rope_d\n    q = tl.load(Q_nope + off_q, mask=offs_m[:, None] < cur_batch_seq_len,\n        other=0.0)\n    q_rope = tl.load(Q_rope + off_rope_q, mask=offs_m[:, None] <\n        cur_batch_seq_len, other=0.0)\n    kv_ptrs = KV_nope + off_kv\n    kv_rope_ptrs = KV_rope + off_rope_kv\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv = tl.load(kv_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kv_bs, mask=start_n + offs_n[None, :] <\n            cur_batch_seq_len, other=0.0)\n        kv_rope = tl.load(kv_rope_ptrs + (cur_batch_in_all_start_index +\n            start_n) * stride_kv_rope_bs, mask=start_n + offs_n[None, :] <\n            cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, kv)\n        qk += tl.dot(q_rope, kv_rope)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.trans(kv)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, Req_to_tokens, B_req_idx, B_Start_Loc,\n    B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, stride_logic_h,\n    stride_logic_bs, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_token_b, stride_req_to_token_s,\n    other_kv_index, kv_group_num, sliding_window, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    cur_cache_start_loc = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_token_b + (cur_cache_start_loc + start_n + offs_n\n            ) * stride_req_to_token_s, mask=start_n + offs_n <\n            cur_att_seq_len, other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_att_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, sliding_window, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.\n    constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_att_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n +\n            cur_batch_start_index < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] + cur_batch_start_index <\n            cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, Att_Out,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_qbs, stride_qh,\n    stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, sliding_window, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_att_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value = att_value.to(tl.float32)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, stride_vbs,\n    stride_vh, stride_vd, stride_obs, stride_oh, stride_od, kv_group_num,\n    sliding_window, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < cur_batch_seq_len,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        qk = tl.where(start_n + offs_n[None, :] > offs_m[:, None] -\n            sliding_window, qk, float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < cur_batch_seq_len,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_init_att_window_info(b_seq_len, b_att_seq_len, batch_size,\n    sliding_window, BLOCK_SIZE: tl.constexpr):\n    cur_index = tl.program_id(0)\n    cur_start = cur_index * BLOCK_SIZE\n    offsets = cur_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < batch_size\n    cur_seq_len = tl.load(b_seq_len + offsets, mask=mask)\n    b_att_seq_len_data = tl.minimum(cur_seq_len, sliding_window)\n    tl.store(b_att_seq_len + offsets, b_att_seq_len_data, mask=mask)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, x_stride0, x_stride1, y_stride0, y_stride1,\n    N, eps, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    Y += row * y_stride0\n    X += row * x_stride0\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols * x_stride1, mask=cols < N, other=0.0).to(tl.\n            float32)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols * y_stride1, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, Req_to_tokens, B_req_idx, B_Start_Loc,\n    B_Seqlen, stride_logic_h, stride_logic_bs, stride_vbs, stride_vh,\n    stride_vd, stride_obs, stride_oh, stride_od, stride_req_to_token_b,\n    stride_req_to_token_s, other_kv_index, kv_group_num, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_token_b + (start_n + offs_n) *\n            stride_req_to_token_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_g, stride_k_d, stride_o_bs,\n    stride_o_h, stride_o_g, stride_o_d, stride_os_bs, stride_os_h,\n    stride_os_g, group_size, BLOCK_GROUP_NUM: tl.constexpr, BLOCK_GROUP_DIM:\n    tl.constexpr):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + cur_head * stride_k_h +\n        offs_g[:, None] * stride_k_g + offs_d[None, :], mask=offs_g[:, None\n        ] < group_size, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.\n        element_ty)\n    q_src_data = (src_data / data_scale[:, None]).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[\n        :, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = (Out_scale + dest_index * stride_os_bs + cur_head *\n        stride_os_h + offs_g)\n    tl.store(o_ptrs, q_src_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_dequantize_kv(mem_kv_buffer, mem_kv_scale,\n    req_to_token_indexs, b_seq_len, b_req_idx, Out, stride_kv_b,\n    stride_kv_h, stride_kv_g, stride_kv_d, stride_o_bh, stride_o_l,\n    stride_o_g, stride_o_d, stride_s_b, stride_s_h, stride_s_g,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, group_size, head_num:\n    tl.constexpr, BLOCK_SIZE: tl.constexpr, BLOCK_GROUP_NUM: tl.constexpr,\n    BLOCK_GROUP_DIM: tl.constexpr):\n    cur_group = tl.program_id(0)\n    start_m = tl.program_id(1)\n    cur_bh = tl.program_id(2)\n    cur_batch = cur_bh // head_num\n    cur_head = cur_bh % head_num\n    block_start_loc = BLOCK_SIZE * start_m\n    cur_batch_req_idx = tl.load(b_req_idx + cur_batch)\n    cur_seq_len = tl.load(b_seq_len + cur_batch)\n    offs_kv_loc = block_start_loc + tl.arange(0, BLOCK_SIZE)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM)\n    kv_loc = tl.load(req_to_token_indexs + cur_batch_req_idx *\n        stride_req_to_tokens_b + offs_kv_loc, mask=offs_kv_loc < cur_seq_len)\n    offs_kv = (kv_loc[:, None] * stride_kv_b + cur_head * stride_kv_h + \n        cur_group * stride_kv_g + offs_d[None, :])\n    src_data = tl.load(mem_kv_buffer + offs_kv, mask=offs_kv_loc[:, None] <\n        cur_seq_len, other=0.0).to(Out.dtype.element_ty)\n    s_ptrs = (mem_kv_scale + kv_loc * stride_s_b + cur_head * stride_s_h + \n        cur_group * stride_s_g)\n    data_scale = tl.load(s_ptrs, mask=offs_kv_loc < cur_seq_len)\n    out_data = src_data * data_scale[:, None]\n    o_ptrs = Out + cur_bh * stride_o_bh + offs_kv_loc[:, None\n        ] * stride_o_l + cur_group * stride_o_g + offs_d[None, :]\n    tl.store(o_ptrs, out_data, mask=offs_kv_loc[:, None] < cur_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, Q_HEAD_NUM: tl.\n    constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_kv_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_q_head_offs = tl.arange(0, Q_HEAD_NUM)\n    cur_q_head_range = cur_kv_head * gqa_group_size + cur_q_head_offs\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_q_head_range[:, None\n        ] * stride_qh + offs_d[None, :]\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q, mask=cur_q_head_range[:, None] < (cur_kv_head + \n        1) * gqa_group_size, other=0.0)\n    sum_exp = tl.zeros([Q_HEAD_NUM], dtype=tl.float32)\n    max_logic = tl.zeros([Q_HEAD_NUM], dtype=tl.float32) - float('inf')\n    acc = tl.zeros([Q_HEAD_NUM, BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            :, None]\n        k = tl.load(K + off_k, mask=offs_n_new[None, :] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.dot(q, k)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new[None, :] < cur_batch_end_index,\n            att_value, float('-inf'))\n        v = tl.load(V + k_loc[:, None] * stride_kbs + cur_kv_head *\n            stride_kh + offs_d[None, :], mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        cur_max_logic = tl.max(att_value, axis=1)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic[:, None])\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale[:, None]\n        acc += tl.dot(exp_logic.to(v.dtype), v)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=1)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = cur_batch * stride_mid_ob + cur_q_head_range[:, None\n            ] * stride_mid_oh + seq_start_block * stride_mid_os + offs_d[\n            None, :]\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + \n            cur_q_head_range * stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp[:, None], mask=\n            cur_q_head_range[:, None] < (cur_kv_head + 1) * gqa_group_size)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp), mask=cur_q_head_range < (cur_kv_head + 1) *\n            gqa_group_size)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef embedding_kernel(weight, input_ids, out, vob_start_id, vob_end_id,\n    stride_weight_seq, stride_out_seq, n_ctx, hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_NN: tl.constexpr):\n    start_n = tl.program_id(0) * BLOCK_N\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=\n            vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq +\n            offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :],\n            vecs, mask=store_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _silu_and_mul_kernel(input_ptr, output_ptr, stride_input_m,\n    stride_input_n, stride_output_m, stride_output_n, size_m, size_n,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    stride_input_m = stride_input_m.to(tl.int64)\n    stride_output_m = stride_output_m.to(tl.int64)\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0).to(tl.\n        float32)\n    gate = gate / (1 + tl.exp(-gate))\n    gate = gate.to(input_ptr.dtype.element_ty)\n    tl.store(output_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Req_to_tokens, B_req_idx, B_seqlen, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num,\n    Q_HEAD_NUM: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr\n    ):\n    cur_batch = tl.program_id(0)\n    cur_kv_head = tl.program_id(1)\n    cur_q_head_offs = tl.arange(0, Q_HEAD_NUM)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_seq_len = tl.load(B_seqlen + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_q_head_range = cur_kv_head * kv_group_num + cur_q_head_offs\n    off_q = cur_batch * stride_qbs + cur_q_head_range[:, None\n        ] * stride_qh + offs_d[None, :]\n    off_k = cur_kv_head * stride_kh + offs_d[:, None]\n    off_v = cur_kv_head * stride_vh + offs_d[None, :]\n    q = tl.load(Q + off_q, mask=cur_q_head_range[:, None] < (cur_kv_head + \n        1) * kv_group_num, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([Q_HEAD_NUM], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([Q_HEAD_NUM], dtype=tl.float32)\n    acc = tl.zeros([Q_HEAD_NUM, BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_tokens_b + start_n + offs_n, mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + kv_loc[None, :] * stride_kbs, mask=start_n +\n            offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([Q_HEAD_NUM, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(cur_batch_seq_len - 1 >= start_n + offs_n[None, :],\n            qk, float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + kv_loc[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = cur_batch * stride_obs + cur_q_head_range[:, None\n        ] * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=cur_q_head_range[:, None] < (cur_kv_head +\n        1) * kv_group_num)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, BLOCK_SEQ: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, BLOCK_HEAD:\n    tl.constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2)\n    dim_range1 = tl.arange(BLOCK_DMODEL // 2, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos - k1 * sin\n    out_k1 = k0 * sin + k1 * cos\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    stride_ph, stride_pbs, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_ph, stride_pbs, stride_vbs, stride_vh,\n    stride_vd, stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh,\n    stride_od, kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr\n    ):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_kv_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Req_to_tokens, B_req_idx,\n    B_split_start_loc, B_split_ready_cache_len, B_seqlen, Out, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, stride_vbs,\n    stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num, BLOCK_M:\n    tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_q_split_start_loc = tl.load(B_split_start_loc + cur_batch)\n    cur_batch_seq_start = tl.load(B_split_ready_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_seqlen + cur_batch)\n    cur_batch_q_split_seq_len = cur_batch_seq_len - cur_batch_seq_start\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :]\n    off_k = cur_kv_head * stride_kh + offs_d[:, None]\n    off_v = cur_kv_head * stride_vh + offs_d[None, :]\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_q_split_seq_len,\n        other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(start_m * BLOCK_M < cur_batch_q_split_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (cur_batch_seq_start + (start_m + \n        1) * BLOCK_M), BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_tokens_b + start_n + offs_n, mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + kv_loc[None, :] * stride_kbs, mask=start_n +\n            offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(cur_batch_seq_start + offs_m[:, None] >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + kv_loc[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_q_split_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_int8(Q, K, K_scale, V, V_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_split_start_loc, B_split_ready_cache_len, B_seqlen, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, stride_vbs, stride_vh, stride_vd,\n    stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num, BLOCK_M:\n    tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_q_split_start_loc = tl.load(B_split_start_loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_seqlen + cur_batch)\n    cur_batch_seq_start = tl.load(B_split_ready_cache_len + cur_batch)\n    cur_batch_q_split_seq_len = cur_batch_seq_len - cur_batch_seq_start\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :]\n    off_k = cur_kv_head * stride_kh + offs_d[:, None]\n    off_v = cur_kv_head * stride_vh + offs_d[None, :]\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_q_split_seq_len,\n        other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    ks_ptrs = K_scale + cur_kv_head * stride_ksh\n    vs_ptrs = V_scale + cur_kv_head * stride_vsh\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(start_m * BLOCK_M < cur_batch_q_split_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (cur_batch_seq_start + (start_m + \n        1) * BLOCK_M), BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_tokens_b + start_n + offs_n, mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + kv_loc[None, :] * stride_kbs, mask=start_n +\n            offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        k_scale = tl.load(ks_ptrs + kv_loc[None, :] * stride_ksbs, mask=\n            start_n + offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k_scale * k)\n        qk *= sm_scale\n        qk = tl.where(cur_batch_seq_start + offs_m[:, None] >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + kv_loc[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        v_scale = tl.load(vs_ptrs + kv_loc[:, None] * stride_vsbs, mask=(\n            start_n + offs_n)[:, None] < cur_batch_seq_len, other=0.0)\n        p = p.to(V.dtype.element_ty)\n        acc += tl.dot(p, v.to(V.dtype.element_ty) * v_scale)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_q_split_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, att_stride_h, att_stride_bs, kv_group_num,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value = att_value.to(tl.float32)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_ksbs, stride_ksh, stride_ksd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.\n    constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_kv_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_int4_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_g, stride_k_d, stride_o_bs,\n    stride_o_h, stride_o_g, stride_o_d, stride_os_bs, stride_os_h,\n    stride_os_g, group_size, BLOCK_GROUP_NUM: tl.constexpr, BLOCK_GROUP_DIM:\n    tl.constexpr):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM // 2)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data_0 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    src_data_1 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2 + 1,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    abs_data_0 = tl.abs(src_data_0)\n    abs_data_1 = tl.abs(src_data_1)\n    data_scale = (tl.maximum(tl.max(abs_data_0, axis=1), tl.max(abs_data_1,\n        axis=1)) / 7.0).to(Out_scale.dtype.element_ty)\n    q_src_data_0 = (src_data_0 / data_scale[:, None]).to(tl.int8)\n    q_src_data_0 = tl.where(q_src_data_0 > 7, 7, q_src_data_0)\n    q_src_data_0 = tl.where(q_src_data_0 < -7, -7, q_src_data_0)\n    q_src_data_1 = (src_data_1 / data_scale[:, None]).to(tl.int8)\n    q_src_data_1 = tl.where(q_src_data_1 > 7, 7, q_src_data_1)\n    q_src_data_1 = tl.where(q_src_data_1 < -7, -7, q_src_data_1)\n    low_4 = (q_src_data_0 & 128) >> 4 | q_src_data_0 & 15\n    high_4 = ((q_src_data_1 & 128) >> 4 | q_src_data_1 & 15) << 4\n    out_data = low_4 | high_4\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[\n        :, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = (Out_scale + dest_index * stride_os_bs + cur_head *\n        stride_os_h + offs_g)\n    tl.store(o_ptrs, out_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Out, B_Start_Loc, B_Seqlen,\n    Req_to_tokens, B_req_idx, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    kv_group_num, b_prompt_cache_len, H: tl.constexpr, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len,\n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_k = kv_loc[None, :\n            ] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=start_n + offs_n[None, :] <\n            block_end_loc, other=0.0)\n        qk = tl.dot(q, k)\n        mask = offs_m[:, None] + prompt_cache_len >= start_n + offs_n[None, :]\n        qk = tl.where(mask, qk * sm_scale, -100000000.0)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        off_v = kv_loc[:, None\n            ] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=start_n + offs_n[:, None] <\n            block_end_loc, other=0.0)\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n    acc = acc / l_i[:, None]\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_no_prompt_cache(Q, K, V, sm_scale, Out, B_Start_Loc,\n    B_Seqlen, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh,\n    stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh,\n    stride_od, kv_group_num, H, BLOCK_DMODEL: tl.constexpr, BLOCK_M: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M, cur_batch_seq_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < block_end_loc, other=0\n            )\n        qk = tl.dot(q, k)\n        mask = offs_m[:, None] >= start_n + offs_n[None, :]\n        qk = tl.where(mask, qk * sm_scale, -100000000.0)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < block_end_loc,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n    acc = acc / l_i[:, None]\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_int8kv(Q, K, V, sm_scale, Out, B_Start_Loc, B_Seqlen,\n    b_prompt_cache_len, stride_qbs, stride_qh, stride_qd, stride_kb,\n    stride_kh, stride_ks, stride_kd, stride_vb, stride_vh, stride_vs,\n    stride_vd, stride_obs, stride_oh, stride_od, kv_group_num, H: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n    cur_kv_head = cur_head // kv_group_num\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len,\n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        off_k = cur_batch * stride_kb + (start_n + offs_n[None, :]\n            ) * stride_ks + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=start_n + offs_n[None, :] <\n            block_end_loc, other=0.0)\n        qk = tl.dot(q, k)\n        mask = offs_m[:, None] + prompt_cache_len >= start_n + offs_n[None, :]\n        qk = tl.where(mask, qk * sm_scale, -100000000.0)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        off_v = cur_batch * stride_vb + (start_n + offs_n[:, None]\n            ) * stride_vs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=start_n + offs_n[:, None] <\n            block_end_loc, other=0.0)\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n    acc = acc / l_i[:, None]\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Prompt_ids, Text_weight_embs, Img_embs, Out, Img_token_lens,\n    Img_start_token_ids, Img_start_locs, stride_text_emb_s,\n    stride_text_emb_d, stride_img_emb_s, stride_img_emb_d, stride_out_s,\n    stride_out_d, tp_text_start_token_id, tp_text_end_token_id, hidden_size,\n    BLOCK_HIDDEN_DIM: tl.constexpr):\n    seq_index = tl.program_id(0).to(tl.int64)\n    img_handle_id = tl.program_id(1)\n    token_id = tl.load(Prompt_ids + seq_index)\n    off_d = tl.arange(0, BLOCK_HIDDEN_DIM)\n    for _ in range(0, tl.where((img_handle_id == 0) & (token_id <\n        tp_text_end_token_id) & (token_id >= tp_text_start_token_id), 1, 0), 1\n        ):\n        load_emb = tl.load(Text_weight_embs + stride_text_emb_s * (token_id -\n            tp_text_start_token_id) + off_d * stride_text_emb_d, mask=off_d <\n            hidden_size, other=0)\n        tl.store(Out + stride_out_s * seq_index + stride_out_d * off_d,\n            load_emb, mask=off_d < hidden_size)\n    img_start_token_id = tl.load(Img_start_token_ids + img_handle_id - 1,\n        mask=img_handle_id >= 1, other=0)\n    img_start_loc = tl.load(Img_start_locs + img_handle_id - 1, mask=\n        img_handle_id >= 1, other=0)\n    img_token_len = tl.load(Img_token_lens + img_handle_id - 1, mask=\n        img_handle_id >= 1, other=0)\n    for _ in range(0, tl.where((img_handle_id != 0) & (token_id >=\n        img_start_token_id) & (token_id < img_start_token_id +\n        img_token_len), 1, 0), 1):\n        load_emb = tl.load(Img_embs + stride_img_emb_s * (img_start_loc +\n            token_id - img_start_token_id) + off_d * stride_img_emb_d, mask\n            =off_d < hidden_size, other=0)\n        tl.store(Out + stride_out_s * seq_index + stride_out_d * off_d,\n            load_emb, mask=off_d < hidden_size)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_copy_kv_index_to_req(req_to_token_indexs, b_req_idx,\n    b_split_seq_len, cumsum_split_seq_len, b_seq_len, memindex,\n    stride_req_to_token_b, stride_req_to_token_s, BLOCK_M: tl.constexpr):\n    cur_index = tl.program_id(0)\n    cur_req_idx = tl.load(b_req_idx + cur_index)\n    q_split_len = tl.load(b_split_seq_len + cur_index)\n    q_mem_end = tl.load(cumsum_split_seq_len + cur_index)\n    q_mem_start = q_mem_end - q_split_len\n    store_end = tl.load(b_seq_len + cur_index)\n    store_start = store_end - q_split_len\n    off_m = tl.arange(0, BLOCK_M)\n    for block_start in range(0, q_split_len, BLOCK_M):\n        read_index = tl.load(memindex + q_mem_start + block_start + off_m,\n            mask=q_mem_start + block_start + off_m < q_mem_end, other=0)\n        tl.store(req_to_token_indexs + cur_req_idx * stride_req_to_token_b +\n            (block_start + store_start + off_m), read_index, mask=\n            block_start + store_start + off_m < store_end)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, BLOCK_DMODEL:\n    tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=offs_h[:, None] <\n        head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.\n        element_ty)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_copy_kv_index_to_req(req_to_token_indexs, b_req_idx,\n    b_seq_len, memindex, stride_req_to_token_b, stride_req_to_token_s):\n    cur_index = tl.program_id(0)\n    cur_req_idx = tl.load(b_req_idx + cur_index)\n    cur_token_index = tl.load(memindex + cur_index)\n    cur_seq_len = tl.load(b_seq_len + cur_index)\n    dest_offset = req_to_token_indexs + cur_req_idx * stride_req_to_token_b + (\n        cur_seq_len - 1) * stride_req_to_token_s\n    tl.store(dest_offset, cur_token_index)\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\n    tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, bs_ptr, bzp_ptr, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_bsk,\n    stride_bsn, stride_bzpk, stride_bzpn, group_size, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr):\n    \"\"\"\n    assert K % (BLOCK_SIZE_K * SPLIT_K) == 0\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] // 8 * stride_bk + offs_bn[None, :\n        ] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        bs_ptrs = bs_ptr + (offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K\n            ) // group_size * stride_bsk + offs_bn[None, :] * stride_bsn\n        bzp_ptrs = bzp_ptr + (offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K\n            ) // group_size * stride_bzpk + offs_bn[None, :] // 8 * stride_bzpn\n        b_shift_bits = offs_k[:, None] % 8 * 4\n        bzp_shift_bits = offs_bn[None, :] % 8 * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n        int_b = b >> b_shift_bits & 15\n        int_bzp = bzp >> bzp_shift_bits & 15\n        b = ((int_b - int_bzp) * bs).to(a.dtype)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5,\n    num_warps=2), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,\n    num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr, K, N,\n    group_size, stride_bk, stride_bn, stride_bsk, stride_bsn, stride_bzpk,\n    stride_bzpn, stride_fpbk, stride_fpbn, BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = offs_k[:, None] // 8 * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = offs_k[:, None] // group_size * stride_bzpk + offs_n[None, :\n        ] // 8 * stride_bzpn\n    bs_offs = offs_k[:, None] // group_size * stride_bsk + offs_n[None, :\n        ] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = offs_k[:, None] % 8 * 4\n    bzp_shift = offs_n[None, :] % 8 * 4\n    fp_weight = ((int32_b >> b_shift & 15) - (zp_b >> bzp_shift & 15)\n        ) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len,\n    stride_logit_b, stride_logit_s, BLOCK_P: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_repetition = tl.load(repetition_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    rep_logits = tl.where(cur_logits > 0, cur_logits / cur_repetition, \n        cur_logits * cur_repetition)\n    freq_logits = rep_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = (fpa / a_scale[:, None]).to(tl.int8)\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.\n    constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(\n        c_ptr.dtype.element_ty)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS: tl.constexpr,\n    CLASS_INDICES: tl.constexpr, LABEL_SMOOTHING: tl.constexpr,\n    IGNORE_INDEX: tl.constexpr, BUFFER_DTYPE: tl.constexpr, BLOCK: tl.constexpr\n    ):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev.to(buffer_dtype)\n    l_prev = l_prev.to(buffer_dtype)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf')).to(buffer_dtype)\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    if LABEL_SMOOTHING:\n        sum_total = 0.0\n        sum_total = sum_total.to(buffer_dtype)\n        weights_total = 0.0\n        weights_total = weights_total.to(buffer_dtype)\n        if WEIGHTS:\n            weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev).to(buffer_dtype)\n        if LABEL_SMOOTHING and WEIGHTS:\n            full_weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        if LABEL_SMOOTHING and WEIGHTS:\n            log_softmax *= full_weights_val\n        if LABEL_SMOOTHING:\n            sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        if LABEL_SMOOTHING and WEIGHTS:\n            weight_ptr += BLOCK\n    idx = tl.load(IDX + row)\n    use_class = 0.0\n    if IGNORE_INDEX >= 0:\n        use_class = idx == IGNORE_INDEX\n    READ_PROBS = PROBS + row * N + idx\n    tl.debug_barrier()\n    probs = tl.load(READ_PROBS)\n    if WEIGHTS and not LABEL_SMOOTHING:\n        weight_ptr = weight + idx\n        weights_val = tl.load(weight_ptr)\n        probs = weights_val * probs\n    if LABEL_SMOOTHING:\n        tl.store(WEIGHT_BUFFER + row, weights_total)\n        probs = (1 - smoothing_factor\n            ) * probs + smoothing_factor * sum_total / N\n    probs = probs * (1.0 - use_class)\n    tl.store(LOSS + row, probs)\n"
    },
    {
      "input": "@triton.jit\ndef _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: tl.constexpr, CLASS_INDICES:\n    tl.constexpr, LABEL_SMOOTHING: tl.constexpr, IGNORE_INDEX: tl.constexpr,\n    BUFFER_DTYPE: tl.constexpr, BLOCK: tl.constexpr):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev.to(buffer_dtype)\n    l_prev = l_prev.to(buffer_dtype)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf')).to(buffer_dtype)\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    sum_total = 0.0\n    weights_total = 0.0\n    sum_total = sum_total.to(buffer_dtype)\n    weights_total = weights_total.to(buffer_dtype)\n    idx_ptr = IDX + row * N + cols\n    if WEIGHTS:\n        weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev).to(buffer_dtype)\n        idx = tl.load(idx_ptr, mask=cols < N - start_n * BLOCK, other=0.0)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        if WEIGHTS:\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            full_weights_val = weights_val * full_weights_val\n        else:\n            full_weights_val = tl.where(cols < N - start_n * BLOCK,\n                full_weights_val, 0.0)\n        weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        log_softmax *= full_weights_val\n        sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        idx_ptr += BLOCK\n        if WEIGHTS:\n            weight_ptr += BLOCK\n    tl.store(WEIGHT_BUFFER + row, weights_total)\n    probs = sum_total\n    tl.store(LOSS + row, probs)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=\n    FORWARD_NUM_STAGES, num_warps=1), triton.Config({'BLOCK': 2048},\n    num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({'BLOCK': \n    4096}, num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({\n    'BLOCK': 8192}, num_stages=FORWARD_NUM_STAGES, num_warps=16), triton.\n    Config({'BLOCK': 16384}, num_stages=FORWARD_NUM_STAGES, num_warps=16)],\n    key=['N', 'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: tl.constexpr, CLASS_INDICES:\n    tl.constexpr, LABEL_SMOOTHING: tl.constexpr, IGNORE_INDEX: tl.constexpr,\n    BUFFER_DTYPE: tl.constexpr, BLOCK: tl.constexpr):\n    if CLASS_INDICES:\n        _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n    else:\n        _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=1,\n    num_warps=1), triton.Config({'BLOCK': 2048}, num_stages=1, num_warps=8),\n    triton.Config({'BLOCK': 4096}, num_stages=1, num_warps=8), triton.\n    Config({'BLOCK': 8192}, num_stages=1, num_warps=16), triton.Config({\n    'BLOCK': 16384}, num_stages=1, num_warps=16)], key=['N',\n    'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _backward(PROBS, IDX, DPROBS, dprob_stride, DIN, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS: tl.constexpr,\n    CLASS_INDICES: tl.constexpr, LABEL_SMOOTHING: tl.constexpr,\n    IGNORE_INDEX: tl.constexpr, BUFFER_DTYPE: tl.constexpr, BLOCK: tl.constexpr\n    ):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    start_n = tl.program_id(1)\n    cols = tl.arange(0, BLOCK)\n    PROBS = PROBS + row * N\n    probs_start = PROBS + cols + BLOCK * start_n\n    probs = -tl.load(probs_start, mask=cols < N - start_n * BLOCK, other=\n        float('inf')).to(buffer_dtype)\n    DIN = DIN + row * N + cols + BLOCK * start_n\n    dout = tl.load(DPROBS + row * dprob_stride).to(buffer_dtype)\n    if CLASS_INDICES:\n        idx = tl.load(IDX + row)\n        delta = start_n * BLOCK + cols == idx\n        if IGNORE_INDEX >= 0:\n            use_class = idx == IGNORE_INDEX\n            dout = dout * (1 - use_class)\n        if LABEL_SMOOTHING:\n            if WEIGHTS:\n                weight_ptr = weight + cols + BLOCK * start_n\n                full_weights_val = tl.load(weight_ptr, mask=cols < N - \n                    start_n * BLOCK, other=0.0).to(buffer_dtype)\n                weights_val = tl.load(weight + idx)\n                probs = probs / full_weights_val\n            probs = tl.exp(probs)\n            if WEIGHTS:\n                weights_total = tl.load(WEIGHT_BUFFER + row)\n                numerator_contrib = weights_val * (1.0 - smoothing_factor) * (\n                    probs - delta)\n                mean_contrib = (weights_total * probs - full_weights_val\n                    ) * smoothing_factor / N\n            else:\n                numerator_contrib = (1.0 - smoothing_factor) * (probs - delta)\n                mean_contrib = smoothing_factor * probs - smoothing_factor / N\n            din = (numerator_contrib + mean_contrib) * dout\n        else:\n            probs = tl.exp(probs)\n            din = (probs - delta) * dout\n            if WEIGHTS:\n                weight_ptr = weight + idx\n                weights_val = tl.load(weight_ptr)\n                din = weights_val * din\n    else:\n        idx = tl.load(IDX + row * N + cols + BLOCK * start_n, mask=cols < N -\n            start_n * BLOCK, other=0.0).to(buffer_dtype)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        weights_total = tl.load(WEIGHT_BUFFER + row)\n        if WEIGHTS:\n            weight_ptr = weight + cols + BLOCK * start_n\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0).to(buffer_dtype)\n            full_weights_val = weights_val * full_weights_val\n        probs = probs / full_weights_val\n        probs = tl.exp(probs.to(buffer_dtype))\n        weighted_probs = probs * weights_total\n        weighted_probs_per_class = weighted_probs - full_weights_val\n        din = weighted_probs_per_class * dout\n    tl.store(DIN, din.to(DIN.dtype.element_ty), mask=cols + BLOCK * start_n < N\n        )\n"
    },
    {
      "input": "@triton.jit\ndef triton_batch_lora_B(output, x, w, a_start, a_len, a_loc, batch_req_bins,\n    a_scaling, qkvo_offset: tl.constexpr, NUM_TOKENS: tl.constexpr, HIDDEN:\n    tl.constexpr, MAX_LORA_RANK: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    return\n"
    },
    {
      "input": "@triton.jit\ndef triton_batch_lora_B(output, x, w, a_start, a_len, a_loc, batch_req_bins,\n    a_scaling, qkvo_offset: tl.constexpr, NUM_TOKENS: tl.constexpr, HIDDEN:\n    tl.constexpr, MAX_LORA_RANK: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    return\n"
    },
    {
      "input": "@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location,\n    new_a_start, new_a_location, BLOCK_SIZE: tl.constexpr):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=\n            old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=\n            new_offset < length)\n"
    },
    {
      "input": "@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location,\n    new_a_start, new_a_location, BLOCK_SIZE: tl.constexpr):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=\n            old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=\n            new_offset < length)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs, stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od, stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.\n    constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len,\n            other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc.to(tl.float16)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.\n    constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, stride_vbs,\n    stride_vh, stride_vd, stride_obs, stride_oh, stride_od, kv_group_num,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < cur_batch_seq_len,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < cur_batch_seq_len,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, TMP, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    stride_tmp_b, stride_tmp_h, stride_tmp_s, kv_group_num, BLOCK_M: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = (TMP + cur_batch * stride_tmp_b + cur_head * stride_tmp_h + \n        offs_m * stride_tmp_s)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < cur_batch_seq_len,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < cur_batch_seq_len,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _expand_fwd_kernel(X, W, scale, B_Loc, B_Lora_Start_Loc, B_Lora_Ranks,\n    B_Start_Loc, B_Seqlen, B_Indicies, Out, qkvo, stride_xbs, stride_xh,\n    stride_wbs, stride_wh, stride_obs, stride_oh, BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_RANK: tl.\n    constexpr, TILE_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_tile = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_adapter = tl.load(B_Indicies + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_rank_size = tl.load(B_Lora_Ranks + cur_adapter) // 4\n    cur_batch_adapter_start_index = tl.load(B_Lora_Start_Loc + cur_adapter\n        ) + cur_batch_rank_size * qkvo\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_scale = tl.load(scale + cur_adapter)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_RANK)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_x = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_xbs + offs_d[None, :] * stride_xh\n    x = tl.load(X + off_x, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    for start_n in range(cur_tile * TILE_N, (cur_tile + 1) * TILE_N, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        w_loc = tl.load(B_Loc + cur_batch_adapter_start_index + (start_n +\n            offs_n) * cur_batch_rank_size // BLOCK_DMODEL, mask=start_n +\n            offs_n < BLOCK_DMODEL, other=0)\n        off_w = w_loc[None, :] * stride_wbs + ((start_n + offs_n) *\n            cur_batch_rank_size + offs_d[:, None]) % BLOCK_DMODEL * stride_wh\n        w = tl.load(W + off_w, mask=offs_d[:, None] < cur_batch_rank_size,\n            other=0.0)\n        off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n            ) * stride_obs + (start_n + offs_n[None, :]) * stride_oh\n        out_ptrs = Out + off_o\n        wx = tl.load(out_ptrs, mask=offs_m[:, None] < cur_batch_seq_len,\n            other=0.0)\n        wx += tl.dot(x, w) * cur_batch_scale\n        tl.store(out_ptrs, wx, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _shrink_fwd_kernel(X, W, B_Loc, B_Lora_Start_Loc, B_Lora_Ranks,\n    B_Start_Loc, B_Seqlen, B_Indicies, Out, qkvo, stride_xbs, stride_xh,\n    stride_wbs, stride_wh, stride_obs, stride_oh, BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    start_n = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_adapter = tl.load(B_Indicies + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_rank_size = tl.load(B_Lora_Ranks + cur_adapter) // 4\n    cur_batch_adapter_start_index = tl.load(B_Lora_Start_Loc + cur_adapter\n        ) + cur_batch_rank_size * qkvo\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_x = (cur_batch_in_all_start_index + offs_m) * stride_xbs\n    offs_k = tl.arange(0, BLOCK_K)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    w_loc = tl.load(B_Loc + cur_batch_adapter_start_index + offs_n, mask=\n        offs_n < cur_batch_rank_size, other=0)\n    off_w = w_loc * stride_wbs\n    wx = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    for start_k in range(0, BLOCK_DMODEL, BLOCK_K):\n        start_k = tl.multiple_of(start_k, BLOCK_K)\n        x = tl.load(X + off_x[:, None] + (start_k + offs_k[None, :]) *\n            stride_xh, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n        w = tl.load(W + off_w[None, :] + (start_k + offs_k[:, None]) *\n            stride_wh, mask=offs_n[None, :] < cur_batch_rank_size, other=0.0)\n        wx += tl.dot(x, w)\n    c = wx.to(tl.float16)\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + offs_n[None, :] * stride_oh\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, c, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y.to(tl.float16), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs, stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od, stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len,\n            other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_cosbs, stride_cosd, stride_sinbs, stride_sind, max_total_len, H,\n    BLOCK_HEAD: tl.constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.\n    constexpr):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2)\n    dim_range1 = tl.arange(BLOCK_DMODEL // 2, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc.to(tl.float16)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph,\n    stride_pbs, stride_vbs, stride_vh, stride_vd, stride_vsbs, stride_vsh,\n    stride_vsd, stride_obs, stride_oh, stride_od, BLOCK_DMODEL: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc.to(tl.float16)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, att_stride_h, att_stride_bs,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, stride_vbs,\n    stride_vh, stride_vd, stride_obs, stride_oh, stride_od, BLOCK_M: tl.\n    constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:,\n        None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < cur_batch_seq_len,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < cur_batch_seq_len,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, TMP, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    stride_tmp_b, stride_tmp_h, stride_tmp_s, BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:,\n        None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = (TMP + cur_batch * stride_tmp_b + cur_head * stride_tmp_h + \n        offs_m * stride_tmp_s)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=start_n + offs_n[None, :] < cur_batch_seq_len,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=start_n + offs_n[:, None] < cur_batch_seq_len,\n            other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, BLOCK_DMODEL:\n    tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num,\n    BLOCK_DMODEL: tl.constexpr, BLOCK_HEAD: tl.constexpr):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=offs_h[:, None] <\n        head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(tl.float16)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: tl.constexpr, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\n    tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, bs_ptr, bzp_ptr, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_bsk,\n    stride_bsn, stride_bzpk, stride_bzpn, group_size, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr):\n    \"\"\"\n    assert K % (BLOCK_SIZE_K * SPLIT_K) == 0\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] // 8 * stride_bk + offs_bn[None, :\n        ] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        bs_ptrs = bs_ptr + (offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K\n            ) // group_size * stride_bsk + offs_bn[None, :] * stride_bsn\n        bzp_ptrs = bzp_ptr + (offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K\n            ) // group_size * stride_bzpk + offs_bn[None, :] // 8 * stride_bzpn\n        b_shift_bits = offs_k[:, None] % 8 * 4\n        bzp_shift_bits = offs_bn[None, :] % 8 * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n        int_b = b >> b_shift_bits & 15\n        int_bzp = bzp >> bzp_shift_bits & 15\n        b = ((int_b - int_bzp) * bs).to(tl.float16)\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16))\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5,\n    num_warps=2), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,\n    num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr, K, N,\n    group_size, stride_bk, stride_bn, stride_bsk, stride_bsn, stride_bzpk,\n    stride_bzpn, stride_fpbk, stride_fpbn, BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = offs_k[:, None] // 8 * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = offs_k[:, None] // group_size * stride_bzpk + offs_n[None, :\n        ] // 8 * stride_bzpn\n    bs_offs = offs_k[:, None] // group_size * stride_bsk + offs_n[None, :\n        ] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = offs_k[:, None] % 8 * 4\n    bzp_shift = offs_n[None, :] % 8 * 4\n    fp_weight = ((int32_b >> b_shift & 15) - (zp_b >> bzp_shift & 15)\n        ) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, stride_logit_b,\n    stride_logit_s, BLOCK_P: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    freq_logits = cur_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = (fpa / a_scale[:, None]).to(tl.int8)\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.\n    constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(\n        tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, M, V, stride, N, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    X += row * stride\n    Y += row * stride\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    mean = tl.sum(x, axis=0) / N\n    xmean = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(xmean * xmean, axis=0) / N\n    rstd = 1 / tl.sqrt(var + 1e-05)\n    xhat = xmean * rstd\n    tl.store(M + row, mean)\n    tl.store(V + row, rstd)\n    w = tl.load(W + cols, mask=mask)\n    y = xhat * w\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, X, W, M, V, Lock, stride, N,\n    GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    mean = tl.load(M + row)\n    rstd = tl.load(V + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    mean1 = tl.sum(xhat * wdy, axis=0) / N\n    mean2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = (dy * xhat).to(w.dtype)\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dw(DW, FINAL_DW, M, N, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_kernel_forward(output_ptr, input_ptr, input_row_stride,\n    output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    causal_mask = col_offsets > row_idx % n_cols\n    row = row + tl.where(causal_mask, -float('inf'), 0.0)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_kernel_backward(output_ptr, input_ptr, grad_ptr,\n    grad_row_stride, input_row_stride, output_row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    grad_row_start_ptr = grad_ptr + row_idx * grad_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    grad_ptrs = grad_row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    probs_row = tl.load(input_ptrs, mask=mask, other=0.0)\n    grad_row = tl.load(grad_ptrs, mask=mask, other=0.0)\n    dxhat = probs_row * grad_row\n    softmax_grad_output = dxhat - probs_row * tl.sum(dxhat, axis=0)\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_grad_output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_inner(start_loop, end_loop, q, span_q, k_block_ptr,\n    v_block_ptr, bias_block_ptr, mask_block_ptr, k_start, k_end, seq_len_k,\n    acc, m_i, l_i, bias_advance: tl.constexpr, mask_advance: tl.constexpr,\n    is_causal: tl.constexpr, use_attention_mask: tl.constexpr, use_k_start:\n    tl.constexpr, use_k_end: tl.constexpr, use_bias: tl.constexpr, block_k:\n    tl.constexpr, use_mask_k: tl.constexpr, k_boundary_check: tl.constexpr,\n    v_boundary_check: tl.constexpr, dot_fn_qk: tl.constexpr, dot_fn_kv: tl.\n    constexpr):\n    \"\"\"Triton MHA forward kernel's inner loop.\"\"\"\n    for start_k in range(start_loop, end_loop, block_k):\n        start_k = tl.multiple_of(start_k, block_k)\n        span_k = start_k + tl.arange(0, block_k)\n        k = tl.load(k_block_ptr, boundary_check=k_boundary_check,\n            padding_option='zero' if len(k_boundary_check.value) else '')\n        v = tl.load(v_block_ptr, boundary_check=v_boundary_check,\n            padding_option='zero' if len(v_boundary_check.value) else '')\n        if use_bias:\n            bias = tl.load(bias_block_ptr)\n        qk = dot_fn_qk(q.to(k.dtype), k)\n        if use_bias:\n            qk = qk.to(tl.int32, bitcast=True) & 4294967295\n            qk = qk.to(tl.float32, bitcast=True)\n            qk += bias\n        if use_attention_mask | use_k_start | use_k_end:\n            mask_value = float(jnp.finfo(jnp.float32).min)\n        if use_attention_mask:\n            mask = tl.load(mask_block_ptr)\n            qk = tl.where(mask, qk, mask_value)\n        if use_k_start:\n            if tl.sum(k_start) != 0:\n                qk = tl.where(k_start[:, None] <= span_k[None, :], qk,\n                    mask_value)\n        if is_causal:\n            qk = tl.where(span_q[:, None] >= span_k[None, :], qk, float('-inf')\n                )\n        elif use_k_end:\n            qk = tl.where(k_end[:, None] > span_k[None, :], qk, mask_value)\n        if use_mask_k:\n            qk = tl.where((span_k < seq_len_k)[None, :], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        m_i = m_ij\n        acc *= alpha[:, None]\n        l_i *= alpha\n        l_i += tl.sum(p, axis=1)\n        acc += dot_fn_kv(p.to(v.dtype), v)\n        k_block_ptr = tl.advance(k_block_ptr, (0, block_k))\n        v_block_ptr = tl.advance(v_block_ptr, (block_k, 0))\n        bias_block_ptr = tl.advance(bias_block_ptr, bias_advance.value)\n        mask_block_ptr = tl.advance(mask_block_ptr, mask_advance.value)\n    return (k_block_ptr, v_block_ptr, bias_block_ptr, mask_block_ptr, acc,\n        m_i, l_i)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(q_ptr, k_ptr, v_ptr, bias_ptr, mask_ptr, k_start_ptr,\n    k_end_ptr, q_offset, k_offset, v_offset, q_stride_b, q_stride_s,\n    q_stride_h, q_stride_d, k_stride_b, k_stride_s, k_stride_h, k_stride_d,\n    v_stride_b, v_stride_s, v_stride_h, v_stride_d, bias_stride_b,\n    bias_stride_h, bias_stride_sq, bias_stride_sk, mask_stride_b,\n    mask_stride_h, mask_stride_sq, mask_stride_sk, k_start_stride_b,\n    k_start_stride_h, k_start_stride_sq, k_end_stride_b, k_end_stride_h,\n    k_end_stride_sq, o_stride_b, o_stride_s, o_stride_h, o_stride_d,\n    num_heads_q, num_heads_k, seq_len_q, seq_len_k, o_ptr, is_causal: tl.\n    constexpr, use_attention_mask: tl.constexpr, use_k_start: tl.constexpr,\n    use_k_end: tl.constexpr, use_bias: tl.constexpr, sm_scale: tl.constexpr,\n    block_q: tl.constexpr, block_k: tl.constexpr, head_dim: tl.constexpr,\n    use_mask_q: tl.constexpr, use_mask_k: tl.constexpr, bias_bcast_sq: tl.\n    constexpr, mask_bcast_sq: tl.constexpr, dot_fn_qk: tl.constexpr,\n    dot_fn_kv: tl.constexpr):\n    \"\"\"Triton MHA forward kernel.\"\"\"\n    block_d: tl.constexpr = jt.utils.next_power_of_2(head_dim.value)\n    start_q = tl.program_id(1) * block_q\n    off_h = tl.program_id(0)\n    off_b = tl.program_id(2)\n    off_h_k = off_h // (num_heads_q // num_heads_k)\n    q_ptr += off_h * q_stride_h + off_b * q_stride_b + q_offset\n    k_ptr += off_h_k * k_stride_h + off_b * k_stride_b + k_offset\n    v_ptr += off_h_k * v_stride_h + off_b * v_stride_b + v_offset\n    o_ptr += off_h * o_stride_h + off_b * o_stride_b\n    if use_bias:\n        bias_ptr += off_b * bias_stride_b + off_h * bias_stride_h\n    if use_attention_mask:\n        mask_ptr += off_b * mask_stride_b + off_h * mask_stride_h\n    if use_k_start:\n        k_start_ptr += off_b * k_start_stride_b + off_h * k_start_stride_h\n    if use_k_end:\n        k_end_ptr += off_b * k_end_stride_b + off_h * k_end_stride_h\n    q_block_ptr = tl.make_block_ptr(q_ptr, shape=(seq_len_q, head_dim),\n        strides=(q_stride_s, q_stride_d), offsets=(start_q, 0), block_shape\n        =(block_q, block_d), order=(1, 0))\n    k_block_ptr = tl.make_block_ptr(k_ptr, shape=(head_dim, seq_len_k),\n        strides=(k_stride_d, k_stride_s), offsets=(0, 0), block_shape=(\n        block_d, block_k), order=(0, 1))\n    v_block_ptr = tl.make_block_ptr(v_ptr, shape=(seq_len_k, head_dim),\n        strides=(v_stride_s, v_stride_d), offsets=(0, 0), block_shape=(\n        block_k, block_d), order=(1, 0))\n    q_boundary_check0: tl.constexpr = (0,) if use_mask_q else ()\n    q_boundary_check1: tl.constexpr = (1,) if head_dim != block_d else ()\n    q_boundary_check: tl.constexpr = q_boundary_check0 + q_boundary_check1\n    q_padding_option: tl.constexpr = 'zero' if len(q_boundary_check.value\n        ) else ''\n    k_boundary_check: tl.constexpr = (0,) if head_dim != block_d else ()\n    v_boundary_check: tl.constexpr = (0,) if use_mask_k else ()\n    bias_start_dim: tl.constexpr = 1 if bias_bcast_sq else 0\n    bias_block_ptr = tl.make_block_ptr(bias_ptr, shape=(seq_len_q,\n        seq_len_k)[bias_start_dim:], strides=(bias_stride_sq,\n        bias_stride_sk)[bias_start_dim:], offsets=(start_q, 0)[\n        bias_start_dim:], block_shape=(block_q, block_k)[bias_start_dim:],\n        order=(1, 0)[bias_start_dim:])\n    bias_advance: tl.constexpr = (0, block_k)[bias_start_dim:]\n    mask_start_dim: tl.constexpr = 1 if mask_bcast_sq else 0\n    mask_block_ptr = tl.make_block_ptr(mask_ptr, shape=(seq_len_q,\n        seq_len_k)[mask_start_dim:], strides=(mask_stride_sq,\n        mask_stride_sk)[mask_start_dim:], offsets=(start_q, 0)[\n        mask_start_dim:], block_shape=(block_q, block_k)[mask_start_dim:],\n        order=(1, 0)[mask_start_dim:])\n    mask_advance: tl.constexpr = (0, block_k)[mask_start_dim:]\n    k_start_block_ptr = tl.make_block_ptr(k_start_ptr, shape=(seq_len_q,),\n        strides=(k_start_stride_sq,), offsets=(start_q,), block_shape=(\n        block_q,), order=(0,))\n    k_end_block_ptr = tl.make_block_ptr(k_end_ptr, shape=(seq_len_q,),\n        strides=(k_end_stride_sq,), offsets=(start_q,), block_shape=(\n        block_q,), order=(0,))\n    span_q = start_q + tl.arange(0, block_q)\n    m_i = tl.full([block_q], float('-inf'), dtype=tl.float32)\n    l_i = tl.zeros([block_q], dtype=tl.float32)\n    acc = tl.zeros([block_q, block_d], dtype=tl.float32)\n    q = tl.load(q_block_ptr, boundary_check=q_boundary_check,\n        padding_option=q_padding_option)\n    q *= sm_scale\n    k_start = None\n    if use_k_start:\n        k_start = tl.load(k_start_block_ptr)\n        start_loop = tl.maximum(tl.min(k_start), 0)\n        blocks_to_skip = start_loop // block_k\n        start_loop = block_k * blocks_to_skip\n        for _ in range(blocks_to_skip):\n            k_block_ptr = tl.advance(k_block_ptr, (0, block_k))\n            v_block_ptr = tl.advance(v_block_ptr, (block_k, 0))\n            bias_block_ptr = tl.advance(bias_block_ptr, bias_advance.value)\n            mask_block_ptr = tl.advance(mask_block_ptr, mask_advance.value)\n    else:\n        start_loop = 0\n    k_end = None\n    if is_causal:\n        end_loop = tl.minimum(start_q // block_k * block_k, seq_len_k)\n    elif use_k_end:\n        k_end = tl.load(k_end_block_ptr)\n        end_loop = tl.minimum(tl.max(k_end), seq_len_k)\n    else:\n        end_loop = seq_len_k\n    (k_block_ptr, v_block_ptr, bias_block_ptr, mask_block_ptr, acc, m_i, l_i\n        ) = (_fwd_kernel_inner(start_loop, end_loop, q, span_q, k_block_ptr,\n        v_block_ptr, bias_block_ptr, mask_block_ptr, k_start, k_end,\n        seq_len_k, acc, m_i, l_i, bias_advance, mask_advance, False,\n        use_attention_mask, use_k_start, use_k_end, use_bias, block_k,\n        use_mask_k, k_boundary_check, v_boundary_check, dot_fn_qk, dot_fn_kv))\n    if is_causal:\n        tl.debug_barrier()\n        start_loop, end_loop = end_loop, tl.minimum(end_loop + block_k,\n            seq_len_k)\n        _, _, _, _, acc, _, l_i = _fwd_kernel_inner(start_loop, end_loop, q,\n            span_q, k_block_ptr, v_block_ptr, bias_block_ptr,\n            mask_block_ptr, k_start, k_end, seq_len_k, acc, m_i, l_i,\n            bias_advance, mask_advance, True, use_attention_mask,\n            use_k_start, use_k_end, use_bias, block_k, use_mask_k,\n            k_boundary_check, v_boundary_check, dot_fn_qk, dot_fn_kv)\n    l_i += float(jnp.finfo(jnp.float32).tiny)\n    acc /= l_i[:, None]\n    o_block_ptr = tl.make_block_ptr(o_ptr, shape=(seq_len_q, head_dim),\n        strides=(o_stride_s, o_stride_d), offsets=(start_q, 0), block_shape\n        =(block_q, block_d), order=(1, 0))\n    acc = acc.to(o_ptr.dtype.element_ty)\n    tl.store(o_block_ptr, acc, boundary_check=q_boundary_check)\n"
    },
    {
      "input": "@triton.jit\ndef _dot_tf32_f32_3x(a, b, trans_a=False, trans_b=False):\n    \"\"\"Perform the 3-pass tf32 dot function.\"\"\"\n    tl.static_assert(a.dtype == tl.float32)\n    tl.static_assert(b.dtype == tl.float32)\n    a_ = (a.to(tl.uint32, bitcast=True) & 4294959104).to(tl.float32,\n        bitcast=True)\n    b_ = (b.to(tl.uint32, bitcast=True) & 4294959104).to(tl.float32,\n        bitcast=True)\n    a_err = a - a_\n    b_err = b - b_\n    if trans_a:\n        a_ = tl.trans(a_)\n        a_err = tl.trans(a_err)\n    if trans_b:\n        b_ = tl.trans(b_)\n        b_err = tl.trans(b_err)\n    return tl.dot(a_, b_, out_dtype=tl.float32) + (tl.dot(a_, b_err,\n        out_dtype=tl.float32) + tl.dot(a_err, b_, out_dtype=tl.float32))\n"
    },
    {
      "input": "@triton.jit\ndef MSAFwdFused(v_si_ptr, b_ij_ptr, g_si_ptr, output_ptr, vw_ptr,\n    logsumexp_ptr, C_hidden, N_head, C_LEN_POW2: tl.constexpr, RES_LEN_POW2:\n    tl.constexpr, SEQ_LEN: tl.constexpr, RES_LEN: tl.constexpr,\n    BLOCK_SIZE_ROW: tl.constexpr, BLOCK_SIZE_COL: tl.constexpr,\n    BLOCK_SIZE_SEQ: tl.constexpr):\n    pid_z = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    z_off = pid_z.to(tl.int64)\n    h_off = pid_h.to(tl.int64)\n    i_off = pid_i.to(tl.int64) * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    offs_c = tl.arange(0, C_LEN_POW2)\n    log2_e = 1.44269504089\n    prev_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    new_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    l = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        new_row_max = tl.maximum(tl.max(b, axis=1, keep_dims=True),\n            prev_row_max)\n        w = tl.exp2(log2_e * (b - new_row_max))\n        l *= tl.exp2(log2_e * (prev_row_max - new_row_max))\n        l += tl.sum(w, axis=1, keep_dims=True)\n        for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n            for ch in range(0, C_hidden, 1):\n                offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n                si_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_i[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                sj_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_j[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[\n                    :, None]\n                sj_mask = (offs_s < SEQ_LEN)[None, :] & (offs_j < RES_LEN)[\n                    :, None]\n                v = tl.load(v_si_ptr + sj_off, sj_mask, 0)\n                vw = tl.load(output_ptr + si_off, si_mask, 0)\n                vw = vw * tl.exp2(log2_e * (prev_row_max - new_row_max))\n                vw = tl.dot(w, v, acc=vw)\n                tl.store(output_ptr + si_off, vw, si_mask)\n        prev_row_max = new_row_max\n    for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n        for ch in range(0, C_hidden, 1):\n            offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n            si_off = z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + offs_s[\n                None, :] * RES_LEN * N_head * C_hidden + offs_i[:, None\n                ] * N_head * C_hidden + h_off * C_hidden + ch\n            si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[:, None]\n            g = tl.load(g_si_ptr + si_off, si_mask, 0)\n            g = tl.sigmoid(g)\n            vw = tl.load(output_ptr + si_off, si_mask, 0)\n            vw = vw / l\n            out = g * vw\n            tl.store(output_ptr + si_off, out, si_mask)\n            tl.store(vw_ptr + si_off, vw, si_mask)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    tl.store(logsumexp_ptr + lse_off, new_row_max + tl.log(l), lse_mask)\n"
    },
    {
      "input": "@triton.jit\ndef MSABwdFused(b_ij_ptr, logsumexp_ptr, N_head, RES_LEN: tl.constexpr,\n    BLOCK_SIZE_ROW: tl.constexpr, BLOCK_SIZE_COL: tl.constexpr):\n    pid_zh = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_z = pid_zh // N_head\n    pid_h = pid_zh % N_head\n    log2_e = 1.44269504089\n    z_off = pid_z.to(tl.int64)\n    h_off = pid_h.to(tl.int64)\n    i_off = pid_i.to(tl.int64) * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    logsumexp = tl.load(logsumexp_ptr + lse_off, lse_mask, 0)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        b = tl.exp2(log2_e * (b - logsumexp))\n        tl.store(b_ij_ptr + b_offs, b, ij_mask)\n"
    },
    {
      "input": "@triton.jit\ndef attention_fwd_kernel(q, k, v, h, o, s_qh, s_qt, s_qd, s_hh, s_ht, T,\n    scale, BT: tl.constexpr, BD: tl.constexpr, NT: tl.constexpr, STORE: tl.\n    constexpr, IFCOND: tl.constexpr):\n    i_bh = tl.program_id(0)\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0,\n            i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd\n            ), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        if STORE:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty))\n        b_q = tl.load(p_q)\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k)\n        b_v = tl.load(p_v)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty))\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: tl.constexpr, IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr, INTERLEAVED: tl.constexpr, CONJUGATE: tl.\n    constexpr, BLOCK_M: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0).to(tl.float32)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n            ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_h(k, v, z, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: tl.constexpr, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT:\n    tl.constexpr, NORMK: tl.constexpr, USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    if NORMK:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_k *\n            BK,), (BK,), (0,))\n    else:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_v *\n            BV,), (BV,), (0,))\n    b_zp = tl.load(p_z0).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[:, None]\n            b_k = tl.exp(b_k - b_zc[:, None]).to(b_k.dtype)\n        else:\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[None, :]\n            b_v = tl.exp(b_v - b_zc[None, :]).to(b_v.dtype)\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_K(v, z, o, A, s_v_h, s_v_t, s_v_d, T: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.\n    constexpr, NC: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, tl.exp(b_v - b_zn[None, :]).to(b_v.dtype),\n            allow_tf32=False)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_o *= tl.exp(b_zn[None, :] - b_z)\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n        m_i = o_i[:, None] >= j\n        b_o += tl.where(m_i, b_A[:, None] * tl.exp(b_v[None, :] - b_z), 0)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_K(q, k, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_A += tl.dot(b_q, b_k, allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p * V +\n        i_v * BV,), (BV,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_zp[None, :] - b_z)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_V(q, k, z, A, s_k_h, s_k_t, s_k_d, scale, T:\n    tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK:\n    tl.constexpr, NC: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_q = (b_q * tl.exp(b_zn[None, :] - b_z) * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k = tl.exp(b_k - b_zn[:, None]).to(b_k.dtype)\n        b_A = tl.dot(b_q, b_k, allow_tf32=False)\n        tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)\n            b_A = tl.sum(b_q * tl.exp(b_k[None, :] - b_z) * scale, 1)\n            b_A = tl.where(o_i >= j, b_A, 0.0)\n            tl.store(A + o_A + j, b_A.to(b_q.dtype), mask=m_A)\n            p_k = tl.advance(p_k, (K,))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_V(q, v, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p *\n            K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_q = (b_q * tl.exp(b_zp[None, :] - b_z)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dh(q, z, do, dh, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t,\n    s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.constexpr, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT:\n    tl.constexpr, NORMK: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    b_zp = tl.full([BK if NORMK else BV], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        i_p = tl.maximum(i_t * BT - 1, 0)\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        if NORMK:\n            p_z = tl.make_block_ptr(z + i_bh * s_k_h, (K, T), (s_k_d, s_k_t\n                ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                (i_p * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0, 1))\n            b_q = (b_q * tl.exp(b_zc[:, None] - b_z)).to(b_q.dtype)\n            b_dh = b_dh * b_r[:, None]\n        else:\n            p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d\n                ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                (i_p * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0,))\n            b_do = (b_do * tl.exp(b_zc[None, :] - b_z)).to(b_do.dtype)\n            b_dh = b_dh * b_r[None, :]\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_V(k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k = tl.exp(b_k - b_zc[None, :]).to(b_k.dtype)\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n        b_do = (b_do * scale).to(b_do.dtype)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p * K +\n        i_k * BK,), (BK,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_z = tl.exp(b_zp[None, :] - b_z)\n    b_dq = b_dq * b_z\n    b_dk = b_dk * b_k\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0).to(b_k.dtype)\n    if i_k == 0:\n        tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_V(q, k, z, dA, dq, dk, s_k_h, s_k_t, s_k_d,\n    T: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr,\n    BK: tl.constexpr, NC: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_zq = tl.exp(b_zn[None, :] - b_z)\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kz = tl.exp(b_k - b_zn[None, :]).to(b_k.dtype)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)\n    b_dq *= b_zq\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * tl.exp(b_kj[None, :] - b_z), 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kz = tl.exp(b_k - b_zn[None, :])\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_qz = (b_q * tl.exp(b_zn[None, :] - b_z)).to(b_q.dtype)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)\n    b_dk *= b_kz\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_zj = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)\n        b_zj = tl.load(p_zj, boundary_check=(0,)).to(tl.float32)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_k -\n            b_zj[None, :]), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_K(v, z, do, dA, s_v_h, s_v_t, s_v_d, scale,\n    T: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr,\n    BV: tl.constexpr, NC: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_zn[None, :] - b_z) * scale).to(b_do.dtype)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zn[:, None]).to(b_v.dtype)\n        b_dA = tl.dot(b_do, b_v, allow_tf32=False)\n        tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n            BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_v * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n            b_dA = tl.sum(b_do * tl.exp(b_v[None, :] - b_z), 1)\n            b_dA = tl.where(o_i >= j, b_dA, 0)\n            tl.store(dA + o_A + j, b_dA.to(b_do.dtype), mask=m_A)\n            p_v = tl.advance(p_v, (V,))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_K(q, k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k), allow_tf32=False)\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p *\n            V + i_v * BV,), (BV,), (0,))\n        p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zc[None, :]).to(b_v.dtype)\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_z = tl.exp(b_zp[None, :] - b_z)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * b_z * scale).to(b_do.dtype)\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q, allow_tf32=False)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_KV(v, z, A, do, dv, s_v_h, s_v_t, s_v_d, T:\n    tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV:\n    tl.constexpr, NC: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_zn[None, :] - b_z)).to(b_do.dtype)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n    b_dv *= tl.exp(b_v - b_zn[None, :])\n    o_i = tl.arange(0, BC)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T * BT,), (1,), ((i_t *\n            BT + i_i * BC + j) * BT + i_i * BC,), (BC,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(p_A, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_do = tl.load(p_do, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_v - b_z[None, :]) * b_A[:, None] *\n            b_do[None, :], 0.0)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_inter(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, NT:\n    tl.constexpr):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    b_sp = tl.zeros([BS], dtype=tl.float32)\n    b_zp = tl.full([BS], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_zc = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), (i_t *\n            BT * S + i_m * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d\n            ), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo = tl.exp(b_s - b_zp[None, :]) * b_sp[None, :]\n        tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1)\n            )\n        b_sp = b_sp * tl.exp(b_zc - b_zp) + tl.sum(b_ss * tl.exp(b_zc[None,\n            :] - b_z), 0)\n        b_zp = b_zc\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_intra(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BS:\n    tl.constexpr, NC: tl.constexpr):\n    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    o_i = tl.arange(0, BC)\n    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * S + i_s * BS,), (BS,), (0,))\n    p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_doo = tl.zeros([BC, BS], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo += b_ss * tl.exp(b_zn[None, :] - b_z)\n    b_doo = tl.exp(b_s - b_zn[None, :]) * tl.dot(m_o.to(b_s.dtype), b_doo.\n        to(b_s.dtype), allow_tf32=False)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_ss = tl.load(p_ss, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_doo += tl.where(m_i, tl.exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\n    b_doo += tl.load(p_doo, boundary_check=(0, 1))\n    tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_prepare_dv(q, k, do, dv, scale, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_k.dtype)\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0).to(do.dtype.element_ty)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dv = tl.dot(b_A, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(k, v, d, v_new, h, h0, ht, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BC: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl\n    .constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.\n    constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n            if HEAD_FIRST:\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_d = tl.make_block_ptr(d + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n                p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (\n                    i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_v_new = tl.make_block_ptr(v_new + i_bh * T * V, (T, V), (\n                    V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            else:\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_d = tl.make_block_ptr(d + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_v_new = tl.make_block_ptr(v_new + i_b * T * H * V + i_h *\n                    V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV),\n                    (BC, BV), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h.to(b_k.dtype))\n            tl.store(p_v_new, b_v.to(p_v_new.dtype.element_ty),\n                boundary_check=(0, 1))\n            b_hc += tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        b_h += b_hc\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_o(q, k, v, h, o, scale, T: tl.constexpr, H:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dhu(q, k, d, dht, dh0, do, dh, dv, dv2,\n    scale, T: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, BV: tl\n    .constexpr, NT: tl.constexpr, STORE_FINAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if STORE_FINAL_STATE:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):\n            if HEAD_FIRST:\n                p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n                p_d = tl.make_block_ptr(d + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_dv2 = tl.make_block_ptr(dv2 + i_bh * T * V, (T, V), (V, 1\n                    ), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            else:\n                p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_d = tl.make_block_ptr(d + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_dv2 = tl.make_block_ptr(dv2 + i_b * T * H * V + i_h * V,\n                    (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (\n                    BC, BV), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = (b_q * scale).to(b_q.dtype)\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n            tl.store(p_dv2, b_dv.to(p_dv.dtype.element_ty), boundary_check=\n                (0, 1))\n            b_dh_tmp += tl.dot(b_q, b_do.to(b_q.dtype), allow_tf32=False)\n            b_dh_tmp -= tl.dot(b_d, b_dv.to(b_q.dtype), allow_tf32=False)\n        b_dh += b_dh_tmp\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dqkw(q, k, v, w, h, do, dh, dq, dk, dv, dw,\n    scale, T: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl\n    .constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T), (1, \n            H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (V, NT * K), (1, V\n            ), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, b_dh, allow_tf32=False)\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0).to(b_q.dtype)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dq *= scale\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK'])\n@triton.jit\ndef fused_chunk_delta_rule_fwd_kernel(q, k, v, v_new, d, o, initial_state,\n    final_state, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, DK: tl.constexpr,\n    DV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE:\n    tl.constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_d = tl.make_block_ptr(d + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, DV), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + i_bh * s_v_h, (T, DV), (s_v_t,\n        s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_k.dtype)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_v_prime = tl.dot(b_d, b_h.to(b_q.dtype), allow_tf32=False)\n        b_v = b_v - b_v_prime\n        tl.store(p_v_new, b_v.to(p_v.dtype.element_ty), boundary_check=(0, 1))\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v.to(b_q.dtype), allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_v_new = tl.advance(p_v_new, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_d = tl.advance(p_d, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=\n            (0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fused_chunk_delta_rule_bwd_kernel(q, k, v, d, dht, dh0, do, dq, dk, dv,\n    dd, initial_state, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T,\n    scale, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, DK: tl.\n    constexpr, DV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_DHT:\n    tl.constexpr, USE_DHO: tl.constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_dht = tl.make_block_ptr(dht + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t),\n            (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_d = tl.make_block_ptr(d + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t),\n            (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, DV),\n            (s_v_t, s_v_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        b_dh -= tl.dot(b_d, b_dv.to(b_d.dtype), allow_tf32=False)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    if USE_DHO:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dd = tl.dot(b_dv.to(k.dtype.element_ty), b_h.to(k.dtype.\n            element_ty), allow_tf32=False)\n        p_dd = tl.make_block_ptr(dd + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_dd, -b_dd.to(p_dd.dtype.element_ty), boundary_check=(0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (DV, T), (s_v_d, s_v_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk32(k, beta, A, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BC: tl.\n    constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_A, 0)\n    for i in range(1, BT):\n        mask = tl.arange(0, BT) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n    b_A = b_A.to(k.dtype.element_ty)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk64(k, beta, A, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BC: tl.\n    constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BC,), (0,))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BC,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    if HEAD_FIRST:\n        p_beta2 = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT +\n            BC,), (BC,), (0,))\n    else:\n        p_beta2 = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT + BC,), (BC,), (0,))\n    b_beta2 = tl.load(p_beta2, boundary_check=(0,))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BC, BK), (1, 0))\n            p_k2 = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + BC, i_k * BK), (BC, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\n            p_k2 = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + BC, i_k * BK), (BC, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1))\n        b_kb2 = (b_k2 * b_beta2[:, None]).to(b_k2.dtype)\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n        b_A2 += tl.dot(b_kb2, tl.trans(b_k2), allow_tf32=False)\n        b_A3 += tl.dot(b_kb2, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A, 0)\n    b_A2 = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A2, 0)\n    for i in range(1, BC):\n        mask = tl.arange(0, BC) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n        b_A2 = tl.where(mask[:, None], b_a2, b_A2)\n    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A3 = -tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)\n    if HEAD_FIRST:\n        p_A1 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BC, BC), (1, 0))\n        p_A2 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + BC, BC), (BC, BC), (1, 0))\n        p_A3 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + BC, 0), (BC, BC), (1, 0))\n        p_A4 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, BC), (BC, BC), (1, 0))\n    else:\n        p_A1 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0))\n        p_A2 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + BC, BC), (BC, BC), (1, 0))\n        p_A3 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + BC, 0), (BC, BC), (1, 0))\n        p_A4 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0))\n    tl.store(p_A1, b_A.to(p_A1.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_A2, b_A2.to(p_A2.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_A3, b_A3.to(p_A3.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_A4, tl.zeros([BC, BC], dtype=tl.float32).to(p_A4.dtype.\n        element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fwd_recompute_w_u_kernel(k, v, beta, w, u, A, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_u = tl.make_block_ptr(u + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_u = tl.make_block_ptr(u + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_u = tl.dot(b_A.to(b_vb.dtype), b_vb, allow_tf32=False)\n        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_w = tl.make_block_ptr(w + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_w = tl.make_block_ptr(w + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_w = tl.dot(b_A.to(b_kb.dtype), b_kb, allow_tf32=False)\n        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK'])\n@triton.jit\ndef fwd_recompute_w_kernel(k, beta, w, A, T: tl.constexpr, H: tl.constexpr,\n    K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, HEAD_FIRST: tl.\n    constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1)).to(k.dtype.element_ty)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_w = tl.dot(b_A, b_kb, allow_tf32=False)\n        p_w = tl.make_block_ptr(w + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef bwd_prepare_wy_repr_kernel(k, v, beta, A, dw, du, dk, dv, dbeta, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, \n            i_t * BT), (BT, BT), (0, 1))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (BT, T), (\n            1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dbeta = tl.zeros([BT], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_du = tl.make_block_ptr(du + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_du = tl.make_block_ptr(du + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n        b_dv = b_dv_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dw = tl.make_block_ptr(dw + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dw = tl.make_block_ptr(dw + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n        b_dk = b_dk_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_dA, 0)\n    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)\n    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], \n        -b_dA, 0).to(k.dtype.element_ty)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n        b_dk += b_dk_beta * b_beta[:, None]\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    if HEAD_FIRST:\n        p_dbeta = tl.make_block_ptr(dbeta + i_bh * T, (T,), (1,), (i_t * BT\n            ,), (BT,), (0,))\n    else:\n        p_dbeta = tl.make_block_ptr(dbeta + i_b * T * H + i_h, (T,), (H,),\n            (i_t * BT,), (BT,), (0,))\n    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,)\n        )\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'K', 'V'])\n@triton.jit\ndef chunk_transform_qk_fwd_kernel(q, k, v, beta, o, A, q_new, k_new,\n    A_local, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr, BT: tl.constexpr, OUTPUT_ATTENTIONS: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, 0), (BT, BV), (1, 0))\n    b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(p_q.dtype.element_ty\n        )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    p_T = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_T = tl.load(p_T, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_t = o_i[:, None] >= o_i[None, :]\n    b_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0).to(\n        b_q.dtype)\n    m_t = o_i[:, None] > o_i[None, :]\n    b_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0).to(\n        b_k.dtype)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_qkT = tl.dot(b_qk, b_T, allow_tf32=False).to(b_k.dtype)\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1),\n            (i_t * BT, 0), (BT, BT), (1, 0))\n        tl.store(p_a, b_qkT.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n    b_kkT = tl.dot(b_kk, b_T, allow_tf32=False).to(b_k.dtype)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, 0), (BT, BV), (1, 0))\n    tl.store(p_o, tl.dot(b_qkT, b_v).to(p_o.dtype.element_ty),\n        boundary_check=(0, 1))\n    p_q_new = tl.make_block_ptr(q_new + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n        ), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_q_new, (b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False)).to(\n        p_q_new.dtype.element_ty), boundary_check=(0, 1))\n    p_k_new = tl.make_block_ptr(k_new + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n        ), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_k_new, (b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=\n        False)).to(p_k_new.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2)], key=['BT'])\n@triton.jit\ndef save_intra_chunk_attn(A, A_local, T: tl.constexpr, BT: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_A = tl.make_block_ptr(A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, \n        i_t * BT), (BT, BT), (1, 0))\n    p_A_local = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1),\n        (i_t * BT, 0), (BT, BT), (1, 0))\n    b_A_local = tl.load(p_A_local, boundary_check=(0, 1))\n    tl.store(p_A, b_A_local.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.heuristics({'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None}\n    )\n@triton.jit\ndef parallel_delta_rule_fwd_kernel(q, k, k2, v, beta, o, o_new, attn, s_k_h,\n    s_k_t, s_v_h, s_v_t, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    OUTPUT_ATTENTIONS: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        0), (BT, BK), (1, 0))\n    b_q = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q += tl.load(p_q, boundary_check=(0, 1))\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        0), (BT, BV), (1, 0))\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (0,\n            offset), (BK, BS), (0, 1))\n        p_k2 = tl.make_block_ptr(k2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n            offset, 0), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            offset, 0), (BS, BV), (1, 0))\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,),\n            (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n        m_s = tl.arange(0, BT) >= offset - i_t * BT + BS\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n        b_s = tl.where(m_s[:, None], b_s, 0)\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v\n            .dtype)\n        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (\n                i_t * BT, offset), (BT, BS), (1, 0))\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n    for offset in range(i_t * BT - BS, -BS, -BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (0,\n            offset), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            offset, 0), (BS, BV), (1, 0))\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,),\n            (BS,), (0,))\n        p_k2 = tl.make_block_ptr(k2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n            offset, 0), (BS, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v\n            .dtype)\n        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False).to(b_q.dtype)\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (\n                i_t * BT, offset), (BT, BS), (1, 0))\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n    p_o_new = tl.make_block_ptr(o_new + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n        i_t * BT, 0), (BT, BV), (1, 0))\n    tl.store(p_o_new, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_delta_rule_fwd_kernel(q, k, v, u, beta, o, h0, ht,\n    scale, B, T, H, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV:\n    tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.\n    constexpr, IS_BETA_HEADWISE: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_u = u + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        else:\n            p_beta = beta + i_bh * T\n        p_o = o + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_u = u + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV)\n        else:\n            p_beta = beta + i_b * T * H + i_h\n        p_o = o + (i_k * B + i_b) * T * H * V + i_h * V + i_v * BV + tl.arange(\n            0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)\n        b_v -= b_v_minus\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        tl.store(p_u, b_v.to(p_v.dtype.element_ty), mask=mask_v)\n        b_v *= b_beta\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += K if HEAD_FIRST else H * K\n        p_k += K if HEAD_FIRST else H * K\n        p_o += V if HEAD_FIRST else H * V\n        p_v += V if HEAD_FIRST else H * V\n        p_u += V if HEAD_FIRST else H * V\n        p_beta += (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_delta_rule_bwd_kernel(q, k, v, beta, h0, dh0, dht, do,\n    dq, dk, dv, db, scale, B: tl.constexpr, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr, NK: tl.constexpr, USE_INITIAL_STATE: tl.constexpr,\n    IS_BETA_HEADWISE: tl.constexpr, USE_DH0: tl.constexpr, USE_DHT: tl.\n    constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_dk = dk + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK\n            ) + (T - 1) * K\n        p_dv = dv + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV\n            ) + (T - 1) * V\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1\n                ) * V\n            p_dbeta = db + (i_v * NK * B * H + i_k * B * H + i_bh\n                ) * T * V + tl.arange(0, BV) + (T - 1) * V\n        else:\n            p_beta = beta + i_bh * T + T - 1\n            p_dbeta = db + (i_v * B * H + i_bh) * T + T - 1\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T\n             - 1) * H * V\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (\n            T - 1) * H * V\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T - 1\n            ) * H * K\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T - 1\n            ) * H * V\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV) + (T - 1) * H * V\n            p_dbeta = db + (i_v * NK * B + i_k * B + i_b\n                ) * T * H * V + i_h * V + tl.arange(0, BV) + (T - 1) * H * V\n        else:\n            p_beta = beta + i_b * T * H + (T - 1) * H + i_h\n            p_dbeta = db + (i_v * B + i_b) * T * H + (T - 1) * H + i_h\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0\n            ).to(tl.float32)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * (b_v * b_beta)[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        b_db = b_dv * b_v if IS_BETA_HEADWISE else tl.sum(b_dv * b_v)\n        b_dv = b_dv * b_beta\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n        if IS_BETA_HEADWISE:\n            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty), mask=mask_v)\n        else:\n            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty))\n        b_dh -= b_k[:, None] * b_dv[None, :]\n        p_q -= K if HEAD_FIRST else H * K\n        p_k -= K if HEAD_FIRST else H * K\n        p_v -= V if HEAD_FIRST else H * V\n        p_do -= V if HEAD_FIRST else H * V\n        p_dk -= K if HEAD_FIRST else H * K\n        p_dv -= V if HEAD_FIRST else H * V\n        p_dbeta -= (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n        p_beta -= (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_k[:,\n            None] & mask_v[None, :])\n    tl.debug_barrier()\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        else:\n            p_beta = beta + i_bh * T\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n        p_dk = dk + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV)\n        else:\n            p_beta = beta + i_b * T * H + i_h\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n    if USE_INITIAL_STATE:\n        mask_h = mask_k[:, None] & mask_v[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_dk = tl.load(p_dk, mask=mask_k, other=0).to(tl.float32)\n        b_dv = tl.load(p_dv, mask=mask_v, other=0).to(tl.float32)\n        b_dk -= tl.sum(b_dv[None, :] * b_h, axis=1)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_v *= b_beta\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = b_h * b_do[None, :]\n        d_q = tl.sum(b_dq, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_do += V if HEAD_FIRST else H * V\n        p_dq += K if HEAD_FIRST else H * K\n        p_dk += K if HEAD_FIRST else H * K\n        p_dv += V if HEAD_FIRST else H * V\n        p_beta += (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_k_kernel_intra(v, g, o, A, T: tl.constexpr, HQ: tl.\n    constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.\n    constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + min(i_t *\n            BT + i_i * BC, T) * V + o_v, BV), BV)\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * V + min(\n            i_t * BT + i_i * BC, T) * H * V + i_h * V + o_v, BV), BV)\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        if HEAD_FIRST:\n            p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (\n                i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n            p_gv = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        else:\n            p_A = tl.make_block_ptr(A + i_b * T * HQ * BT + i_hq * BT, (T,\n                BT), (HQ * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC\n                ), (1, 0))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n            p_gv = tl.make_block_ptr(g + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = (b_v * tl.exp(b_gn[None, :] - b_gv)).to(b_v.dtype)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, b_vg)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        if HEAD_FIRST:\n            p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * T * V + (i_t *\n                BT + i_i * BC + j) * V + o_v, BV), BV)\n            p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n                BT + i_i * BC + j) * V + o_v, BV), BV)\n        else:\n            p_v = tl.max_contiguous(tl.multiple_of(v + i_b * T * H * V + \n                i_h * V + (i_t * BT + i_i * BC + j) * V + o_v, BV), BV)\n            p_gv = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * V + \n                i_h * V + (i_t * BT + i_i * BC + j) * V + o_v, BV), BV)\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n        b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n        b_vg = b_v[None, :] * tl.exp(b_g - b_gv[None, :])\n        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)\n    if HEAD_FIRST:\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    else:\n        p_o = tl.make_block_ptr(o + i_b * T * HQ * V + i_hq * V, (T, V), (\n            HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_k_kernel_inter(q, k, h, g, o, A, scale, T: tl.constexpr,\n    HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT:\n    tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, NT:\n    tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bg * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * HQ * K + i_hq * K, (T, K),\n                (HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h)\n        b_A += tl.dot(b_q, b_k)\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * HQ * V + i_hq * V, (T, V), (\n            HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_b * T * HQ * BT + i_hq * BT, (T, BT),\n            (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o = b_o * tl.exp(b_g)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_k_kernel_dA(v, g, do, dA, scale, B: tl.constexpr, T: tl.\n    constexpr, HQ: tl.constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl\n    .constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    if HEAD_FIRST:\n        p_dA = tl.make_block_ptr(dA + (i_v * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    else:\n        p_dA = tl.make_block_ptr(dA + ((i_v * B + i_b) * T * HQ + i_hq) *\n            BT, (T, BT), (HQ * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC,\n            BC), (1, 0))\n    b_dA = tl.zeros([BC, BC], dtype=tl.float32)\n    if i_i > i_j:\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bg * T * V, (V, T), (1, V), (i_v *\n                BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n            p_gv = tl.make_block_ptr(g + i_bg * T * V, (V, T), (1, V), (i_v *\n                BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n                BT + i_i * BC) * V + o_v, BV), BV)\n            p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (V, T), (1,\n                H * V), (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n            p_gv = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (V, T), (\n                1, H * V), (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + (i_b * T * H + i_h) *\n                V + (i_t * BT + i_i * BC) * H * V + o_v, BV), BV)\n            p_g = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (T, V), (H *\n                V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + (i_b * T * HQ + i_hq) * V, (T, V),\n                (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_gn = tl.load(p_gn, mask=m_v, other=0.0)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_g - b_gn[None, :]) * scale).to(b_do.dtype)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = (b_v * tl.exp(b_gn[:, None] - b_gv)).to(b_v.dtype)\n        b_dA = tl.dot(b_do, b_vg)\n    elif i_i == i_j:\n        if HEAD_FIRST:\n            p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * T * V + (i_t *\n                BT + i_j * BC) * V + o_v, BV), BV)\n            p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n                BT + i_j * BC) * V + o_v, BV), BV)\n        else:\n            p_g = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (T, V), (H *\n                V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + (i_b * T * HQ + i_hq) * V, (T, V),\n                (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n            p_v = tl.max_contiguous(tl.multiple_of(v + (i_b * T * H + i_h) *\n                V + (i_t * BT + i_j * BC) * H * V + o_v, BV), BV)\n            p_gv = tl.max_contiguous(tl.multiple_of(g + (i_b * T * H + i_h) *\n                V + (i_t * BT + i_j * BC) * H * V + o_v, BV), BV)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        m_v = o_v < V\n        o_i = tl.arange(0, BC)\n        m_dA = o_i[:, None] >= o_i[None, :]\n        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):\n            b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n            b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n            b_dAj = tl.sum(b_do * b_v[None, :] * tl.exp(b_g - b_gv[None, :]), 1\n                )\n            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)\n            p_v += V\n            p_gv += V\n        b_dA = tl.where(m_dA, b_dA, 0.0)\n    tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_k_kernel_dqkvg(q, k, v, h, g, A, do, dh, dq, dk, dv, dg,\n    dgv, dA, scale, B: tl.constexpr, T: tl.constexpr, HQ: tl.constexpr, H:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, NT: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    o_t = min(i_t * BT + BT, T)\n    m_s = o_i[:, None] >= o_i[None, :]\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bg * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + (i_b * T * HQ + i_hq) * K, (T, K), (HQ *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + (i_b * T * H + i_h) * K, (T, K), (H * K,\n            1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_A = tl.make_block_ptr(A + ((i_k * B + i_b) * T * HQ + i_hq) * BT,\n            (T, BT), (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k))\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (o_t -\n                1) * V + o_v, BV), BV)\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T,\n                V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dg = tl.make_block_ptr(dg + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dgv = tl.make_block_ptr(dgv + (i_k * n_bh + i_bh) * T * V, (T,\n                V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (T, V), (H *\n                V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_g = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (T, V), (H *\n                V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + (i_b * T * H + i_h) *\n                V + (o_t - 1) * H * V + o_v, BV), BV)\n            p_do = tl.make_block_ptr(do + (i_b * T * HQ + i_hq) * V, (T, V),\n                (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + ((i_k * B + i_b) * T * HQ + i_hq) *\n                V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dg = tl.make_block_ptr(dg + (i_b * T * HQ + i_hq) * V, (T, V),\n                (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dgv = tl.make_block_ptr(dgv + ((i_k * B + i_b) * T * HQ +\n                i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT,\n                BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * K * V, (V, K),\n            (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        m_v = o_v < V\n        b_gn = tl.load(p_gn, mask=m_v, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_gv = tl.exp(b_gn[None, :] - b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_g) * scale).to(b_do.dtype)\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dg = tl.sum(tl.trans(b_h) * b_dh, 0) * tl.exp(b_gn)\n        b_dh = b_dh.to(b_k.dtype)\n        b_dq += tl.dot(b_do, b_h.to(b_k.dtype))\n        b_dk += tl.dot((b_v * b_gv).to(b_v.dtype), tl.trans(b_dh))\n        b_dv = tl.dot(b_k, b_dh) * b_gv\n        b_dg += tl.sum(b_dv * b_v, 0)\n        if i_k == 0:\n            b_dgv = tl.load(p_dg, boundary_check=(0, 1)) + b_dg[None, :]\n        else:\n            b_dgv = tl.zeros([BT, BV], dtype=tl.float32) + b_dg[None, :]\n        tl.store(p_dgv, b_dgv.to(p_dgv.dtype.element_ty), boundary_check=(0, 1)\n            )\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    if HEAD_FIRST:\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dA = tl.make_block_ptr(dA + (i_b * T * HQ + i_hq) * BT, (T, BT),\n            (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_b * T * HQ + i_hq) * K, (T, K), (\n            HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_b * T * HQ + i_hq) * K, (T, K), (\n            HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k)\n    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_k_kernel_intra_dvg(v, g, o, A, do, dv, dg, T: tl.\n    constexpr, HQ: tl.constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl\n    .constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    if HEAD_FIRST:\n        p_gv = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (min(i_t *\n            BT + i_i * BC + BC, T) - 1) * V + o_v, BV), BV)\n    else:\n        p_gv = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (T, V), (H *\n            V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + (i_b * T * H + i_h) * V +\n            (min(i_t * BT + i_i * BC + BC, T) - 1) * H * V + o_v, BV), BV)\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        if HEAD_FIRST:\n            p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n                BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n            p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (\n                i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        else:\n            p_g = tl.make_block_ptr(g + (i_b * T * H + i_h) * V, (T, V), (H *\n                V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n            p_A = tl.make_block_ptr(A + (i_b * T * HQ + i_hq) * BT, (BT, T),\n                (1, HQ * BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1)\n                )\n            p_do = tl.make_block_ptr(do + (i_b * T * HQ + i_hq) * V, (T, V),\n                (HQ * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_g - b_gn[None, :])).to(b_do.dtype)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do)\n    b_dv *= tl.exp(b_gn[None, :] - b_gv)\n    o_i = tl.arange(0, BC)\n    o_c = i_i * BC + tl.arange(0, BC)\n    if HEAD_FIRST:\n        p_g = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t * BT +\n            i_i * BC) * V + o_v, BV), BV)\n        p_A = tl.max_contiguous(tl.multiple_of(A + i_bh * T * BT + (i_t *\n            BT + i_i * BC) * BT + o_c, BC), BC)\n        p_do = tl.max_contiguous(tl.multiple_of(do + i_bh * T * V + (i_t *\n            BT + i_i * BC) * V + o_v, BV), BV)\n    else:\n        p_g = tl.max_contiguous(tl.multiple_of(g + (i_b * T * H + i_h) * V +\n            (i_t * BT + i_i * BC) * H * V + o_v, BV), BV)\n        p_A = tl.max_contiguous(tl.multiple_of(A + (i_b * T * HQ + i_hq) *\n            BT + (i_t * BT + i_i * BC) * HQ * BT + o_c, BC), BC)\n        p_do = tl.max_contiguous(tl.multiple_of(do + (i_b * T * HQ + i_hq) *\n            V + (i_t * BT + i_i * BC) * HQ * V + o_v, BV), BV)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.load(p_A)\n        b_g = tl.load(p_g, mask=m_v, other=0)\n        b_do = tl.load(p_do, mask=m_v, other=0)\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_g[None, :] - b_gv) * b_A[:, None] *\n            b_do[None, :], 0.0)\n        p_g += V\n        p_A += BT\n        p_do += V\n    if HEAD_FIRST:\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    else:\n        p_o = tl.make_block_ptr(o + (i_b * T * HQ + i_hq) * V, (T, V), (HQ *\n            V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (T, V), (H * V,\n            1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + (i_b * T * HQ + i_hq) * V, (T, V), (\n            HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_b * T * HQ + i_hq) * V, (T, V), (\n            HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dg = tl.make_block_ptr(dg + (i_b * T * HQ + i_hq) * V, (T, V), (\n            HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1)).to(tl.float32)\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(tl.float32)\n    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1)).to(tl.float32)\n    b_dg = b_o * b_do - b_v * b_dv\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gsa_inference_kernel(q, k, v, s, g, o, hk0, hv0, hkt,\n    hvt, scale, K: tl.constexpr, V: tl.constexpr, M: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NG: tl.constexpr):\n    i_bh = tl.program_id(0)\n    i_bg = i_bh // NG\n    b_s = tl.load(s + i_bg * M + tl.arange(0, M)).to(tl.float32)\n    b_g = tl.load(g + i_bg * M + tl.arange(0, M)).to(tl.float32)\n    b_g = tl.exp(b_g)\n    b_ok = tl.zeros([M], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        p_hk0 = hk0 + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[:, None\n            ]\n        mask_k = o_k < K\n        mask_hk = (tl.arange(0, M) < M)[:, None] & mask_k[None, :]\n        b_hk = tl.load(p_hk0, mask=mask_hk, other=0.0).to(tl.float32)\n        b_q = tl.load(q + i_bh * K + o_k, mask=mask_k, other=0.0).to(tl.float32\n            ) * scale\n        b_k = tl.load(k + i_bg * K + o_k, mask=mask_k, other=0.0).to(tl.float32\n            )\n        b_hk = b_hk * b_g[:, None] + b_k[None, :] * b_s[:, None]\n        b_ok += tl.sum(b_hk * b_q[None, :], axis=1)\n        if i_bh % NG == 0:\n            p_hkt = hkt + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[\n                :, None]\n            tl.store(p_hkt, b_hk.to(p_hkt.dtype.element_ty), mask=mask_hk)\n    b_qv = tl.softmax(b_ok)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_hv0 = hv0 + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[:, None\n            ]\n        mask_v = o_v < V\n        mask_hv = mask_v[:, None] & (tl.arange(0, M) < M)[None, :]\n        b_hv = tl.load(p_hv0, mask=mask_hv, other=0).to(tl.float32)\n        b_v = tl.load(v + i_bg * V + o_v, mask=mask_v, other=0).to(tl.float32)\n        b_hv = b_hv * b_g[None, :] + b_s[None, :] * b_v[:, None]\n        b_ov = tl.sum(b_hv * b_qv[None, :], axis=1)\n        tl.store(o + i_bh * V + o_v, b_ov.to(o.dtype.element_ty), mask=mask_v)\n        if i_bh % NG == 0:\n            p_hvt = hvt + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[\n                :, None]\n            tl.store(p_hvt, b_hv.to(p_hvt.dtype.element_ty), mask=mask_hv)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, h0, ht, H: tl.constexpr, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NT: tl.constexpr, USE_INITIAL_STATE: tl.\n    constexpr, STORE_FINAL_STATE: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and T % BT != 0:\n            d_b = tl.math.exp2(T % BT * b_b)\n            d_i = tl.math.exp2((T % BT - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_i[:, None]).to(b_k.dtype),\n            allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, scale, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_o = b_o * d_i[:, None]\n    b_s = b_s * d_s\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, dh0, dht, scale, H: tl.\n    constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT:\n    tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        d_b = tl.math.exp2(min(BT, T - i_t * BT) * b_b)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, (b_do * d_i[:, None]).to(b_q.dtype),\n            allow_tf32=False)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, scale,\n    H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT:\n    tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((min(BT, T - i_t *\n        BT) - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T), (1, \n            H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T,\n                V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_b * H) * T * V + \n                i_h * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV),\n                (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (NT * K, V), (V, 1\n            ), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s\n            .to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_k.dtype)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:,\n                None]\n            b_h = d_b * b_h + tl.dot(b_k, (b_v * d_h[:, None]).to(b_k.dtype\n                ), allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:,\n                None]\n            if i == NT - 1 and T % BT != 0:\n                d_b = tl.math.exp2(T % BT * b_b)\n                d_h = tl.math.exp2((T % BT - o_i - 1) * b_b)\n            b_h = d_b * b_h + tl.dot(b_k, (b_v * d_h[:, None]).to(b_k.dtype\n                ), allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.\n    constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.\n    constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = (b_ds * d_s).to(b_k.dtype)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n            b_h = d_b * b_h + tl.dot((b_v * d_k[None, :]).to(b_k.dtype),\n                b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n            b_h = d_b * b_h + tl.dot((b_v * d_k[None, :]).to(b_k.dtype),\n                b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = (b_ds * d_s).to(b_k.dtype)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False\n                ) * d_k[:, None]\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[\n                :, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False\n                ) * d_k[:, None]\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[\n                :, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']),\n    'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})\n@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, attn, scale, B: tl.constexpr,\n    H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT:\n    tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NV:\n    tl.constexpr, OUTPUT_ATTENTIONS: tl.constexpr):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_k = tl.arange(0, BS)\n    d_h = tl.math.exp2((BS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0),\n        (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV),\n        (BS, BV), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, 0), (BT, BS), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i in range(0, i_t * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h\n        if i > 0:\n            b_o = b_o * tl.math.exp2(b_b * BS)\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BS))\n        p_v = tl.advance(p_v, (BS, 0))\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BT)\n    d_q = tl.math.exp2(tl.arange(0, BT) * b_b)\n    b_o *= d_q[:, None]\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, \n        i_t * BT), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BS, BV), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, i_t * BT), (BT, BS), (1, 0))\n    for _ in range(i_t * BT, (i_t + 1) * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n        p_k = tl.advance(p_k, (0, BS))\n        p_v = tl.advance(p_v, (BS, 0))\n        o_k += BS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * T * V, (T, V), (V, 1\n        ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_bwd_kernel_dq(i_bh, i_t, i_k, i_v, i_h, k, v, do, dq,\n    scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK),\n        (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0),\n        (BV, BS), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BS)\n    d_h = tl.math.exp2((BS - tl.arange(0, BS)) * b_b)\n    for i in range(0, i_t * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        if i != 0:\n            b_dq *= d_b\n        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BS, 0))\n        p_v = tl.advance(p_v, (0, BS))\n    b_dq *= tl.math.exp2(tl.arange(0, BT) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BT)\n    o_k = tl.arange(0, BS)\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, \n        i_t * BT), (BV, BS), (0, 1))\n    for _ in range(i_t * BT, (i_t + 1) * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BS, 0))\n        p_v = tl.advance(p_v, (0, BS))\n        o_k += BS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * T * K, (T, K), (K,\n        1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, i_h, q, k, v, do,\n    dk, dv, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl\n    .constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl\n    .constexpr, BV: tl.constexpr):\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BS)\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    NTS = tl.cdiv(T, BS)\n    d_h = tl.math.exp2((BT - tl.arange(0, BT)) * b_b)\n    b_kd = (b_k * d_h[:, None]).to(b_k.dtype)\n    d_q = tl.math.exp2(tl.arange(0, BS) * b_b)\n    for i in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i, i_k *\n            BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i, i_v *\n            BV), (BS, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * d_q[:, None]).to(b_do.dtype)\n        b_dk *= d_b\n        b_dv *= d_b\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_s = tl.dot(b_kd, tl.trans(b_q), allow_tf32=False)\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_do.dtype), b_do, allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BS), tl.arange(0, BT)\n    for i in range(i_t * BT, (i_t + 1) * BT, BS):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i, i_k *\n            BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i, i_v *\n            BV), (BS, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b.to(tl.float32)), 0) * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False) * d_s\n        b_s = tl.dot(b_k, tl.trans(b_q), allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        o_q += BS\n    p_dk = tl.make_block_ptr(dk + (i_v * B * H + i_bh) * T * K, (T, K), (K,\n        1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * B * H + i_bh) * T * V, (T, V), (V,\n        1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV'])})\n@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, scale, B: tl.\n    constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl\n    .constexpr, NV: tl.constexpr):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_h = i_bh % H\n    parallel_retention_bwd_kernel_dq(i_bh, i_t, i_k, i_v, i_h, k, v, do, dq,\n        scale, B=B, H=H, T=T, K=K, V=V, BT=BT, BS=BS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    parallel_retention_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, i_h, q, k, v, do,\n        dk, dv, scale, B, H, T, K, V, BT, BS, BK, BV)\n"
    },
    {
      "input": "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, h0, ht, scale, B: tl.\n    constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.\n    constexpr, STORE_FINAL_STATE: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B + i_b) * T * H * V + i_h * V + i_v * BV + tl.arange(\n            0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_h = b_b * b_h + b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += K if HEAD_FIRST else H * K\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_o += V if HEAD_FIRST else H * V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)\n"
    },
    {
      "input": "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None})\n@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, h0, do, dq, dk, dv, dh0,\n    dht, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_h = b_b * b_h + b_k[:, None] * b_v[None, :]\n        b_dq = tl.sum(b_h * b_do[None, :], axis=1) * scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_do += V if HEAD_FIRST else H * V\n        p_dq += K if HEAD_FIRST else H * K\n    tl.debug_barrier()\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK\n            ) + (T - 1) * K\n        p_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV\n            ) + (T - 1) * V\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T\n             - 1) * H * V\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (\n            T - 1) * H * V\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T - 1\n            ) * H * K\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T - 1\n            ) * H * V\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_dh += tl.load(p_ht, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        b_dh *= b_b\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n        p_q -= K if HEAD_FIRST else H * K\n        p_k -= K if HEAD_FIRST else H * K\n        p_v -= V if HEAD_FIRST else H * V\n        p_do -= V if HEAD_FIRST else H * V\n        p_dk -= K if HEAD_FIRST else H * K\n        p_dv -= V if HEAD_FIRST else H * V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_fwd_kernel_h(x, g, gc, o, h0, T: tl.constexpr, D: tl.\n    constexpr, BT: tl.constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.\n    constexpr):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + i_t * BT * D + o_d\n    p_g = g + i_b * T * D + i_t * BT * D + o_d\n    p_gc = gc + i_b * T * D + i_t * BT * D + o_d\n    p_o = o + i_b * T * D + i_t * BT * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    b_gc = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        if i_t == 0:\n            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.\n                float32)\n    for i in range(0, BT):\n        mask_t = mask & (i_t * BT + i < T)\n        b_x = tl.load(p_x, mask=mask_t, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask_t, other=0).to(tl.float32)\n        b_h = tl.exp(b_g) * b_h + b_x\n        b_gc = b_gc + b_g\n        tl.store(p_gc, b_gc.to(p_o.dtype.element_ty), mask=mask_t)\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask_t)\n        p_x += D\n        p_g += D\n        p_gc += D\n        p_o += D\n"
    },
    {
      "input": "@triton.jit\ndef chunk_hgrn_fwd_kernel_o(gc, o, s_b, s_t, s_d, T: tl.constexpr, D: tl.\n    constexpr, BT: tl.constexpr, BD: tl.constexpr):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(1, tl.cdiv(T, BT)):\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask,\n            other=0).to(tl.float32)\n        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n        b_o = b_o + tl.exp(b_gc) * b_h0[None, :]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_bwd_kernel_h(g, gc, dx, do, T: tl.constexpr, D: tl.constexpr,\n    BT: tl.constexpr, BD: tl.constexpr):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    BC = min(BT, T - i_t * BT)\n    NT = tl.num_programs(1)\n    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    if i_t == NT - 1:\n        b_gc = tl.zeros([BD], dtype=tl.float32)\n    else:\n        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask,\n            other=0).to(tl.float32)\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for _ in range(BC - 1, -1, -1):\n        tl.store(p_gc, b_gc.to(p_gc.dtype.element_ty), mask=mask)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        b_gc = b_gc + b_g\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        p_g -= D\n        p_gc -= D\n        p_dx -= D\n        p_do -= D\n"
    },
    {
      "input": "@triton.jit\ndef chunk_hgrn_bwd_kernel_o(g, gc, o, dx, dg, s_b, s_t, s_d, T: tl.\n    constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_g = tl.make_block_ptr(g + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT - 1, i_d * BD), (BT, BD), (1, 0))\n        p_dx = tl.make_block_ptr(dx + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        mask_t = mask & ((i_t + 1) * BT < T)\n        b_ht = tl.load(dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=\n            mask_t, other=0).to(tl.float32)\n        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n        b_dx = tl.load(p_dx, boundary_check=(0, 1)).to(tl.float32)\n        b_dx = b_dx + tl.exp(b_gc) * b_ht[None, :]\n        b_dg = b_o * b_dx * tl.exp(b_g)\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(x, g, o, h0, ht, T: tl.constexpr, D: tl\n    .constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + o_d\n    p_g = g + i_b * T * D + o_d\n    p_o = o + i_b * T * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_b * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_h = tl.exp(b_g) * b_h + b_x\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n        p_x += D\n        p_g += D\n        p_o += D\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_b * D + o_d\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(g, o, dx, dg, do, h0, T: tl.constexpr,\n    D: tl.constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.constexpr):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_g = g + (i_b * T + T - 1) * D + o_d\n    p_o = o + (i_b * T + T - 2) * D + o_d\n    p_dx = dx + (i_b * T + T - 1) * D + o_d\n    p_dg = dg + (i_b * T + T - 1) * D + o_d\n    p_do = do + (i_b * T + T - 1) * D + o_d\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.float32\n                )\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        b_dg = b_dh * b_o\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_fwd_kernel(q, k, v, alpha, beta, o, ha, h0, ht, s_k_h,\n    s_v_h, scale, B, H, T, K: tl.constexpr, V: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0).to(tl.float32)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0).to(tl.float32)\n        tmp = tl.sum(h * b_alpha[None, :], axis=1)\n        h += tmp[:, None] * b_beta[None, :] + b_k[None, :] * b_v[:, None]\n        _o = h * b_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o.to(p_o.dtype.element_ty), mask=mask_bv)\n        tl.store(p_ha, tmp.to(p_ha.dtype.element_ty), mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n        p_ha += V\n        p_alpha += K\n        p_beta += K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, h.to(p_ht.dtype.element_ty), mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_bwd_kernel(q, k, v, alpha, beta, ha, dht, dh0, do, dq,\n    dk, dv, dalpha, dbeta, dha, h0, s_k_h, s_v_h, NK, scale, B, H, T, K: tl\n    .constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, USE_DH0: tl.constexpr, USE_DHT: tl.\n    constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_ha = ha + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_alpha = alpha + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (T\n         - 1) * K\n    p_dbeta = dbeta + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK\n        ) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (T\n         - 1) * V\n    p_dha = dha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV\n        ) + (T - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_bk[:, None] & mask_bv[None, :], other=0\n            ).to(tl.float32)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0).to(tl.float32)\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0).to(tl.float32)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0).to(tl.float32)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n        b_dha = tl.sum(d_h * b_beta[:, None], axis=0)\n        tl.store(p_dha, b_dha.to(p_dha.dtype.element_ty), mask=mask_bv)\n        b_dbeta = tl.sum(d_h * b_ha[None, :], axis=1)\n        tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), mask=mask_bk)\n        d_h += b_dha[None, :] * b_alpha[:, None]\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n        p_beta -= K\n        p_dbeta -= K\n        p_alpha -= K\n        p_dha -= V\n        p_ha -= V\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h.to(p_dh0.dtype.element_ty), mask=mask_bk[:,\n            None] & mask_bv[None, :])\n    tl.debug_barrier()\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dha = dha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK\n        )\n    p_dalpha = dalpha + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    for i in range(0, T):\n        d_ha = tl.load(p_dha, mask=mask_bv, other=0).to(tl.float32)\n        d_alpha = tl.sum(d_ha[None, :] * h, axis=1)\n        tl.store(p_dalpha, d_alpha.to(p_dalpha.dtype.element_ty), mask=mask_bk)\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0).to(tl.float32)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0).to(tl.float32)\n        h += b_k[:, None] * b_v[None, :] + b_beta[:, None] * b_ha[None, :]\n        _d_q = h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dk += K\n        p_dalpha += K\n        p_dha += V\n        p_ha += V\n        p_dq += K\n        p_beta += K\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv4_forward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_c, wkv_ptr, wkv_s_b, wkv_s_t, wkv_s_c,\n    state_out_ptr, state_out_s_b, state_out_s_abe, state_out_s_t,\n    state_out_s_c, chans, tsz, BLOCK_SIZE_C: tl.constexpr):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    wkv_ptr = wkv_ptr + b_idx * wkv_s_b\n    alpha_out_ptr = state_out_ptr + b_idx * state_out_s_b\n    beta_out_ptr = state_out_ptr + b_idx * state_out_s_b + state_out_s_abe\n    eps_out_ptr = state_out_ptr + b_idx * state_out_s_b + 2 * state_out_s_abe\n    alpha = tl.load(alpha_ptr + cs * state_s_c, mask=cmask).to(tl.float32)\n    beta = tl.load(beta_ptr + cs * state_s_c, mask=cmask).to(tl.float32)\n    eps = tl.load(eps_ptr + cs * state_s_c, mask=cmask).to(tl.float32)\n    w = tl.load(w_ptr + cs * w_s_c, mask=cmask).to(tl.float32)\n    u = tl.load(u_ptr + cs * u_s_c, mask=cmask).to(tl.float32)\n    for t in range(tsz):\n        kt = tl.load(k_ptr + t * k_s_t + cs * k_s_c, mask=cmask).to(tl.float32)\n        vt = tl.load(v_ptr + t * v_s_t + cs * v_s_c, mask=cmask).to(tl.float32)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps)\n        e1a = tl.exp(eps - tau)\n        e2a = tl.exp(ukt - tau)\n        wkv = (e1a * alpha + e2a * vt) / (e1a * beta + e2a)\n        tl.store(wkv_ptr + t * wkv_s_t + cs * wkv_s_c, wkv, mask=cmask)\n        w_eps = w + eps\n        eps = tl.maximum(w_eps, kt)\n        e1b = tl.exp(w_eps - eps)\n        e2b = tl.exp(kt - eps)\n        alpha = e1b * alpha + e2b * vt\n        beta = e1b * beta + e2b\n        tl.store(alpha_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            alpha, mask=cmask)\n        tl.store(beta_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            beta, mask=cmask)\n        tl.store(eps_out_ptr + t * state_out_s_t + cs * state_out_s_c, eps,\n            mask=cmask)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv4_backward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_t, state_s_c, gwkv_ptr, gwkv_s_b, gwkv_s_t,\n    gwkv_s_c, gstate_out_ptr, gstate_out_s_b, gstate_out_s_abe,\n    gstate_out_s_c, gw_ptr, gw_s_c, gu_ptr, gu_s_c, gk_ptr, gk_s_b, gk_s_t,\n    gk_s_c, gv_ptr, gv_s_b, gv_s_t, gv_s_c, gstate_ptr, gstate_s_b,\n    gstate_s_abe, gstate_s_c, tsz, chans, BLOCK_SIZE_C: tl.constexpr):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    gk_ptr = gk_ptr + b_idx * gk_s_b\n    gv_ptr = gv_ptr + b_idx * gv_s_b\n    gwkv_ptr = gwkv_ptr + b_idx * gwkv_s_b\n    galpha_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b\n    gbeta_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b + gstate_out_s_abe\n    geps_out_ptr = (gstate_out_ptr + b_idx * gstate_out_s_b + 2 *\n        gstate_out_s_abe)\n    galpha = tl.load(galpha_out_ptr + gstate_out_s_c * cs, mask=cmask).to(tl\n        .float32)\n    gbeta = tl.load(gbeta_out_ptr + gstate_out_s_c * cs, mask=cmask).to(tl.\n        float32)\n    geps = tl.load(geps_out_ptr + gstate_out_s_c * cs, mask=cmask).to(tl.\n        float32)\n    w = tl.load(w_ptr + w_s_c * cs, mask=cmask).to(tl.float32)\n    u = tl.load(u_ptr + u_s_c * cs, mask=cmask).to(tl.float32)\n    gw = tl.zeros_like(w)\n    gu = tl.zeros_like(u)\n    alpha_prev = tl.load(alpha_ptr + tsz * state_s_t + state_s_c * cs, mask\n        =cmask).to(tl.float32)\n    beta_prev = tl.load(beta_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask\n        ).to(tl.float32)\n    eps_prev = tl.load(eps_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask\n        ).to(tl.float32)\n    for t in range(tsz):\n        tc = tsz - t - 1\n        kt = tl.load(k_ptr + tc * k_s_t + k_s_c * cs, mask=cmask).to(tl.float32\n            )\n        vt = tl.load(v_ptr + tc * v_s_t + v_s_c * cs, mask=cmask).to(tl.float32\n            )\n        alpha_curr = alpha_prev\n        beta_curr = beta_prev\n        eps_curr = eps_prev\n        alpha_prev = tl.load(alpha_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask).to(tl.float32)\n        beta_prev = tl.load(beta_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask).to(tl.float32)\n        eps_prev = tl.load(eps_ptr + tc * state_s_t + state_s_c * cs, mask=\n            cmask).to(tl.float32)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps_prev)\n        e1 = tl.exp(eps_prev - tau)\n        e2 = tl.exp(ukt - tau)\n        euke = tl.exp(ukt + eps_prev - 2 * tau)\n        denom = e1 * beta_prev + e2\n        denom_sq = denom * denom\n        gwkvt = tl.load(gwkv_ptr + tc * gwkv_s_t + gwkv_s_c * cs, mask=cmask\n            ).to(tl.float32)\n        guk = gwkvt * e2 * (e1 * beta_prev * vt - e1 * alpha_prev) / denom_sq\n        gu += guk\n        gk = guk\n        gv = gwkvt * e2 / denom\n        galpha_wkv = gwkvt * e1 / denom\n        gbeta_wkv = -gwkvt * e1 * (e2 * vt + e1 * alpha_prev) / denom_sq\n        geps_wkv_denom = e1 * beta_prev + e2\n        geps_wkv = gwkvt * euke * (alpha_prev - vt * beta_prev) / (\n            geps_wkv_denom * geps_wkv_denom)\n        e1 = tl.exp(w + eps_prev - eps_curr)\n        e2 = tl.exp(kt - eps_curr)\n        galpha_we = galpha * e1 * alpha_prev\n        gw += galpha_we\n        gk += galpha * e2 * vt\n        gv += galpha * e2\n        geps += galpha * -alpha_curr\n        gbeta_we = gbeta * e1 * beta_prev\n        gw += gbeta_we\n        gk += gbeta * e2\n        geps += gbeta * -beta_curr\n        geps_mask = w + eps_prev > kt\n        geps_we = tl.where(geps_mask, geps, tl.zeros_like(geps))\n        gw += geps_we\n        gk += tl.where(geps_mask, tl.zeros_like(geps), geps)\n        tl.store(gk_ptr + tc * gk_s_t + gk_s_c * cs, gk, mask=cmask)\n        tl.store(gv_ptr + tc * gv_s_t + gv_s_c * cs, gv, mask=cmask)\n        galpha = galpha * e1 + galpha_wkv\n        gbeta = gbeta * e1 + gbeta_wkv\n        geps = galpha_we + gbeta_we + geps_we + geps_wkv\n    galpha_ptr = gstate_ptr + b_idx * gstate_s_b\n    gbeta_ptr = gstate_ptr + b_idx * gstate_s_b + gstate_s_abe\n    geps_ptr = gstate_ptr + b_idx * gstate_s_b + 2 * gstate_s_abe\n    tl.store(galpha_ptr + gstate_s_c * cs, galpha, mask=cmask)\n    tl.store(gbeta_ptr + gstate_s_c * cs, gbeta, mask=cmask)\n    tl.store(geps_ptr + gstate_s_c * cs, geps, mask=cmask)\n    gw_temp = tl.load(gw_ptr + gw_s_c * cs, mask=cmask).to(tl.float32)\n    gw_temp += gw\n    tl.store(gw_ptr + gw_s_c * cs, gw_temp, mask=cmask)\n    gu_temp = tl.load(gu_ptr + gu_s_c * cs, mask=cmask).to(tl.float32)\n    gu_temp += gu\n    tl.store(gu_ptr + gu_s_c * cs, gu_temp, mask=cmask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4)], key=['BT', 'BK',\n    'BV'])\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(q, k, v, h, g, o, scale, T: tl.constexpr,\n    H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H + i_h, (T,), (H,), (i_t *\n            BT,), (BT,), (0,))\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(q, k, v, h, g, do, dh, dq, dk, dg,\n    scale, B: tl.constexpr, T: tl.constexpr, H: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        b_g_last = tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1)\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H + i_h, (T,), (H,), (i_t *\n            BT,), (BT,), (0,))\n        b_g_last = tl.load(g + i_b * T * H + (min(i_t * BT + BT, T) - 1) * H)\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n    b_dg_last = tl.zeros([1], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (V, NT * K), (1, V\n            ), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dg_last += tl.sum(b_h * b_dh)\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + (i_k * B * H + i_bh) * T, (T,), (1,),\n            (i_t * BT,), (BT,), (0,))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dg = tl.make_block_ptr(dg + (i_k * B + i_b) * T * H + i_h, (T,),\n            (H,), (i_t * BT,), (BT,), (0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale * tl.exp(b_g\n        [:, None] - b_g[None, :]), 0)\n    b_ds = b_ds.to(b_k.dtype)\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dv(q, k, g, do, dv, dh, scale, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        b_g = tl.load(g + i_bh * T + i_t * BT + tl.arange(0, BT))\n        b_g_last = tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1)\n    else:\n        b_g = tl.load(g + i_b * T * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            )\n        b_g_last = tl.load(g + i_b * T * H + (min(i_t * BT + BT, T) - 1) *\n            H + i_h)\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (NT * K, V), (V, 1\n            ), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype)) * tl.exp(-b_g + b_g_last)[:,\n            None]\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = b_A * tl.exp(b_g[None, :] - b_g[:, None]) * scale\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0).to(do.dtype.element_ty)\n    if HEAD_FIRST:\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A, b_do)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT'])\n@triton.jit\ndef compute_final_dg(dg, o, T: tl.constexpr, BT: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_o = tl.make_block_ptr(dg + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_o = tl.load(p_o, boundary_check=(0,))\n    b_o = b_o - tl.cumsum(b_o, axis=0) + tl.sum(b_o, axis=0)\n    p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']),\n    'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})\n@triton.jit\ndef parallel_simple_gla_fwd_kernel(q, k, v, g, o, attn, s_k_h, s_k_t, s_v_h,\n    s_v_t, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NV: tl.constexpr, OUTPUT_ATTENTIONS: tl.\n    constexpr):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, 0), (BT, BS), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_s,\n            i_v * BV), (BS, BV), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gp = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_kg = (b_k * tl.exp(b_gn - b_g)).to(b_k.dtype)\n        b_s = tl.dot(b_q, b_kg, allow_tf32=False)\n        if i_s > 0:\n            b_o = b_o * tl.exp(b_gn - b_gp)\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n    tl.debug_barrier()\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_g, boundary_check=(0,))\n    b_o *= tl.exp(b_gq)[:, None]\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, i_t * BT), (BT, BS), (1, 0))\n    o_q = i_t * BT + tl.arange(0, BT)\n    o_k = i_t * BT + tl.arange(0, BS)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_s,\n            i_v * BV), (BS, BV), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.where(m_s, tl.dot(b_q, b_k, allow_tf32=False) * tl.exp(\n            b_gq[:, None] - b_gk[None, :]), 0)\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n        o_k += BS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_simple_gla_bwd_kernel_dq(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n    dq, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B: tl.constexpr, H: tl.\n    constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_s), (BV, BS), (0, 1))\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gp = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * tl.exp(b_gn - b_g)[None, :\n            ]\n        if i_s > 0:\n            b_dq *= tl.exp(b_gn - b_gp)\n        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k, allow_tf32=False)\n    p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_gq = tl.load(p_gq, boundary_check=(0,))\n    b_dq *= tl.exp(b_gq)[:, None] * scale\n    o_q = i_t * BT + tl.arange(0, BT)\n    o_k = i_t * BT + tl.arange(0, BS)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_s), (BV, BS), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.where(m_s, tl.dot(b_do, b_v, allow_tf32=False) * tl.exp(\n            b_gq[:, None] - b_gk[None, :]), 0) * scale\n        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        o_k += BS\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (i_v * B * H + i_bh) * s_k_h, (T, K), (\n        s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_v * B * H + i_bh) * T, (T,), (1,), (\n        i_t * BT,), (BT,), (0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg = tl.sum(b_dq * b_q, 1)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_simple_gla_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n    dk, dv, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B: tl.constexpr, H: tl.\n    constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.\n    constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    b_gk = tl.load(p_gk, boundary_check=(0,))\n    NTS = tl.cdiv(T, BS)\n    b_kg = (b_k * tl.exp(tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1) -\n        b_gk)[:, None]).to(b_k.dtype)\n    for i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_s, i_v * BV), (BS, BV), (1, 0))\n        p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        b_gp = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gn = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * tl.exp(b_gq - b_gn)[:, None]).to(b_do.dtype)\n        b_dk *= tl.exp(b_gp - b_gn)\n        b_dv *= tl.exp(b_gp - b_gn)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_s = tl.dot(b_kg, tl.trans(b_q), allow_tf32=False)\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_do.dtype), b_do, allow_tf32=False)\n    b_dk *= tl.exp(tl.load(g + i_bh * T + min(T, i_t * BT + BT) - 1) - b_gk)[\n        :, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q = i_t * BT + tl.arange(0, BS)\n    o_k = i_t * BT + tl.arange(0, BT)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_s, i_v * BV), (BS, BV), (1, 0))\n        p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.exp(-b_gk[:, None] + b_gq[None, :]), 0) * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False) * d_s\n        b_s = tl.dot(b_k, tl.trans(b_q), allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        o_q += BS\n    p_dk = tl.make_block_ptr(dk + (i_v * B * H + i_bh) * s_k_h, (T, K), (\n        s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * B * H + i_bh) * s_v_h, (T, V), (\n        s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_v * B * H + i_bh) * T, (T,), (1,), (\n        i_t * BT,), (BT,), (0,))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_dg = tl.load(p_dg, boundary_check=(0,))\n    b_dg -= tl.sum(b_dk * b_k, 1)\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV'])})\n@triton.jit\ndef parallel_simple_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, dg, s_k_h,\n    s_k_t, s_v_h, s_v_t, scale, B: tl.constexpr, H: tl.constexpr, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, NV: tl.constexpr):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    parallel_simple_gla_bwd_kernel_dq(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n        dq, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B=B, H=H, T=T, K=K, V=V,\n        BT=BT, BS=BS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    parallel_simple_gla_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n        dk, dv, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B, H, T, K, V, BT,\n        BS, BK, BV)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BM': 128, 'BK': 64, 'BN': 256,\n    'G': 4}, num_stages=3, num_warps=8), triton.Config({'BM': 64, 'BK': 32,\n    'BN': 256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': \n    128, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 32,\n    'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 32, 'BK': 32,\n    'BN': 64, 'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 256, 'G': 4}, num_stages=3, num_warps=8), triton.\n    Config({'BM': 256, 'BK': 128, 'BN': 128, 'G': 4}, num_stages=3,\n    num_warps=8), triton.Config({'BM': 256, 'BK': 128, 'BN': 64, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 128, 'BN': \n    256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 64, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 64, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 64, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4)], key=['M', 'N', 'K'])\n@triton.heuristics({'HAS_INPUT': lambda args: args['input'] is not None,\n    'HAS_ALPHA': lambda args: args['alpha'] is not None, 'HAS_BETA': lambda\n    args: args['beta'] is not None})\n@triton.jit\ndef matmul_kernel(a, b, c, input, alpha, beta, M, N, K, s_am, s_ak, s_bk,\n    s_bn, s_cm, s_cn, BM: tl.constexpr, BK: tl.constexpr, BN: tl.constexpr,\n    G: tl.constexpr, ACTIVATION: tl.constexpr, HAS_INPUT: tl.constexpr,\n    HAS_ALPHA: tl.constexpr, HAS_BETA: tl.constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    NM, NN = tl.num_programs(0), tl.num_programs(1)\n    i_m, i_n = tl.program_id(0), tl.program_id(1)\n    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)\n    o_am = (i_m * BM + tl.arange(0, BM)) % M\n    o_bn = (i_n * BN + tl.arange(0, BN)) % N\n    o_k = tl.arange(0, BK)\n    p_a = a + (o_am[:, None] * s_am + o_k[None, :] * s_ak)\n    p_b = b + (o_k[:, None] * s_bk + o_bn[None, :] * s_bn)\n    b_acc = tl.zeros((BM, BN), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BK)):\n        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)\n        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)\n        b_acc += tl.dot(b_a, b_b, allow_tf32=False)\n        p_a += BK * s_ak\n        p_b += BK * s_bk\n    o_cm = i_m * BM + tl.arange(0, BM)\n    o_cn = i_n * BN + tl.arange(0, BN)\n    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)\n    b_c = b_acc\n    if ACTIVATION == 'leaky_relu':\n        b_c = leaky_relu(b_c)\n    if HAS_ALPHA:\n        b_c *= tl.load(alpha)\n    if HAS_INPUT:\n        p_i = input + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n        b_i = tl.load(p_i, mask=mask, other=0.0).to(tl.float32)\n        if HAS_BETA:\n            b_i *= tl.load(beta)\n        b_c += b_i\n    p_c = c + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n    tl.store(p_c, b_c.to(c.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef logcumsumexp_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: tl.constexpr, S:\n    tl.constexpr, BT: tl.constexpr):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_mp = tl.full([S], float('-inf'), dtype=tl.float32)\n    b_zp = tl.zeros([S], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_mc = tl.max(b_s, 0)\n        if i_t > 0:\n            b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * tl.exp(b_mp - b_mc)\n        b_s = tl.exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n        b_z = tl.log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})\n@triton.jit\ndef logsumexp_fwd_kernel(x, z, scale, D: tl.constexpr, B: tl.constexpr,\n    HAS_SCALE: tl.constexpr):\n    i_n, i_d = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_cumsum_scalar_kernel(s, o, offsets, T: tl.constexpr, H: tl.\n    constexpr, BT: tl.constexpr, HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl.\n    constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n    else:\n        p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n    b_o = tl.cumsum(b_s, axis=0)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_reversed_cumsum_scalar_kernel(s, o, offsets, T: tl.\n    constexpr, H: tl.constexpr, BT: tl.constexpr, HEAD_FIRST: tl.constexpr,\n    USE_OFFSETS: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n    else:\n        p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n    b_z = tl.sum(b_s, axis=0)\n    b_o = b_z[None] - tl.cumsum(b_s, axis=0) + b_s\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S', 'BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_cumsum_vector_kernel(s, o, offsets, T: tl.constexpr, H: tl.\n    constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr,\n    HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl.constexpr):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n    else:\n        p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S', 'BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_reversed_cumsum_vector_kernel(s, o, offsets, T: tl.\n    constexpr, H: tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.\n    constexpr, HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl.constexpr):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n    else:\n        p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(s, o, offsets, T: tl.constexpr, H: tl\n    .constexpr, BT: tl.constexpr, HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl\n    .constexpr):\n    i_bh = tl.program_id(0)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n            p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n        else:\n            p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n            p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_z += tl.sum(b_s, axis=0)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(s, o, offsets, T: tl.\n    constexpr, H: tl.constexpr, BT: tl.constexpr, HEAD_FIRST: tl.constexpr,\n    USE_OFFSETS: tl.constexpr):\n    i_bh = tl.program_id(0)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n            p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n        else:\n            p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n            p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(s, z, offsets, T: tl.constexpr, H: tl\n    .constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr,\n    HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl.constexpr):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n        else:\n            p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(s, z, offsets, T: tl.\n    constexpr, H: tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.\n    constexpr, HEAD_FIRST: tl.constexpr, USE_OFFSETS: tl.constexpr):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b).to(tl.int32), tl.load(offsets +\n            i_b + 1).to(tl.int32)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n        else:\n            p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef softmax_fwd_kernel(x, p, D: tl.constexpr, B: tl.constexpr):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    b_m = tl.max(b_x, 0)\n    b_x = tl.exp(b_x - b_m)\n    b_p = b_x / tl.sum(b_x, 0)\n    tl.store(p + i_n * D + o_d, b_p.to(p.dtype.element_ty), mask=m_d)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef softmax_bwd_kernel(p, dp, ds, D: tl.constexpr, B: tl.constexpr):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n    b_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)\n    b_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)\n    b_pp = tl.sum(b_p * b_dp, 0)\n    b_ds = b_p * b_dp - b_p * b_pp\n    tl.store(ds + i_n * D + o_d, b_ds.to(ds.dtype.element_ty), mask=m_d)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(q, k, g, A, scale, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.\n    constexpr, BK: tl.constexpr, NC: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gk = tl.make_block_ptr(g + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC) * K + o_k, BK), BK)\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(q, k, g, A, scale, T: tl.\n    constexpr, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.\n    constexpr, BK: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * BT + i_j * BC\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, 0), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, 0), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n    else:\n        o_A = i_b * T * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * H * BT + i_h * BT + i_j * BC\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K if HEAD_FIRST else H * K\n        p_gk += K if HEAD_FIRST else H * K\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(q, k, g, A, scale, B: tl.\n    constexpr, T: tl.constexpr, H: tl.constexpr, K: tl.constexpr, BT: tl.\n    constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_A = (i_k * B * H + i_bh) * T * BC + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BC\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n    else:\n        o_A = (i_k * B + i_b) * T * H * BC + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * H * BC + i_h * BC\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K if HEAD_FIRST else H * K\n        p_gk += K if HEAD_FIRST else H * K\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(A, A2, B: tl.constexpr, T:\n    tl.constexpr, H: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, NK:\n    tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if i_t * BT + i_c * BC >= T:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        if HEAD_FIRST:\n            p_A = tl.make_block_ptr(A + (i_k * B * H + i_bh) * T * BC, (T,\n                BC), (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        else:\n            p_A = tl.make_block_ptr(A + (i_k * B + i_b) * T * H * BC + i_h *\n                BC, (T, BC), (H * BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC\n                ), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    if HEAD_FIRST:\n        p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    else:\n        p_A2 = tl.make_block_ptr(A2 + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_kernel_o(q, v, g, h, o, A, scale, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_intra(q, k, g, dA, dq, dk, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.\n    constexpr, NC: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        if HEAD_FIRST:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC) * K + o_k, BK), BK)\n        else:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(0, i_i):\n            if HEAD_FIRST:\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1),\n                    (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, \n                    1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            else:\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT,\n                    (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC),\n                    (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * BT + i_i * BC\n        p_kj = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_gkj = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        o_dA = i_b * T * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * H * BT + i_h * BT + i_i * BC\n        p_kj = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gkj = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n        p_kj += K if HEAD_FIRST else H * K\n        p_gkj += K if HEAD_FIRST else H * K\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.debug_barrier()\n    if HEAD_FIRST:\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    NC = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < NC - 1:\n        if HEAD_FIRST:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC + BC - 1) * K + o_k, BK), BK)\n        else:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC + BC - 1) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(i_i + 1, NC):\n            if HEAD_FIRST:\n                p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (BT, T), (1,\n                    BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n            else:\n                p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT,\n                    (BT, T), (1, H * BT), (i_i * BC, i_t * BT + i_j * BC),\n                    (BC, BC), (0, 1))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(b_dA, b_qg)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    if HEAD_FIRST:\n        o_dA = i_bh * T * BT + (i_t * BT + i_i * BC\n            ) * BT + i_i * BC + tl.arange(0, BC)\n        p_qj = tl.max_contiguous(tl.multiple_of(q + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_gqj = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        o_dA = i_b * T * H * BT + (i_t * BT + i_i * BC\n            ) * H * BT + i_h * BT + i_i * BC + tl.arange(0, BC)\n        p_qj = tl.max_contiguous(tl.multiple_of(q + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gqj = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j * (1 if HEAD_FIRST else H) * BT)\n        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n        p_qj += K if HEAD_FIRST else H * K\n        p_gqj += K if HEAD_FIRST else H * K\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dA(v, do, dA, scale, T: tl.constexpr, H: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BV: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v *\n                BV, i_t * BT), (BV, BT), (0, 1))\n        else:\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (V, T),\n                (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, b_v)\n    if HEAD_FIRST:\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_dA = tl.where(m_s, b_dA * scale, 0.0)\n    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dv(k, g, A, do, dh, dv, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NT: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, \n            i_t * BT), (BT, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (BT, T), (\n            1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0.0)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(b_A, b_do.to(b_A.dtype), allow_tf32=False)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + min(\n                i_t * BT + BT, T) * K - K + o_k, BK), BK)\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k, BK), BK)\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_gn = tl.exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n        b_k = (b_k * b_gn).to(b_k.dtype)\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_inter(q, k, v, h, g, do, dh, dq, dk, dq2, dk2, dg,\n    scale, T: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl\n    .constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    if HEAD_FIRST:\n        p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (min(T, \n            i_t * BT + BT) - 1) * K + o_k, BK), BK)\n    else:\n        p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (min(\n            T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * V * K, (V, K),\n            (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * V * K, (V,\n            K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * tl.exp(b_gn[None, :] - b_gk)\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :]\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq2 + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk2 + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq2 + i_b * T * H * K + i_h * K, (T, K), (\n            H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk2 + i_b * T * H * K + i_h * K, (T, K), (\n            H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef prepare_qg_kg(q, k, g, qg, kg, s_k_h, scale, K: tl.constexpr, BT: tl.\n    constexpr, BK: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_decay = tl.load(g + i_bh * s_k_h + (i_c * BT + BT - 1) * K + i_k *\n        BK + tl.arange(0, BK))\n    for i in range(BT):\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_q *= tl.exp(_g) * scale\n        b_k *= tl.exp(last_decay - _g)\n        tl.store(p_kg, b_k.to(p_kg.dtype.element_ty), mask=mask)\n        tl.store(p_qg, b_q.to(p_qg.dtype.element_ty), mask=mask)\n        p_q += K\n        p_g += K\n        p_k += K\n        p_kg += K\n        p_qg += K\n"
    },
    {
      "input": "@triton.jit\ndef bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g,\n    dg, s_k_h, BT: tl.constexpr, BK: tl.constexpr, K: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_g = g + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_dg = dg + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_dq_inner = dq_inner + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inner = dk_inner + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dq_inter = dq_inter + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inter = dk_inter + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT - 1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        if j == BT - 1:\n            last_g = _g\n        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        b_dq2 *= tl.exp(_g)\n        b_dq = b_dq1 + b_dq2\n        tl.store(p_dq_inter, b_dq, mask=mask)\n        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        b_dk2 *= tl.exp(last_g - _g)\n        b_dk = b_dk1 + b_dk2\n        tl.store(p_dk_inter, b_dk, mask=mask)\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        b_dg = b_dq * b_q - b_dk * b_k\n        cum_grad_dg += b_dg\n        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)\n        p_g -= K\n        p_k -= K\n        p_q -= K\n        p_dq_inner -= K\n        p_dk_inner -= K\n        p_dq_inter -= K\n        p_dk_inter -= K\n        p_dg -= K\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, h0, ht, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr,\n    K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr,\n    BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE:\n    tl.constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_k_h + (BT - 1) * s_k_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0).to(tl.float32)\n        if CHECK and i == 0:\n            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False\n                )\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k.to(b_v.dtype),\n                b_v, allow_tf32=False)\n        else:\n            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False\n                )\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k.to(b_v.dtype),\n                b_v, allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * K\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=\n            (0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, h0, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, CHECK:\n    tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_k_h + ((i + 1) * BT - 1\n            ) * s_k_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0).to(tl.float32)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k.to(b_v.dtype\n                ), allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k.to(b_v.dtype\n                ), allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_k_h + (T - (i - 1) * BT - 1\n            ) * s_k_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_db = tl.load(p_db, mask=mask, other=0).to(tl.float32)\n        if CHECK and i == 1:\n            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v),\n                allow_tf32=False))\n            b_dv = tl.dot(b_k.to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32\n                =False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q.to(b_do.dtype),\n                b_do, allow_tf32=False)\n        else:\n            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v),\n                allow_tf32=False))\n            b_dv = tl.dot(b_k.to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32\n                =False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q.to(b_do.dtype),\n                b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fwd_inner_chunk(q, k, g, A, s_k_h, s_k_t, s_k_d, scale, B: tl.constexpr,\n    H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_A = A + (i_bh + i_k * B * H) * (tl.cdiv(T, BT) * BT * BT\n        ) + i_t * BT * BT + tl.arange(0, BT)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0) * scale\n        gq = tl.load(p_gq, mask=mask, other=0).to(tl.float32)\n        s = _q[None, :] * b_k * tl.exp(gq[None, :] - b_g)\n        score = tl.sum(s, axis=1)\n        score = tl.where(o_i <= i, score, 0)\n        tl.store(p_A, score.to(p_A.dtype.element_ty))\n        p_q += K\n        p_gq += K\n        p_A += BT\n"
    },
    {
      "input": "@triton.jit\ndef bwd_inner_chunk(q, k, g, dA, dq, dk, s_k_h, s_k_t, s_k_d, T: tl.\n    constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dq = dq + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dA = dA + i_bh * (tl.cdiv(T, BT) * BT * BT) + i_t * BT * BT + tl.arange(\n        0, BT)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        gq = tl.load(p_gq, mask=mask, other=0).to(tl.float32)\n        score = tl.exp(gq[None, :] - b_g)\n        score = tl.where(o_i[:, None] <= i, score, 0)\n        _dA = tl.load(p_dA)\n        _dA = tl.where(o_i <= i, _dA, 0)\n        b_dk += _dA[:, None] * score * _q[None, :]\n        b_dq = tl.sum(_dA[:, None] * score * b_k, axis=0)\n        tl.store(p_dq, b_dq, mask=mask)\n        p_q += K\n        p_dq += K\n        p_gq += K\n        p_dA += BT\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk.to(dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef chunk_fwd_kernel_h(k, v, h, g, gk, gv, h0, ht, T: tl.constexpr, H: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, NT: tl.constexpr, USE_G: tl.constexpr,\n    USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.\n    constexpr, STORE_FINAL_STATE: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min((i_t + 1) * BT, T) - 1\n        if USE_G:\n            if HEAD_FIRST:\n                b_g_last = tl.load(g + i_bh * T + last_idx)\n                p_g = g + i_bh * T + i_t * BT + tl.arange(0, BT)\n                p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n            else:\n                b_g_last = tl.load(g + i_b * T * H + last_idx * H + i_h)\n                p_g = g + i_b * T * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_h *= tl.exp(b_g_last)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_v = (b_v * tl.exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n        if USE_GK:\n            if HEAD_FIRST:\n                p_gk = tl.make_block_ptr(gk + i_bh * T * K, (K, T), (1, K),\n                    (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_bh * T * K + last_idx * K + i_k * BK +\n                    tl.arange(0, BK))\n            else:\n                p_gk = tl.make_block_ptr(gk + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_b * T * H * K + last_idx * H * K + i_h *\n                    K + i_k * BK + tl.arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_h *= tl.exp(b_gk_last)[:, None]\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_k = (b_k * tl.exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n        if USE_GV:\n            if HEAD_FIRST:\n                p_gv = tl.make_block_ptr(gv + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_bh * T * V + last_idx * V + i_v * BV +\n                    tl.arange(0, BV))\n            else:\n                p_gv = tl.make_block_ptr(gv + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_b * T * H * V + last_idx * H * V + i_h *\n                    V + i_v * BV + tl.arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_h *= tl.exp(b_gv_last)[None, :]\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_v = (b_v * tl.exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n        b_h += tl.dot(b_k, b_v)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_bwd_kernel_dh(q, g, gk, gv, do, dh, dht, dh0, scale, T: tl.\n    constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl\n    .constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * HQ * K + i_hq * K, (K, T),\n                (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_b * T * HQ * V + i_hq * V, (T,\n                V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if USE_G:\n            if HEAD_FIRST:\n                p_g = g + i_bg * T + i_t * BT + tl.arange(0, BT)\n                p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n                b_g_last = tl.load(g + i_bg * T + last_idx)\n            else:\n                p_g = g + i_b * T * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n                b_g_last = tl.load(g + i_b * T * H + last_idx * H + i_h)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_q = (b_q * tl.exp(b_g)[None, :]).to(b_q.dtype)\n            b_dh *= tl.exp(b_g_last)\n        if USE_GK:\n            if HEAD_FIRST:\n                p_gk = tl.make_block_ptr(gk + i_bg * T * K, (K, T), (1, K),\n                    (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_bg * T * K + last_idx * K + i_k * BK +\n                    tl.arange(0, BK))\n            else:\n                p_gk = tl.make_block_ptr(gk + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_b * T * H * K + last_idx * H * K + i_h *\n                    K + i_k * BK + tl.arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_q = (b_q * tl.exp(b_gk)).to(b_q.dtype)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_dh *= tl.exp(b_gk_last)[:, None]\n        if USE_GV:\n            if HEAD_FIRST:\n                p_gv = tl.make_block_ptr(gv + i_bg * T * V, (T, V), (V, 1),\n                    (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_bg * T * V + last_idx * V + i_v * BV +\n                    tl.arange(0, BV))\n            else:\n                p_gv = tl.make_block_ptr(gv + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_b * T * H * V + last_idx * H * V + i_h *\n                    V + i_v * BV + tl.arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_do = (b_do * tl.exp(b_gv)).to(b_do.dtype)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_dh *= tl.exp(b_gv_last)[None, :]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV',\n    'USE_GK', 'USE_GV', 'USE_G'])\n@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef fused_recurrent_fwd_kernel(q, k, v, g, gk, gv, o, h0, ht, scale, B: tl.\n    constexpr, T: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.\n    constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr,\n    USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr,\n    HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.\n        int64), tl.program_id(2).to(tl.int64)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + ((T - 1) * V if REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B * H + i_bh) * T * V + ((T - 1) * V if REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        if USE_G:\n            p_g = g + i_bh * T + (T - 1 if REVERSE else 0)\n        if USE_GK:\n            p_gk = gk + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n                ) + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_bh * T * V + ((T - 1) * V if REVERSE else 0\n                ) + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + ((T - 1) * H * V if REVERSE else 0\n            ) + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B + i_b) * T * H * V + ((T - 1) * H * V if REVERSE\n             else 0) + i_h * V + i_v * BV + tl.arange(0, BV)\n        if USE_G:\n            p_g = g + i_b * T * H + ((T - 1) * H if REVERSE else 0) + i_h\n        if USE_GK:\n            p_gk = gk + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n                ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_b * T * H * V + ((T - 1) * H * V if REVERSE else 0\n                ) + i_h * V + i_v * BV + tl.arange(0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_h = b_h * tl.exp(b_gk[None, :])\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_h = b_h * tl.exp(b_gv[:, None])\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_h = b_h * tl.exp(b_g)\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        p_k += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        p_v += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n        p_o += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n        if USE_GK:\n            p_gk += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        if USE_GV:\n            p_gv += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n        if USE_G:\n            p_g += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H)\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV',\n    'USE_GK', 'USE_GV', 'USE_G'])\n@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not\n    None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None})\n@triton.jit\ndef fused_recurrent_bwd_kernel(q, k, v, g, gk, gv, h0, do, dq, dk, dv, dht,\n    dh0, scale, B: tl.constexpr, T: tl.constexpr, H: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE:\n    tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.\n    constexpr, USE_INITIAL_STATE: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT:\n    tl.constexpr, HEAD_FIRST: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.\n        int64), tl.program_id(2).to(tl.int64)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + ((T - 1) * V if REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_bh * T * V + ((T - 1) * V if REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B * H + i_bh) * T * K + ((T - 1) * K if REVERSE else\n            0) + i_k * BK + tl.arange(0, BK)\n        if USE_G:\n            p_g = g + i_bh * T + (T - 1 if REVERSE else 0)\n        if USE_GK:\n            p_gk = gk + i_bh * T * K + ((T - 1) * K if REVERSE else 0\n                ) + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_bh * T * V + ((T - 1) * V if REVERSE else 0\n                ) + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + ((T - 1) * H * V if REVERSE else 0\n            ) + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_b * T * H * V + ((T - 1) * H * V if REVERSE else 0\n            ) + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B + i_b) * T * H * K + ((T - 1) * H * K if\n            REVERSE else 0) + i_h * K + i_k * BK + tl.arange(0, BK)\n        if USE_G:\n            p_g = g + i_b * T * H + ((T - 1) * H if REVERSE else 0) + i_h\n        if USE_GK:\n            p_gk = gk + i_b * T * H * K + ((T - 1) * H * K if REVERSE else 0\n                ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_b * T * H * V + ((T - 1) * H * V if REVERSE else 0\n                ) + i_h * V + i_v * BV + tl.arange(0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_h = b_h * tl.exp(b_gk[:, None])\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_h = b_h * tl.exp(b_gv[None, :])\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_h = b_h * tl.exp(b_g)\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = b_h * b_do[None, :]\n        b_dq = tl.sum(b_dq, axis=1) * scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n        p_q += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        p_k += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        p_v += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n        p_do += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n        p_dq += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        if USE_G:\n            p_g += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H)\n        if USE_GK:\n            p_gk += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * K\n        if USE_GV:\n            p_gv += (-1 if REVERSE else 1) * (1 if HEAD_FIRST else H) * V\n    tl.debug_barrier()\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + ((T - 1) * K if not REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + ((T - 1) * K if not REVERSE else 0\n            ) + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + ((T - 1) * V if not REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_bh * T * V + ((T - 1) * V if not REVERSE else 0\n            ) + i_v * BV + tl.arange(0, BV)\n        p_dk = dk + (i_v * B * H + i_bh) * T * K + ((T - 1) * K if not\n            REVERSE else 0) + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B * H + i_bh) * T * V + ((T - 1) * V if not\n            REVERSE else 0) + i_v * BV + tl.arange(0, BV)\n        if USE_G:\n            p_g = g + i_bh * T + (T - 1 if not REVERSE else 0)\n        if USE_GK:\n            p_gk = gk + i_bh * T * K + ((T - 1) * K if not REVERSE else 0\n                ) + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_bh * T * V + ((T - 1) * V if not REVERSE else 0\n                ) + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + ((T - 1) * H * K if not REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + ((T - 1) * H * K if not REVERSE else 0\n            ) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + ((T - 1) * H * V if not REVERSE else 0\n            ) + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_b * T * H * V + ((T - 1) * H * V if not REVERSE else 0\n            ) + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dk = dk + (i_v * B + i_b) * T * H * K + ((T - 1) * H * K if not\n            REVERSE else 0) + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B + i_b) * T * H * V + ((T - 1) * H * V if not\n            REVERSE else 0) + i_h * V + i_v * BV + tl.arange(0, BV)\n        if USE_G:\n            p_g = g + i_b * T * H + (T - 1 if not REVERSE else 0) + i_h\n        if USE_GK:\n            p_gk = gk + i_b * T * H * K + ((T - 1) * H * K if not REVERSE else\n                0) + i_h * K + i_k * BK + tl.arange(0, BK)\n        if USE_GV:\n            p_gv = gv + i_b * T * H * V + ((T - 1) * H * V if not REVERSE else\n                0) + i_h * V + i_v * BV + tl.arange(0, BV)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_dh += tl.load(p_dht, mask=mask_h, other=0).to(tl.float32)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_dh *= tl.exp(b_gk)[:, None]\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_dh *= tl.exp(b_gv)[None, :]\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_dh *= tl.exp(b_g)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n        p_q += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * K\n        p_k += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * K\n        p_v += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * V\n        p_do += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * V\n        p_dk += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * K\n        p_dv += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * V\n        if USE_G:\n            p_g += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H)\n        if USE_GK:\n            p_gk += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * K\n        if USE_GV:\n            p_gv += (1 if REVERSE else -1) * (1 if HEAD_FIRST else H) * V\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(k, v, h, h0, ht, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, T: tl.constexpr, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT:\n    tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.\n    constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(q, k, v, h, o, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, scale, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(q, do, dh, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, scale, T: tl.constexpr, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT:\n    tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do.to(b_q.dtype), allow_tf32=False)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, scale, T: tl.constexpr,\n    K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr,\n    BV: tl.constexpr, NT: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s.to(b_q.\n            dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0).to(b_q.dtype\n        )\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B, H, T, K: tl.constexpr, V: tl.\n    constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, CHECK:\n    tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B, H, T, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, CHECK: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_v_h,\n    scale, B, H, T, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV:\n    tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.\n    constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0,\n    s_k_h, s_v_h, scale, B, H, T, K: tl.constexpr, V: tl.constexpr, BK: tl.\n    constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_h += b_k[:, None] * b_v[None, :]\n        _d_q = b_h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dq += K\n    tl.debug_barrier()\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (T\n         - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (T\n         - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B, H, T, K: tl.constexpr, V: tl.constexpr, BTL: tl\n    .constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i_c * BTL + tl.arange(\n        0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl\n    .constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.\n    constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H:\n    tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL:\n    tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q),\n            allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q),\n            allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS:\n    tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B=B, H=H, T=T, K=K,\n        V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B=B, H=H, T=T,\n        K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr,\n    K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr,\n    BV: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT]).to(b_k.dtype)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(b_k.dtype)\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o.to(b_q.dtype), allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK]).to(b_k.dtype)\n        b_o += tl.dot(b_q_2o, b_h_2o.to(b_q_2o.dtype), allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i * BT + tl.arange\n            (0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o.to(b_v.dtype), b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK:\n    tl.constexpr, BV: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o.to(b_do.dtype), allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o.to(b_do.dtype), allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot((b_ds * (1 + b_s)).to(b_q.dtype), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK]).to(b_k.dtype)\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o.to(b_v.dtype), allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = (b_q * scale).to(b_k.dtype)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2.to(b_do.dtype), b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK]).to(b_k.dtype)\n        b_dv += tl.dot(b_k, b_dh_1o.to(b_k.dtype), allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o.to(b_k.dtype), allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o).to(b_k.dtype), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o.to(b_k.dtype), tl.trans(b_v), allow_tf32=False\n            )\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k.to(tl.float32))\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT]).to(b_k.dtype)\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr,\n    K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr,\n    BK: tl.constexpr, BV: tl.constexpr):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i_c * BTL + tl.arange(\n        0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL: tl.\n    constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, K: tl\n    .constexpr, V: tl.constexpr):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot((b_ds * (1 + b_s)).to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot((b_ds + b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False\n            )\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL: tl.\n    constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, K: tl\n    .constexpr, V: tl.constexpr):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot((b_ds + b_ds * b_s).to(b_q.dtype), tl.trans(b_q),\n            allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot((b_ds + b_ds * b_s).to(b_q.dtype), tl.trans(b_q),\n            allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: tl.constexpr, H: tl.constexpr, T:\n    tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS:\n    tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL=BTL,\n        BTS=BTS, BK=BK, BV=BV, K=K, V=V)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL,\n        BTS, BK, BV, K, V)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_rwkv6_fwd_cumsum_kernel(s, o, o_minus_s, s_s_h, s_s_t, s_s_d, T:\n    tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o_minus_s = tl.make_block_ptr(o_minus_s + i_bh * s_s_h, (T, S), (\n        s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_o_minus_s, (b_o - b_s).to(p_o_minus_s.dtype.element_ty),\n        boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_inter(q, k, gi, ge, A, s_k_h, s_k_t,\n    s_k_d, scale, T: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC:\n    tl.constexpr, BK: tl.constexpr, NC: tl.constexpr):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_i <= i_j:\n        return\n    if i_t * BT + i_i * BC >= T:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gq = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gq - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra(q, k, gi, ge, u, A, s_k_h,\n    s_k_t, s_k_d, scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr,\n    BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_j * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    i_k = 0\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(q, k, gi, ge, u, A,\n    s_k_h, s_k_t, s_k_d, scale, H: tl.constexpr, T: tl.constexpr, K: tl.\n    constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl\n    .constexpr):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0,\n        BC)) * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(A, A2, T: tl.constexpr,\n    BT: tl.constexpr, BC: tl.constexpr, NK: tl.constexpr):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k * n_bh) * T * BC, (T, BC),\n            (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_kernel_inter(q, v, g, h, o, A, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: tl.constexpr, K: tl\n    .constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl\n    .constexpr):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_ge = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_g = tl.load(p_ge, boundary_check=(0, 1))\n        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_A = tl.where(m_s, b_A, 0.0)\n    b_o += tl.dot(b_A.to(b_v.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_intra(q, k, gi, ge, dA, dq, dk, s_k_h, s_k_t,\n    s_k_d, scale, T: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC:\n    tl.constexpr, BK: tl.constexpr, NC: tl.constexpr):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_q = i_t * BT + i_i * BC\n    m_k = o_k < K\n    p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_ge = tl.load(p_ge, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    o_i = tl.arange(0, BC)\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        b_gn = tl.load(gi + i_bh * T * K + (o_q - 1) * K + o_k, mask=m_k &\n            (i_i > 0) & (o_q <= T), other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_ge - b_gn[None, :])\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gkj = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)\n        b_gkj = tl.load(p_gkj, boundary_check=(0,)).to(tl.float32)\n        m_i = o_i[:, None] > j\n        tmp = tl.exp(b_ge - b_gkj[None, :])\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tmp, 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.debug_barrier()\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    max_block_idx = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < max_block_idx - 1:\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        for i_j in range(i_i + 1, NC):\n            p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_ge = tl.load(p_ge, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_ge - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(tl.trans(b_dA), b_qg, allow_tf32=False)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gqj = tl.make_block_ptr(ge + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)\n        b_gqj = tl.load(p_gqj, boundary_check=(0,)).to(tl.float32)\n        m_i = o_i[:, None] < j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_inter(q, k, v, h, gi, ge, u, do, dh, dA, dq, dk,\n    dq2, dk2, dg, du, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h,\n    s_h_t, s_h_d, scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr,\n    V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    last_idx = min(T, i_t * BT + BT) - 1\n    p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), (\n        last_idx * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    p_gk = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    p_gi = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * tl.exp(b_gn[None, :] - b_gi)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :] - b_q * b_dq\n    o_i = tl.arange(0, BT)\n    p_dA_dig = dA + i_bh * T * BT + (i_t * BT + o_i) * BT + o_i\n    b_dA_dig = tl.load(p_dA_dig, mask=i_t * BT + o_i < T, other=0)\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k\n    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q\n    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)\n    p_du = tl.make_block_ptr(du + (i_h + i_t * n_bh) * K, (K,), (1,), (i_k *\n        BK,), (BK,), (0,))\n    tl.store(p_du, b_du, boundary_check=(0,))\n    p_dg = tl.make_block_ptr(dg + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_dh(q, gi, ge, do, dh, dht, dh0, s_k_h, s_k_t,\n    s_v_h, s_v_t, s_h_h, s_h_t, scale, T: tl.constexpr, K: tl.constexpr, V:\n    tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT:\n    tl.constexpr, NG: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.\n    constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        p_gk = tl.make_block_ptr(ge + i_bg * s_k_h, (K, T), (1, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = (b_q * tl.exp(b_gk) * scale).to(b_q.dtype)\n        p_gk_last = gi + i_bg * s_k_h + last_idx * K + i_k * BK + tl.arange(\n            0, BK)\n        p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K,\n            other=0.0)\n        b_dh *= tl.exp(b_gk_last)[:, None]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(q, k, v, w, u, o, h0, ht, s_k_h, s_v_h,\n    scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.\n    constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr,\n    REVERSE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    b_u = tl.load(p_u, mask=mask_bk, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0).to(tl.float32)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dq(k, v, w, u, do, dq, dq_aux, h0,\n    s_k_h, s_v_h, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr,\n    K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, REVERSE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if REVERSE else 0)\n    p_dq_aux = dq_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_u = tl.load(p_u, mask=mask_bk, other=0).to(tl.float32)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_w = tl.load(p_w, mask=mask_bk, other=0).to(tl.float32)\n        b_w = tl.exp(b_w)\n        h_q = b_h * b_do[:, None]\n        b_dq = tl.sum(h_q + b_kv * b_u[None, :] * b_do[:, None], axis=0)\n        b_dq *= scale\n        b_dq_aux = tl.sum(h_q, axis=0)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dq_aux, b_dq_aux.to(p_dq_aux.dtype.element_ty), mask=mask_bk\n            )\n        p_k += -K if REVERSE else K\n        p_do += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n        p_dq += -K if REVERSE else K\n        p_dq_aux += -K if REVERSE else K\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dkv(q, k, v, w, u, do, dk, dk_aux, dv,\n    dh0, s_k_h, s_v_h, scale, B: tl.constexpr, H: tl.constexpr, T: tl.\n    constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.\n    constexpr, USE_INITIAL_STATE: tl.constexpr, REVERSE: tl.constexpr):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if \n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if not REVERSE else 0)\n    p_dk_aux = dk_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if not REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    b_u = tl.load(p_u, mask=mask_bk, other=0).to(tl.float32)\n    for _ in range(T - 1, -1, -1):\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_w = tl.load(p_w, mask=mask_bk, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_dkv = b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        tl.store(p_dk_aux, b_dk.to(p_dk_aux.dtype.element_ty), mask=mask_bk)\n        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], axis=1)\n        b_dv = tl.sum((b_dh + b_dkv * b_u[:, None]) * b_k[:, None], axis=0)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_bv)\n        b_dh *= tl.exp(b_w)[:, None]\n        b_dh += b_dkv\n        p_q += K if REVERSE else -K\n        p_k += K if REVERSE else -K\n        p_v += V if REVERSE else -V\n        p_w += K if REVERSE else -K\n        p_do += V if REVERSE else -V\n        p_dk += K if REVERSE else -K\n        p_dk_aux += K if REVERSE else -K\n        p_dv += V if REVERSE else -V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef cross_entropy_kernel(logits, lse, target, loss, total, ignore_index,\n    label_smoothing: tl.constexpr, logit_scale: tl.constexpr, reduction: tl\n    .constexpr, V: tl.constexpr, BV: tl.constexpr):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now.\n    Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Args:\n        logits:\n            Pointer to logits tensor.\n        lse:\n            Pointer to logsumexp tensor.\n        target: Pointer to target tensor.\n        loss:\n            Pointer to tensor to store the loss.\n        V (int):\n            The number of columns in the input tensor.\n        total (int):\n            The number of non-ignored classes.\n        ignore_index (int):\n            The index to ignore in the target.\n        label_smoothing (float):\n            The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n        reduction (str):\n            The string for the reduction to apply\n        BV (int):\n            The block size for vocab.\n    \"\"\"\n    i_n = tl.program_id(0).to(tl.int64)\n    NV = tl.cdiv(V, BV)\n    b_y = tl.load(target + i_n)\n    logits += i_n * V\n    if b_y == ignore_index:\n        for i in range(0, V, BV):\n            o_v = i + tl.arange(0, BV)\n            tl.store(logits + o_v, 0.0, mask=o_v < V)\n        return\n    b_l = tl.load(logits + b_y) * logit_scale\n    b_lse = tl.load(lse + i_n)\n    b_loss = b_lse - b_l\n    b_z = 0.0\n    eps = label_smoothing / V\n    tl.debug_barrier()\n    for iv in range(0, NV):\n        o_v = iv * BV + tl.arange(0, BV)\n        b_logits = tl.load(logits + o_v, mask=o_v < V, other=float('-inf')\n            ) * logit_scale\n        if label_smoothing > 0:\n            b_z += tl.sum(tl.where(o_v < V, -eps * b_logits, 0.0))\n        b_p = (tl.exp(b_logits - b_lse) - eps) * logit_scale\n        if reduction == 'mean':\n            b_p = b_p / total\n        tl.store(logits + o_v, b_p, mask=o_v < V)\n        tl.debug_barrier()\n    if label_smoothing > 0:\n        b_loss = b_loss * (1 - label_smoothing) + (b_z + label_smoothing *\n            b_lse)\n    b_l = tl.load(logits + b_y)\n    if reduction == 'mean':\n        b_loss = b_loss / total\n        b_l += (label_smoothing - 1) / total * logit_scale\n    else:\n        b_l += (label_smoothing - 1) * logit_scale\n    tl.store(loss + i_n, b_loss)\n    tl.store(logits + b_y, b_l)\n"
    },
    {
      "input": "@triton.jit\ndef elementwise_mul_kernel(x, g, N: tl.constexpr, B: tl.constexpr):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by x with the value pointed by g.\n    The multiplication is performed in-place on the tensor pointed by x.\n\n    Parameters:\n    x:\n        Pointer to the input tensor.\n    g:\n        Pointer to the gradient output value.\n    N (int):\n        The number of columns in the input tensor.\n    B (int):\n        The block size for Triton operations.\n    \"\"\"\n    i_x = tl.program_id(0).to(tl.int64)\n    o_x = i_x * B + tl.arange(0, B)\n    b_g = tl.load(g)\n    b_x = tl.load(x + o_x, mask=o_x < N)\n    tl.store(x + o_x, b_x * b_g, mask=o_x < N)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(X, Y, stride_x_row, N, eps, BLOCK_N: tl.constexpr\n    ):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    y = x * rstd\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_bwd_kernel(X, DY, DX, stride_x_row, N, eps, BLOCK_N: tl.constexpr\n    ):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0).to(tl.float32)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var + eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    G, eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL:\n    tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_WEIGHT: tl.\n    constexpr, HAS_BIAS: tl.constexpr):\n    row = tl.program_id(0)\n    group = row % G\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl\n            .float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + group * stride_x_row + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + group * stride_x_row + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, G,\n    rows_per_program, programs_per_group, IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl\n    .constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr):\n    row_block_id = tl.program_id(0)\n    group_id, program_id_in_group = (row_block_id // programs_per_group, \n        row_block_id % programs_per_group)\n    row_start = group_id + program_id_in_group * G * rows_per_program\n    row_end = min(row_start + G * rows_per_program, M)\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + group_id * stride_x_row + cols, mask=mask).to(tl.\n            float32)\n        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + group_id * stride_x_row + cols, mask=mask, other=0.0\n            ).to(tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    for row in range(row_start, row_end, G):\n        x = tl.load(X + row * stride_x_row + cols, mask=mask, other=0).to(tl\n            .float32)\n        dy = tl.load(DY + row * stride_dy_row + cols, mask=mask, other=0).to(tl\n            .float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w if HAS_WEIGHT else xhat\n            if HAS_BIAS:\n                y = y + b\n            tl.store(Y + row * stride_y_row + cols, y, mask=mask)\n        wdy = dy\n        if HAS_WEIGHT:\n            wdy = dy * w\n            dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + row * stride_dres_row + cols, mask=\n                mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + row * stride_dres_in_row + cols, dx,\n                mask=mask)\n        tl.store(DX + row * stride_dx_row + cols, dx, mask=mask)\n    if HAS_WEIGHT:\n        tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef kl_div_kernel(logits, target_logits, loss, s_logits, s_loss, reduction:\n    tl.constexpr, N: tl.constexpr, V: tl.constexpr, BV: tl.constexpr):\n    i_n = tl.program_id(0).to(tl.int64)\n    logits += i_n * s_logits\n    target_logits += i_n * s_logits\n    sm, tm = float('-inf'), float('-inf')\n    sd, td = 0.0, 0.0\n    NV = tl.cdiv(V, BV)\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sm = tl.max(b_sl)\n        m_new = tl.maximum(sm, b_sm)\n        sd = sd * tl.exp(sm - m_new) + tl.sum(tl.exp(b_sl - m_new))\n        sm = m_new\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tm = tl.max(b_tl)\n        m_new = tl.maximum(tm, b_tm)\n        td = td * tl.exp(tm - m_new) + tl.sum(tl.exp(b_tl - m_new))\n        tm = m_new\n    b_loss = 0.0\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sp_log = b_sl - sm - tl.log(sd)\n        b_tp_log = b_tl - tm - tl.log(td)\n        b_sp = tl.exp(b_sp_log)\n        b_tp = tl.exp(b_tp_log)\n        b_kl = tl.where(o_x < V, b_tp * (b_tp_log - b_sp_log), 0)\n        b_dl = -b_tp + b_sp\n        b_loss += tl.sum(b_kl)\n        if reduction == 'batchmean':\n            b_dl = b_dl / N\n        tl.store(logits + o_x, b_dl, mask=o_x < V)\n    if reduction == 'batchmean':\n        b_loss = b_loss / N\n    tl.store(loss + i_n * s_loss, b_loss)\n"
    },
    {
      "input": "@triton.jit\ndef elementwise_mul_kernel(x, g, N: tl.constexpr, B: tl.constexpr):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by x with the value pointed by g.\n    The multiplication is performed in-place on the tensor pointed by x.\n\n    Parameters:\n    x:\n        Pointer to the input tensor.\n    g:\n        Pointer to the gradient output value.\n    N (int):\n        The number of columns in the input tensor.\n    B (int):\n        The block size for Triton operations.\n    \"\"\"\n    i_x = tl.program_id(0).to(tl.int64)\n    o_x = i_x * B + tl.arange(0, B)\n    b_g = tl.load(g)\n    b_x = tl.load(x + o_x, mask=o_x < N)\n    tl.store(x + o_x, b_x * b_g, mask=o_x < N)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, O, Y, W, B, RESIDUAL, RESIDUAL_OUT,\n    Mean, Rstd, stride_x_row, stride_y_row, stride_res_row,\n    stride_res_out_row, N, eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.\n    constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    O += row * stride_x_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl\n            .float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    o = tl.load(O + cols, mask=cols < N, other=0.0).to(tl.float32)\n    y = y * o * tl.sigmoid(o)\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, O, W, B, Y, DY, DX, DO, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_WEIGHT:\n    tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    O += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    DO += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        o = tl.load(O + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        y = xhat * w if HAS_WEIGHT else xhat\n        if HAS_BIAS:\n            y = y + b\n        if RECOMPUTE_OUTPUT:\n            tl.store(Y + cols, y, mask=mask)\n        sigmoid_o = tl.sigmoid(o)\n        do = dy * y * (sigmoid_o + o * sigmoid_o * (1 - sigmoid_o))\n        dy = dy * o * sigmoid_o\n        wdy = dy\n        if HAS_WEIGHT:\n            wdy = dy * w\n            dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        tl.store(DO + cols, do, mask=mask)\n        X += stride_x_row\n        O += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        DO += stride_dx_row\n    if HAS_WEIGHT:\n        tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef logsigmoid_fwd_kernel(x, y, temperature, T: tl.constexpr, D: tl.\n    constexpr, B: tl.constexpr):\n    i = tl.program_id(0)\n    o_i = i * B + tl.arange(0, B)\n    m_i = o_i < T\n    b_x = tl.load(x + o_i, mask=m_i, other=0.0).to(tl.float32)\n    b_m = tl.minimum(0.0, b_x)\n    b_z = 1.0 + tl.exp(-tl.abs(b_x))\n    b_y = (b_m - tl.log(b_z)) / temperature\n    tl.store(y + o_i, b_y.to(y.dtype.element_ty), mask=m_i)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef logsigmoid_bwd_kernel(x, dx, dy, temperature, T: tl.constexpr, D: tl.\n    constexpr, B: tl.constexpr):\n    i = tl.program_id(0)\n    o_i = i * B + tl.arange(0, B)\n    m_i = o_i < T\n    b_x = tl.load(x + o_i, mask=m_i, other=0.0).to(tl.float32)\n    b_dy = tl.load(dy + o_i, mask=m_i, other=0.0).to(tl.float32)\n    b_dx = b_dy * (1.0 - tl.sigmoid(b_x)) / temperature\n    tl.store(dx + o_i, b_dx.to(dx.dtype.element_ty), mask=m_i)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, n_rows,\n    logits_row_stride, BLOCK_SIZE: tl.constexpr, HAS_SMOOTHING: tl.\n    constexpr, SPLIT: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf'))\n    logits = logits.to(tl.float32) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - label_smoothing * sum_logits / total_classes - (1 -\n                    label_smoothing) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = label_smoothing * ((lse if not SPLIT else 0.0) - \n                sum_logits / total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')).to(tl.float32) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = label_smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 -\n            label_smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_z_row, M, N, eps, BLOCK_N: tl.constexpr, HAS_BIAS:\n    tl.constexpr, HAS_Z: tl.constexpr, NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr):\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask).to(tl.float32)\n        y *= z * tl.sigmoid(z)\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd,\n    stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row,\n    stride_dz_row, stride_dw_row, stride_db_row, M, N, eps,\n    rows_per_program, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.\n    constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_N: tl.constexpr):\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        elif RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask\n        =mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db,\n            mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_quant_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl\n    .constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl\n            .float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\n    y = tl.math.round(y * scale)\n    y = tl.maximum(tl.minimum(y, 127), -128) / scale\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_WEIGHT:\n    tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w if HAS_WEIGHT else xhat\n            if HAS_BIAS:\n                y = y + b\n            scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\n            y = tl.math.round(y * scale)\n            y = tl.maximum(tl.minimum(y, 127), -128) / scale\n            tl.store(Y + cols, y, mask=mask)\n        wdy = dy\n        if HAS_WEIGHT:\n            wdy = dy * w\n            dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    if HAS_WEIGHT:\n        tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: tl.constexpr):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_row.to(g_row.dtype)\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\n    h = f * up\n\n    df/de (with help of Wolfram :)\n    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\n\n    Reuse via\n    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_partial_row * e_row\n    f_row = f_row.to(DW_row.dtype)\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    t = 0.3989422804014327\n    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)\n    de_row = dg_row.to(tl.float32) * df_de\n    de_row = de_row.to(DW_row.dtype)\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: tl.constexpr):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    s = 0.7978845608028654\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (triton_tanh(s * e_row * (1.0 + 0.044715 * e_row *\n        e_row)) + 1.0)\n    f_row = f_row.to(g_row.dtype)\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\n    h = f * up\n\n    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\n    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\n            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] *                            ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\n\n    Notice sech^2(x) = 1 - tanh^2(x)\n    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\n\n    See https://www.desmos.com/calculator/nqprfoni6x\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    s = 0.7978845608028654\n    a = s * e_row\n    b = a * 0.044715 * e_row * e_row\n    T = 1.0 + triton_tanh(a + b)\n    T2 = 0.5 * T\n    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b)\n    df_de = T2 + Q2\n    f_row = T2 * e_row\n    f_row = f_row.to(DW_row.dtype)\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row.to(tl.float32) * df_de\n    de_row = de_row.to(DW_row.dtype)\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef layernorm_forward(Y, Y_row_stride, X, X_row_stride, W, b, r, mu, n_cols,\n    eps, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0).to(tl.float32)\n    mean_X = tl.sum(X_row, axis=0) / n_cols\n    XX = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    tl.store(mu, mean_X)\n    output = XX * inv_var * W_row + b_row\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef layernorm_backward(dY, dY_row_stride, X, X_row_stride, W, b, r, mu,\n    n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0).to(tl.float32)\n    inv_var = tl.load(r).to(tl.float32)\n    mean = tl.load(mu).to(tl.float32)\n    normed = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis=0) / n_cols - normed * tl.sum(dY_W *\n        normed, axis=0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'BACKWARD_PASS': lambda args: bool(args['BACKWARD_PASS'])})\n@triton.jit\ndef _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin,\n    sin_row_stride, seqlen, head_dim: tl.constexpr, n_heads: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n    sin1 = tl.load(sin + row_position % seqlen * sin_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    cos1 = tl.load(cos + row_position % seqlen * cos_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    if BACKWARD_PASS:\n        sin1 = -sin1\n    pass\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (row_position * Q_row_stride + k * head_dim + col_offsets +\n            half_head_dim)\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass\n"
    },
    {
      "input": "@triton.jit\ndef triton_cast(x, dtype):\n    return x.to(dtype)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed.to(W_row.dtype)\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'GEMMA': lambda args: bool(args['GEMMA'])})\n@triton.jit\ndef _rms_layernorm_backward(dY, dY_row_stride, dX, dX_row_stride, X,\n    X_row_stride, W, W_row_stride, r, r_row_stride, n_cols, eps, GEMMA: tl.\n    constexpr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    if GEMMA:\n        dX += row_idx * dY_row_stride\n    else:\n        dX = dY\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n    inv_var = tl.load(r).to(tl.float32)\n    normed = X_row * inv_var\n    if GEMMA:\n        dY_W = dY_row * (W_row + 1.0)\n    else:\n        dY_W = dY_row * W_row\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)\n    tl.store(dX + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _gemma_rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE, BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]\n        Pi = exp(xi) / sum(exp(xi))\n        CE_i = -y log(p) = -y log[ exp(x) / sum(exp(x)) ]\n             = -y [ x - log[sum(exp(x))] ]\n             = y * (log[sum(exp(x))] - x)\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        logsumexp is also stable\n        Take    y =         log[sum(exp(x))]\n           exp(y) =             sum(exp(x))\n           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x\n           exp(y) =      exp(c)*sum(exp(x - c))\n               y  = log(exp(c)*sum(exp(x - c)))\n               y  = c + log[sum(exp(x - c))]\n        This means we can set c = max(x) to make sure\n        exp(x - c) always is exp(x - max(x)).\n        This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.\n    \"\"\"\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf')\n        ).to(tl.float32)\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx).to(tl.float32)\n        if DO_LOGIT_SCALING:\n            x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING:\n            x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _chunked_cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE, N_CHUNKS, BLOCK_SIZE: tl.\n    constexpr, DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        256K vocab divided in 4 chunks\n\n        |-65536-| |-65536-| |-65536-| |-65536-|\n        |-------| |-------| |-------| |-------|\n        |-------| |-------| |-------| |-------|\n\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        Notice we can do logsumexp for each chunk and then\n        logsumexp[chunk_sum(logsumexp)] == logsumexp\n\n        chunk_sum = log[chunk_sum(logsumexp)]\n                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]\n                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]\n                  = log[sum(exp(a)) + ... + sum(exp(z))]\n                  = logsumexp(x)\n\n        This means we can perform a logsumexp for each chunk, then do a\n        final logsumexp reduction!\n\n        Ie do: logsumexp(chunked_logsumexp) - x\n    \"\"\"\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf')\n        ).to(tl.float32)\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx).to(tl.float32)\n            if DO_LOGIT_SCALING:\n                x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING:\n                x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n    pass\n    tl.store(logsumexp_ptr, logsumexp)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _cross_entropy_backward(logits_ptr, logits_row_stride, dloss_ptr,\n    dloss_row_stride, logsumexp_ptr, labels_ptr, VOCAB_SIZE, BLOCK_SIZE: tl\n    .constexpr, DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        CE_i = -y log(P) = y * (log[sum(exp(x))] - x)\n        dC/dx = d/dx (y * log[sum(exp(x))] - x * y)\n\n        From https://en.wikipedia.org/wiki/LogSumExp\n        d/dx logsumexp = exp(x) / sum(exp(x)) = softmax(x)\n\n        dC/dx = y * exp(x) / sum(exp(x)) - d/dx (x * y)\n        dC/dx = y * exp[ log[exp(x) / sum(exp(x))] ] using x = exp(log(x)) trick\n        dC/dx = y * exp[x - logsumexp] - d/dx (x * y)\n\n        If y == 0: dC/dx = 0\n        If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1\n        If y == 1 and x != label: dC/dx     = exp[x - logsumexp]\n    \"\"\"\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf')).to(tl\n        .float32)\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n    pass\n    partial = x\n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n    pass\n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x - logsumexp)\n    y = tl.where(col_offsets == label_idx, y - 1.0, y)\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n    pass\n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial * partial)\n    pass\n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fg_kernel(e, g, h, n_elements, BLOCK_SIZE: tl.constexpr):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = e_row * tl.sigmoid(e_row)\n    f_row = f_row.to(g_row.dtype)\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    e = e.float()\n    se = 1.0 / (1.0 + torch.exp(-e))\n    f = (se * e).to(dtype)\n    h = f * g\n    df = DW * f\n    dg = DW * g\n    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0).to(tl.float32)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    se_row = tl.sigmoid(e_row)\n    f_row = se_row * e_row\n    f_row = f_row.to(DW_row.dtype)\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row.to(tl.float32) * se_row * (1.0 + e_row * (1.0 - se_row))\n    de_row = de_row.to(DW_row.dtype)\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_one_block(start_n, seq_len, offs_m, offs_n,\n    mask_m, mask_n, q, K_block_ptr, V_block_ptr, n_targets, ts_1_ptrs, ts_0,\n    TW, PW, alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, bias_ptrs, MAX_ATTN_LEN: tl.constexpr,\n    INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl.constexpr, BUCKET_FN: tl.\n    constexpr, ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS: tl.constexpr,\n    USE_POS_BIAS: tl.constexpr, HAS_MAX_POS_IND: tl.constexpr,\n    HAS_MULTIPLE_TARGETS: tl.constexpr, CONTEXTUAL_SEQ_LEN: tl.constexpr,\n    IS_DELTA_Q: tl.constexpr, ALLOW_TF32: tl.constexpr, BLOCK_M: tl.\n    constexpr, BLOCK_N: tl.constexpr):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    k = tl.load(K_block_ptr, boundary_check=(1,), padding_option='zero')\n    qk = tl.dot(q, k, allow_tf32=ALLOW_TF32) * alpha\n    invalid_mask = offs_m[:, None] == offs_n[None, :]\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            offs_m = tl.where(offs_m < seq_len - n_targets, offs_m, seq_len -\n                n_targets)\n            offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, seq_len -\n                n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            offs_m = tl.where(offs_m > n_targets - 1, offs_m, n_targets - 1)\n            offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1)\n    offs_n_minus_m = offs_n[None, :] - offs_m[:, None]\n    if MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m < 0 and \n                offs_n_minus_m >= -MAX_ATTN_LEN)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m > 0 and \n                offs_n_minus_m <= MAX_ATTN_LEN)\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m < 0\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m > 0\n    if CONTEXTUAL_SEQ_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            row_filter = offs_m < CONTEXTUAL_SEQ_LEN\n            if HAS_MULTIPLE_TARGETS:\n                col_filter = offs_n < seq_len - n_targets\n            else:\n                col_filter = offs_n < seq_len\n            invalid_mask = invalid_mask or row_filter[:, None] and col_filter[\n                None, :]\n    if ATTN_BIAS_TYPE == 'fused':\n        attn_bias = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if USE_TIME_BIAS:\n            if CAUSAL:\n                ts_1 = tl.load(ts_1_ptrs + start_n, mask=mask_n)\n            else:\n                ts_1 = tl.load(ts_1_ptrs + start_n + 1, mask=mask_n)\n            ts = ts_0[:, None] - ts_1[None, :]\n            ts = ts + time_delta\n            ts = tl.where(ts > 1e-06, ts, 1e-06)\n            ts = ts * (1.0 / time_bucket_incr)\n            if BUCKET_FN == 'log':\n                ts = tl.log(ts)\n            elif BUCKET_FN == 'sqrt':\n                ts = tl.sqrt(ts)\n            ts = ts * (1.0 / time_bucket_div)\n            ts = ts.to(tl.int32)\n            ts = tl.where(ts > 0, ts, 0)\n            ts = tl.where(ts < num_buckets, ts, num_buckets)\n            ts_w = tl.load(TW + ts, mask=mask_m[:, None] and mask_n[None, :])\n            attn_bias = attn_bias + ts_w\n        if USE_POS_BIAS:\n            if HAS_MAX_POS_IND:\n                offs_pos_w = offs_n_minus_m + max_pos_ind - 1\n                offs_pos_w = tl.where(offs_pos_w > 0, offs_pos_w, 0)\n                offs_pos_w = tl.where(offs_pos_w < 2 * max_pos_ind - 2,\n                    offs_pos_w, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w = offs_n_minus_m + MAX_SEQ_LEN - 1\n            pos_w = tl.load(PW + offs_pos_w, mask=mask_m[:, None] and\n                mask_n[None, :])\n            attn_bias = attn_bias + pos_w\n        qk = qk + attn_bias\n    elif ATTN_BIAS_TYPE == 'separate':\n        attn_bias = tl.load(bias_ptrs + start_n, mask=mask_m[:, None] &\n            mask_n[None, :], other=0.0)\n        qk = qk + attn_bias\n    silu = fast_dividef(qk, 1.0 + tl.exp(-qk)) * (1.0 / MAX_SEQ_LEN)\n    silu = tl.where(invalid_mask, silu, 0)\n    v = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n    silu = silu.to(v.dtype)\n    return tl.dot(silu, v, allow_tf32=ALLOW_TF32)\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_compute(Q, K, V, seq_offsets, TS, TW, PW, Bias,\n    seq2_offsets, delta_x_offsets, num_targets, Out, stride_qm, stride_qh,\n    stride_kn, stride_kh, stride_vn, stride_vh, stride_ts, stride_om,\n    stride_oh, alpha, Z, H, MAX_SEQ_LEN, DimQ, DimV, DeltaSize, num_buckets,\n    max_pos_ind, time_bucket_incr, time_bucket_div, time_delta, off_z,\n    off_h, pid, INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl.constexpr,\n    BUCKET_FN: tl.constexpr, ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS:\n    tl.constexpr, USE_POS_BIAS: tl.constexpr, HAS_MAX_POS_IND: tl.constexpr,\n    HAS_MULTIPLE_TARGETS: tl.constexpr, IS_DELTA_Q: tl.constexpr,\n    ALLOW_TF32: tl.constexpr, BLOCK_D_Q: tl.constexpr, BLOCK_D_V: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, MAX_ATTN_LEN:\n    tl.constexpr, CONTEXTUAL_SEQ_LEN: tl.constexpr):\n    seq_start = tl.load(seq_offsets + off_z).to(tl.int64)\n    off_h = off_h.to(tl.int64)\n    off_z = off_z.to(tl.int64)\n    seq_end = tl.load(seq_offsets + off_z + 1)\n    seq_len = (seq_end - seq_start).to(tl.int32)\n    if IS_DELTA_Q:\n        start_m_delta = pid * BLOCK_M\n        delta_start = tl.load(delta_x_offsets + off_z * DeltaSize)\n        start_m = (start_m_delta + delta_start - seq_start).to(tl.int32)\n    else:\n        start_m_delta = 0\n        start_m = pid * BLOCK_M\n    if start_m < seq_len:\n        if HAS_MULTIPLE_TARGETS:\n            n_targets = tl.load(num_targets + off_z).to(tl.int32)\n        else:\n            n_targets = None\n        offs_m = start_m + tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        if IS_DELTA_Q:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                off_z * DeltaSize * stride_qm, shape=(DeltaSize, BLOCK_D_Q),\n                strides=(stride_qm, 1), offsets=(start_m_delta, 0),\n                block_shape=(BLOCK_M, BLOCK_D_Q), order=(1, 0))\n        else:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                seq_start * stride_qm, shape=(seq_len, BLOCK_D_Q), strides=\n                (stride_qm, 1), offsets=(start_m, 0), block_shape=(BLOCK_M,\n                BLOCK_D_Q), order=(1, 0))\n        K_block_ptr = tl.make_block_ptr(base=K + off_h * stride_kh + \n            seq_start * stride_kn, shape=(BLOCK_D_Q, seq_len), strides=(1,\n            stride_kn), offsets=(0, 0), block_shape=(BLOCK_D_Q, BLOCK_N),\n            order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=V + off_h * stride_vh + \n            seq_start * stride_vn, shape=(seq_len, BLOCK_D_V), strides=(\n            stride_vn, 1), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_D_V),\n            order=(1, 0))\n        mask_m = offs_m < seq_len\n        if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n            ts_0_ptrs = TS + off_z * stride_ts + offs_m\n            ts_1_ptrs = TS + off_z * stride_ts + offs_n\n            if CAUSAL:\n                ts_0 = tl.load(ts_0_ptrs + 1, mask=mask_m)\n            else:\n                ts_0 = tl.load(ts_0_ptrs, mask=mask_m)\n        elif ATTN_BIAS_TYPE == 'separate':\n            seq2_start = tl.load(seq2_offsets + off_z)\n            bias_start = seq2_start * H + off_h * seq_len * seq_len\n            off_bias = offs_m[:, None] * seq_len + offs_n[None, :]\n            bias_ptrs = Bias + bias_start + off_bias\n        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n        acc = tl.zeros([BLOCK_M, BLOCK_D_V], dtype=tl.float32)\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            if HAS_MULTIPLE_TARGETS:\n                if MAX_ATTN_LEN > 0:\n                    start_m_index = (seq_len - n_targets if start_m > \n                        seq_len - n_targets else start_m)\n                    low = start_m_index - MAX_ATTN_LEN\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                uih_end = (seq_len - n_targets + BLOCK_N - 1\n                    ) // BLOCK_N * BLOCK_N\n                if uih_end < start_m:\n                    high = seq_len - n_targets\n                else:\n                    high = start_m + BLOCK_M\n                if CONTEXTUAL_SEQ_LEN > 0:\n                    if start_m < CONTEXTUAL_SEQ_LEN:\n                        high = seq_len - n_targets\n            else:\n                if MAX_ATTN_LEN > 0:\n                    low = start_m - MAX_ATTN_LEN\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                high = start_m + BLOCK_M\n                if CONTEXTUAL_SEQ_LEN > 0:\n                    if start_m < CONTEXTUAL_SEQ_LEN:\n                        high = seq_len\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            low = start_m\n            high = seq_len\n        if low > 0:\n            K_block_ptr = tl.advance(K_block_ptr, (0, low))\n            V_block_ptr = tl.advance(V_block_ptr, (low, 0))\n        for start_n in range(low, high, BLOCK_N):\n            cur_offs_n = offs_n + start_n\n            mask_n = cur_offs_n < seq_len\n            acc += _ragged_hstu_attn_fwd_one_block(start_n=start_n, seq_len\n                =seq_len, offs_m=offs_m, offs_n=cur_offs_n, mask_m=mask_m,\n                mask_n=mask_n, q=q, K_block_ptr=K_block_ptr, V_block_ptr=\n                V_block_ptr, n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE == 'fused' and\n                USE_TIME_BIAS else None, ts_0=ts_0 if ATTN_BIAS_TYPE ==\n                'fused' and USE_TIME_BIAS else None, TW=TW, PW=PW, alpha=\n                alpha, MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets,\n                max_pos_ind=max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n                time_bucket_incr=time_bucket_incr, time_bucket_div=\n                time_bucket_div, time_delta=time_delta, bias_ptrs=bias_ptrs if\n                ATTN_BIAS_TYPE == 'separate' else None, CONTEXTUAL_SEQ_LEN=\n                CONTEXTUAL_SEQ_LEN, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, IS_DELTA_Q=\n                IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N\n                =BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if HAS_MULTIPLE_TARGETS and INVALID_MASK_TYPE == 'lower_triangular':\n            if uih_end < start_m:\n                low_delta = start_m\n                high_delta = start_m + BLOCK_M\n                offset = (low_delta - uih_end).to(tl.int32)\n                K_block_ptr = tl.advance(K_block_ptr, (0, offset))\n                V_block_ptr = tl.advance(V_block_ptr, (offset, 0))\n                for start_delta in tl.range(low_delta, high_delta, BLOCK_N,\n                    num_stages=0):\n                    cur_offs_n = offs_n + start_delta\n                    mask_n = cur_offs_n < seq_len\n                    acc += _ragged_hstu_attn_fwd_one_block(start_n=\n                        start_delta, seq_len=seq_len, offs_m=offs_m, offs_n\n                        =cur_offs_n, mask_m=mask_m, mask_n=mask_n, q=q,\n                        K_block_ptr=K_block_ptr, V_block_ptr=V_block_ptr,\n                        n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                        None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE ==\n                        'fused' and USE_TIME_BIAS else None, ts_0=ts_0 if \n                        ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS else\n                        None, TW=TW, PW=PW, alpha=alpha, MAX_SEQ_LEN=\n                        MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n                        max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n                        time_bucket_incr=time_bucket_incr, time_bucket_div=\n                        time_bucket_div, time_delta=time_delta, bias_ptrs=\n                        bias_ptrs if ATTN_BIAS_TYPE == 'separate' else None,\n                        CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                        INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n                        BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n                        USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                        USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                        HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,\n                        IS_DELTA_Q=IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32,\n                        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n                    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n                    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if IS_DELTA_Q:\n            start_m_delta = pid * BLOCK_M\n            offs_m_delta = start_m_delta + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = Out + off_z * DeltaSize * stride_om + off_h * stride_oh\n            out_ptrs = off_o + offs_m_delta[:, None] * stride_om + offs_v_d[\n                None, :]\n            tl.store(out_ptrs, acc, mask=(offs_m_delta < DeltaSize)[:, None])\n        else:\n            start_m = pid * BLOCK_M\n            offs_m = start_m + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = Out + seq_start * stride_om + off_h * stride_oh\n            out_ptrs = off_o + offs_m[:, None] * stride_om + offs_v_d[None, :]\n            tl.store(out_ptrs, acc, mask=(offs_m < seq_len)[:, None])\n"
    },
    {
      "input": "@triton.autotune(configs=_get_fw_configs(), key=['AUTOTUNE_Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE',\n    'DeltaSize', 'IS_DELTA_Q'])\n@triton.jit\ndef _ragged_hstu_attn_fwd(Q, K, V, sort_by_length_indices, seq_offsets, TS,\n    TW, PW, Bias, seq2_offsets, delta_x_offsets, num_targets, Out,\n    stride_qm, stride_qh, stride_kn, stride_kh, stride_vn, stride_vh,\n    stride_ts, stride_om, stride_oh, alpha, Z, AUTOTUNE_Z, H, MAX_SEQ_LEN,\n    AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV, DeltaSize, num_buckets, max_pos_ind,\n    time_bucket_incr, time_bucket_div, time_delta, INVALID_MASK_TYPE: tl.\n    constexpr, CAUSAL: tl.constexpr, BUCKET_FN: tl.constexpr,\n    ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS: tl.constexpr, USE_POS_BIAS:\n    tl.constexpr, HAS_MAX_POS_IND: tl.constexpr, HAS_MULTIPLE_TARGETS: tl.\n    constexpr, IS_DELTA_Q: tl.constexpr, ALLOW_TF32: tl.constexpr,\n    BLOCK_D_Q: tl.constexpr, BLOCK_D_V: tl.constexpr, BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr, MAX_ATTN_LEN: tl.constexpr, CONTEXTUAL_SEQ_LEN:\n    tl.constexpr, HAS_SORT_BY_LENGTH_INDICES: tl.constexpr):\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    if HAS_SORT_BY_LENGTH_INDICES:\n        off_z = tl.load(sort_by_length_indices + off_z)\n    off_h = off_hz % H\n    pid = tl.program_id(0)\n    _ragged_hstu_attn_fwd_compute(Q=Q, K=K, V=V, seq_offsets=seq_offsets,\n        TS=TS, TW=TW, PW=PW, Bias=Bias, seq2_offsets=seq2_offsets,\n        delta_x_offsets=delta_x_offsets, num_targets=num_targets, Out=Out,\n        stride_qm=stride_qm, stride_qh=stride_qh, stride_kn=stride_kn,\n        stride_kh=stride_kh, stride_vn=stride_vn, stride_vh=stride_vh,\n        stride_ts=stride_ts, stride_om=stride_om, stride_oh=stride_oh,\n        alpha=alpha, Z=Z, H=H, MAX_SEQ_LEN=MAX_SEQ_LEN, DimQ=DimQ, DimV=\n        DimV, DeltaSize=DeltaSize, num_buckets=num_buckets, max_pos_ind=\n        max_pos_ind, time_bucket_incr=time_bucket_incr, time_bucket_div=\n        time_bucket_div, time_delta=time_delta, off_z=off_z, off_h=off_h,\n        pid=pid, INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n        BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE, USE_TIME_BIAS=\n        USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS, HAS_MAX_POS_IND=\n        HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,\n        IS_DELTA_Q=IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q,\n        BLOCK_D_V=BLOCK_D_V, MAX_ATTN_LEN=MAX_ATTN_LEN, CONTEXTUAL_SEQ_LEN=\n        CONTEXTUAL_SEQ_LEN, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n"
    },
    {
      "input": "@triton.autotune(configs=_get_fw_configs(), key=['AUTOTUNE_Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE',\n    'DeltaSize', 'IS_DELTA_Q'])\n@triton.jit\ndef _ragged_hstu_attn_fwd_persistent(Q, K, V, sort_by_length_indices,\n    seq_offsets, TS, TW, PW, Bias, seq2_offsets, delta_x_offsets,\n    num_targets, Out, stride_qm, stride_qh, stride_kn, stride_kh, stride_vn,\n    stride_vh, stride_ts, stride_om, stride_oh, alpha, Z, AUTOTUNE_Z, H,\n    MAX_SEQ_LEN, AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV, DeltaSize, num_buckets,\n    max_pos_ind, time_bucket_incr, time_bucket_div, time_delta,\n    INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl.constexpr, BUCKET_FN: tl.\n    constexpr, ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS: tl.constexpr,\n    USE_POS_BIAS: tl.constexpr, HAS_MAX_POS_IND: tl.constexpr,\n    HAS_MULTIPLE_TARGETS: tl.constexpr, IS_DELTA_Q: tl.constexpr,\n    ALLOW_TF32: tl.constexpr, BLOCK_D_Q: tl.constexpr, BLOCK_D_V: tl.\n    constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, MAX_ATTN_LEN:\n    tl.constexpr, CONTEXTUAL_SEQ_LEN: tl.constexpr,\n    HAS_SORT_BY_LENGTH_INDICES: tl.constexpr):\n    n_tile_num = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)\n    prog_id = tl.program_id(0)\n    num_progs = tl.num_programs(0)\n    total_tiles = n_tile_num * Z * H\n    tiles_per_sm = total_tiles // num_progs\n    if prog_id < total_tiles % num_progs:\n        tiles_per_sm += 1\n    tile_idx = prog_id\n    for _ in range(0, tiles_per_sm):\n        pid = (total_tiles - tile_idx - 1) // (Z * H)\n        off_hz = (total_tiles - tile_idx - 1) % (Z * H)\n        off_z = off_hz // H\n        off_h = off_hz % H\n        _ragged_hstu_attn_fwd_compute(Q=Q, K=K, V=V, seq_offsets=\n            seq_offsets, TS=TS, TW=TW, PW=PW, Bias=Bias, seq2_offsets=\n            seq2_offsets, delta_x_offsets=delta_x_offsets, num_targets=\n            num_targets, Out=Out, stride_qm=stride_qm, stride_qh=stride_qh,\n            stride_kn=stride_kn, stride_kh=stride_kh, stride_vn=stride_vn,\n            stride_vh=stride_vh, stride_ts=stride_ts, stride_om=stride_om,\n            stride_oh=stride_oh, alpha=alpha, Z=Z, H=H, MAX_SEQ_LEN=\n            MAX_SEQ_LEN, DimQ=DimQ, DimV=DimV, DeltaSize=DeltaSize,\n            num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n            time_bucket_incr=time_bucket_incr, time_bucket_div=\n            time_bucket_div, time_delta=time_delta, off_z=off_z, off_h=\n            off_h, pid=pid, INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=\n            CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n            USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS,\n            HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n            HAS_MULTIPLE_TARGETS, IS_DELTA_Q=IS_DELTA_Q, ALLOW_TF32=\n            ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=BLOCK_D_V,\n            MAX_ATTN_LEN=MAX_ATTN_LEN, CONTEXTUAL_SEQ_LEN=\n            CONTEXTUAL_SEQ_LEN, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n        tile_idx += num_progs\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_bwd_one_block(start_m, offs_n, offs_m, q_ptrs_trans,\n    dq_ptrs_trans, mask_n, ts_0_ptrs, ts_1, bias_ptrs_trans,\n    dbias_ptrs_trans, do_ptrs, dk, dv, k, v, pos_offs_n, seq_len, n_targets,\n    TW, PW, DTW, DPW, LOCK, stride_qm, stride_dom, stride_dqm, alpha,\n    MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, MAX_ATTN_LEN: tl.constexpr,\n    INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl.constexpr, BUCKET_FN: tl.\n    constexpr, ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS: tl.constexpr,\n    USE_POS_BIAS: tl.constexpr, FUSED_BIAS_BWD: tl.constexpr,\n    HAS_MAX_POS_IND: tl.constexpr, HAS_MULTIPLE_TARGETS: tl.constexpr,\n    CONTEXTUAL_SEQ_LEN: tl.constexpr, ALLOW_TF32: tl.constexpr, BLOCK_M: tl\n    .constexpr, BLOCK_N: tl.constexpr, ATOMIC_ADD: tl.constexpr):\n    pos_offs_m = offs_m + start_m\n    mask_m = pos_offs_m < seq_len\n    invalid_mask_trans = pos_offs_m[None, :] == offs_n[:, None]\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            pos_offs_m = tl.where(pos_offs_m < seq_len - n_targets,\n                pos_offs_m, seq_len - n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            pos_offs_m = tl.where(pos_offs_m > n_targets - 1, pos_offs_m, \n                n_targets - 1)\n    q_trans = tl.load(q_ptrs_trans + start_m * stride_qm, mask=mask_m[None,\n        :], other=0.0)\n    qk_trans = tl.dot(k, q_trans, allow_tf32=ALLOW_TF32) * alpha\n    if ATTN_BIAS_TYPE == 'fused':\n        attn_bias_trans = tl.zeros([BLOCK_N, BLOCK_M], dtype=tl.float32)\n        if USE_TIME_BIAS:\n            if CAUSAL:\n                ts_0 = tl.load(ts_0_ptrs + start_m + 1, mask=mask_m)\n            else:\n                ts_0 = tl.load(ts_0_ptrs + start_m, mask=mask_m)\n            ts_trans = ts_0[None, :] - ts_1[:, None]\n            ts_trans = ts_trans + time_delta\n            ts_trans = tl.where(ts_trans > 1e-06, ts_trans, 1e-06)\n            ts_trans = ts_trans * (1.0 / time_bucket_incr)\n            if BUCKET_FN == 'log':\n                ts_trans = tl.log(ts_trans)\n            elif BUCKET_FN == 'sqrt':\n                ts_trans = tl.sqrt(ts_trans)\n            ts_trans = ts_trans * (1.0 / time_bucket_div)\n            ts_trans = ts_trans.to(tl.int32)\n            ts_trans = tl.where(ts_trans > 0, ts_trans, 0)\n            ts_trans = tl.where(ts_trans < num_buckets, ts_trans, num_buckets)\n            ts_w_trans = tl.load(TW + ts_trans, mask=mask_m[None, :] and\n                mask_n[:, None])\n            attn_bias_trans = attn_bias_trans + ts_w_trans\n        if USE_POS_BIAS:\n            offs_pos_w_trans = None\n            if HAS_MAX_POS_IND:\n                offs_pos_w_trans = pos_offs_n[:, None] - pos_offs_m[None, :\n                    ] + max_pos_ind - 1\n                offs_pos_w_trans = tl.where(offs_pos_w_trans > 0,\n                    offs_pos_w_trans, 0)\n                offs_pos_w_trans = tl.where(offs_pos_w_trans < 2 *\n                    max_pos_ind - 2, offs_pos_w_trans, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w_trans = pos_offs_n[:, None] - pos_offs_m[None, :\n                    ] + MAX_SEQ_LEN - 1\n            pos_w_trans = tl.load(PW + offs_pos_w_trans, mask=mask_m[None,\n                :] and mask_n[:, None])\n            attn_bias_trans = attn_bias_trans + pos_w_trans\n        qk_trans = qk_trans + attn_bias_trans\n    elif ATTN_BIAS_TYPE == 'separate':\n        attn_bias_trans = tl.load(bias_ptrs_trans + start_m * seq_len, mask\n            =mask_m[None, :] & mask_n[:, None], other=0.0)\n        qk_trans = qk_trans + attn_bias_trans\n    sig_trans = fast_dividef(1.0, 1.0 + tl.exp(-qk_trans))\n    silu_trans = qk_trans * sig_trans * (1.0 / MAX_SEQ_LEN)\n    if MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n                ] > pos_offs_n[:, None] and pos_offs_n[:, None] - pos_offs_m[\n                None, :] >= -MAX_ATTN_LEN\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n                ] < pos_offs_n[:, None] and pos_offs_n[:, None] - pos_offs_m[\n                None, :] <= MAX_ATTN_LEN\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n            ] > pos_offs_n[:, None]\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n            ] < pos_offs_n[:, None]\n    if CONTEXTUAL_SEQ_LEN > 0 and INVALID_MASK_TYPE == 'lower_triangular':\n        row_filter = pos_offs_m < CONTEXTUAL_SEQ_LEN\n        if HAS_MULTIPLE_TARGETS:\n            col_filter = pos_offs_n < seq_len - n_targets\n        else:\n            col_filter = pos_offs_n < seq_len\n        invalid_mask_trans = invalid_mask_trans or row_filter[None, :\n            ] and col_filter[:, None]\n    silu_trans = tl.where(invalid_mask_trans, silu_trans, 0)\n    silu_trans = silu_trans.to(k.dtype)\n    do = tl.load(do_ptrs + start_m * stride_dom, mask=mask_m[:, None],\n        other=0.0)\n    dv += tl.dot(silu_trans, do, allow_tf32=ALLOW_TF32)\n    dqk_trans = tl.dot(v, tl.trans(do), allow_tf32=ALLOW_TF32)\n    dqk_trans = dqk_trans * sig_trans * (1 + qk_trans * (1 - sig_trans)) * (\n        1.0 / MAX_SEQ_LEN)\n    dqk_trans = tl.where(invalid_mask_trans, dqk_trans, 0)\n    dqk_trans = dqk_trans.to(k.dtype)\n    if ATTN_BIAS_TYPE == 'fused' and FUSED_BIAS_BWD:\n        if USE_TIME_BIAS:\n            tl.atomic_add(DTW + ts_trans, dqk_trans, mask=mask_m[None, :] &\n                mask_n[:, None] & invalid_mask_trans, sem='relaxed')\n        if USE_POS_BIAS:\n            tl.atomic_add(DPW + offs_pos_w_trans, dqk_trans, mask=mask_m[\n                None, :] & mask_n[:, None] & invalid_mask_trans, sem='relaxed')\n    elif ATTN_BIAS_TYPE == 'separate':\n        tl.store(dbias_ptrs_trans + start_m * seq_len, dqk_trans, mask=\n            mask_m[None, :] & mask_n[:, None])\n    dk += tl.dot(dqk_trans, tl.trans(q_trans), allow_tf32=ALLOW_TF32)\n    if ATOMIC_ADD:\n        lock_id = start_m // BLOCK_M\n        stride_lock = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)\n        lock = LOCK + tl.program_id(0) * stride_lock + lock_id\n        tl.debug_barrier()\n        while tl.atomic_cas(lock, 0, 1) == 1:\n            pass\n    dq_trans = tl.load(dq_ptrs_trans + start_m * stride_dqm, mask=mask_m[\n        None, :], other=0.0, eviction_policy='evict_last')\n    dq_trans += tl.dot(tl.trans(k), dqk_trans, allow_tf32=ALLOW_TF32) * alpha\n    dq_trans = dq_trans.to(k.dtype)\n    tl.store(dq_ptrs_trans + start_m * stride_dqm, dq_trans, mask=mask_m[\n        None, :], eviction_policy='evict_last')\n    if ATOMIC_ADD:\n        tl.atomic_xchg(lock, 0)\n    return dk, dv\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_bwd_one_col_block(start_n, seq_len, n_targets, Q, K,\n    V, TS, TW, PW, Bias, DOut, DQ, DK, DV, DBias, DTW, DPW, LOCK, stride_qm,\n    stride_kn, stride_vn, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n    alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, MAX_ATTN_LEN: tl.constexpr,\n    INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl.constexpr, BUCKET_FN: tl.\n    constexpr, ATTN_BIAS_TYPE: tl.constexpr, USE_TIME_BIAS: tl.constexpr,\n    USE_POS_BIAS: tl.constexpr, FUSED_BIAS_BWD: tl.constexpr,\n    HAS_MAX_POS_IND: tl.constexpr, HAS_MULTIPLE_TARGETS: tl.constexpr,\n    CONTEXTUAL_SEQ_LEN: tl.constexpr, ALLOW_TF32: tl.constexpr, BLOCK_D_Q:\n    tl.constexpr, BLOCK_D_V: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N:\n    tl.constexpr, UNROLL: tl.constexpr, ATOMIC_ADD: tl.constexpr):\n    if INVALID_MASK_TYPE == 'lower_triangular':\n        if HAS_MULTIPLE_TARGETS:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high + n_targets < seq_len else seq_len\n            else:\n                high = seq_len\n        else:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high < seq_len else seq_len\n            else:\n                high = seq_len\n        if CONTEXTUAL_SEQ_LEN > 0:\n            contextual_block_end = tl.cdiv(CONTEXTUAL_SEQ_LEN, BLOCK_M\n                ) * BLOCK_M\n            if low < contextual_block_end:\n                low = contextual_block_end\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        low = 0\n        high = start_n + BLOCK_N\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_qk_d = tl.arange(0, BLOCK_D_Q)\n    offs_v_d = tl.arange(0, BLOCK_D_V)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    q_ptrs_trans = Q + (offs_m[None, :] * stride_qm + offs_qk_d[:, None])\n    dq_ptrs_trans = DQ + (offs_m[None, :] * stride_dqm + offs_qk_d[:, None])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_qk_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_v_d[None, :])\n    mask_n = offs_n < seq_len\n    ts_0_ptrs = None\n    ts_1_ptrs = None\n    ts_1 = None\n    off_bias_trans = None\n    bias_ptrs_trans = None\n    dbias_ptrs_trans = None\n    if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n        ts_0_ptrs = TS + offs_m\n        ts_1_ptrs = TS + offs_n\n        if CAUSAL:\n            ts_1 = tl.load(ts_1_ptrs, mask=mask_n)\n        else:\n            ts_1 = tl.load(ts_1_ptrs + 1, mask=mask_n)\n    elif ATTN_BIAS_TYPE == 'separate':\n        off_bias_trans = offs_m[None, :] * seq_len + offs_n[:, None]\n        bias_ptrs_trans = Bias + off_bias_trans\n        dbias_ptrs_trans = DBias + off_bias_trans\n    do_ptrs = DOut + (offs_m[:, None] * stride_dom + offs_v_d[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_D_V], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_D_Q], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0)\n    v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            pos_offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, \n                seq_len - n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            pos_offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1\n                )\n    else:\n        pos_offs_n = offs_n\n    if CONTEXTUAL_SEQ_LEN > 0 and INVALID_MASK_TYPE == 'lower_triangular':\n        for start_m in range(0, CONTEXTUAL_SEQ_LEN, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m,\n                offs_n=offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans,\n                dq_ptrs_trans=dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=\n                ts_0_ptrs, ts_1=ts_1, bias_ptrs_trans=bias_ptrs_trans,\n                dbias_ptrs_trans=dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk,\n                dv=dv, k=k, v=v, pos_offs_n=pos_offs_n, seq_len=seq_len,\n                n_targets=n_targets, TW=TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=\n                LOCK, stride_qm=stride_qm, stride_dom=stride_dom,\n                stride_dqm=stride_dqm, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN,\n                num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n                MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n                time_bucket_incr, time_bucket_div=time_bucket_div,\n                time_delta=time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n                HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n                HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,\n                ATOMIC_ADD=ATOMIC_ADD)\n    for start_m in tl.range(low, high, BLOCK_M, loop_unroll_factor=UNROLL):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m, offs_n=\n            offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans, dq_ptrs_trans\n            =dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=ts_0_ptrs, ts_1=ts_1,\n            bias_ptrs_trans=bias_ptrs_trans, dbias_ptrs_trans=\n            dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk, dv=dv, k=k, v=v,\n            pos_offs_n=pos_offs_n, seq_len=seq_len, n_targets=n_targets, TW\n            =TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm,\n            stride_dom=stride_dom, stride_dqm=stride_dqm, alpha=alpha,\n            MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n            max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n            time_bucket_incr, time_bucket_div=time_bucket_div, time_delta=\n            time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n            BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n            USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS,\n            FUSED_BIAS_BWD=FUSED_BIAS_BWD, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=\n            CONTEXTUAL_SEQ_LEN, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N, ATOMIC_ADD=ATOMIC_ADD)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_v_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_qk_d[None, :])\n    dk = dk * alpha\n    tl.store(dv_ptrs, dv.to(k.dtype), mask=mask_n[:, None])\n    tl.store(dk_ptrs, dk.to(k.dtype), mask=mask_n[:, None])\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bw_configs(), key=['AUTOTUNE_Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE'])\n@triton.jit\ndef _ragged_hstu_attn_bwd(Q, K, V, sort_by_length_indices, seq_offsets, TS,\n    TW, PW, Bias, seq2_offsets, num_targets, DOut, DQ, DK, DV, DBias, DTW,\n    DPW, LOCK, stride_qm, stride_qh, stride_kn, stride_kh, stride_vn,\n    stride_vh, stride_ts, stride_dom, stride_doh, stride_dqm, stride_dqh,\n    stride_dkn, stride_dkh, stride_dvn, stride_dvh, alpha, Z, AUTOTUNE_Z, H,\n    MAX_SEQ_LEN, AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV, num_buckets, max_pos_ind,\n    time_bucket_incr, time_bucket_div, time_delta, CONTEXTUAL_SEQ_LEN: tl.\n    constexpr, MAX_ATTN_LEN: tl.constexpr, INVALID_MASK_TYPE: tl.constexpr,\n    CAUSAL: tl.constexpr, BUCKET_FN: tl.constexpr, ATTN_BIAS_TYPE: tl.\n    constexpr, USE_TIME_BIAS: tl.constexpr, USE_POS_BIAS: tl.constexpr,\n    FUSED_BIAS_BWD: tl.constexpr, HAS_MAX_POS_IND: tl.constexpr,\n    HAS_MULTIPLE_TARGETS: tl.constexpr, ALLOW_TF32: tl.constexpr, BLOCK_D_Q:\n    tl.constexpr, BLOCK_D_V: tl.constexpr, SEQUENCE_PARALLEL: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, UNROLL: tl.constexpr,\n    HAS_SORT_BY_LENGTH_INDICES: tl.constexpr):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    if HAS_SORT_BY_LENGTH_INDICES:\n        off_z = tl.load(sort_by_length_indices + off_z)\n    off_h = off_hz % H\n    off_h = off_h.to(tl.int64)\n    seq_start = tl.load(seq_offsets + off_z).to(tl.int64)\n    seq_end = tl.load(seq_offsets + off_z + 1)\n    seq_len = (seq_end - seq_start).to(tl.int32)\n    if HAS_MULTIPLE_TARGETS:\n        n_targets = tl.load(num_targets + off_z).to(tl.int32)\n    else:\n        n_targets = None\n    Q = Q + seq_start * stride_qm + off_h * stride_qh\n    K = K + seq_start * stride_kn + off_h * stride_kh\n    V = V + seq_start * stride_vn + off_h * stride_vh\n    DOut = DOut + seq_start * stride_dom + off_h * stride_doh\n    DQ = DQ + seq_start * stride_dqm + off_h * stride_dqh\n    DK = DK + seq_start * stride_dkn + off_h * stride_dkh\n    DV = DV + seq_start * stride_dvn + off_h * stride_dvh\n    if ATTN_BIAS_TYPE == 'fused':\n        if USE_TIME_BIAS:\n            TS = TS + off_z * stride_ts\n        if FUSED_BIAS_BWD:\n            if USE_TIME_BIAS:\n                DTW = DTW + off_hz * (num_buckets + 1)\n            if USE_POS_BIAS:\n                if HAS_MAX_POS_IND:\n                    DPW = DPW + off_hz * (2 * max_pos_ind - 1)\n                else:\n                    DPW = DPW + off_hz * (2 * MAX_SEQ_LEN - 1)\n    elif ATTN_BIAS_TYPE == 'separate':\n        seq2_start = tl.load(seq2_offsets + off_z)\n        bias_start = seq2_start * H + off_h * seq_len * seq_len\n        Bias = Bias + bias_start\n        DBias = DBias + bias_start\n    if SEQUENCE_PARALLEL:\n        start_n = tl.program_id(1) * BLOCK_N\n        if start_n >= seq_len:\n            return\n        _ragged_hstu_attn_bwd_one_col_block(start_n=start_n, seq_len=\n            seq_len, n_targets=n_targets, Q=Q, K=K, V=V, TS=TS, TW=TW, PW=\n            PW, Bias=Bias, DOut=DOut, DQ=DQ, DK=DK, DV=DV, DBias=DBias, DTW\n            =DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm, stride_kn=\n            stride_kn, stride_vn=stride_vn, stride_dom=stride_dom,\n            stride_dqm=stride_dqm, stride_dkn=stride_dkn, stride_dvn=\n            stride_dvn, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=\n            num_buckets, max_pos_ind=max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n            time_bucket_incr=time_bucket_incr, time_bucket_div=\n            time_bucket_div, time_delta=time_delta, INVALID_MASK_TYPE=\n            INVALID_MASK_TYPE, CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN,\n            ATTN_BIAS_TYPE=ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS,\n            USE_POS_BIAS=USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n            HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n            HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n            ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=BLOCK_D_V,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, UNROLL=UNROLL, ATOMIC_ADD=True)\n    else:\n        for start_n in range(0, seq_len, BLOCK_N):\n            _ragged_hstu_attn_bwd_one_col_block(start_n=start_n, seq_len=\n                seq_len, n_targets=n_targets, Q=Q, K=K, V=V, TS=TS, TW=TW,\n                PW=PW, Bias=Bias, DOut=DOut, DQ=DQ, DK=DK, DV=DV, DBias=\n                DBias, DTW=DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm,\n                stride_kn=stride_kn, stride_vn=stride_vn, stride_dom=\n                stride_dom, stride_dqm=stride_dqm, stride_dkn=stride_dkn,\n                stride_dvn=stride_dvn, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN,\n                num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n                MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n                time_bucket_incr, time_bucket_div=time_bucket_div,\n                time_delta=time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n                HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n                HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=\n                BLOCK_D_V, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, UNROLL=UNROLL,\n                ATOMIC_ADD=False)\n"
    },
    {
      "input": "@triton.jit\ndef _attn_bias_bwd(Q, K, V, seq_offsets, TS, TW, PW, num_targets, DOut, DTW,\n    DPW, stride_qm, stride_qh, stride_kn, stride_kh, stride_vn, stride_vh,\n    stride_ts, stride_dom, stride_doh, alpha, Z, H, MAX_SEQ_LEN, DimQ, DimV,\n    num_buckets, max_pos_ind, time_bucket_incr, time_bucket_div, time_delta,\n    MAX_ATTN_LEN: tl.constexpr, INVALID_MASK_TYPE: tl.constexpr, CAUSAL: tl\n    .constexpr, BUCKET_FN: tl.constexpr, USE_TIME_BIAS: tl.constexpr,\n    USE_POS_BIAS: tl.constexpr, HAS_MAX_POS_IND: tl.constexpr,\n    HAS_MULTIPLE_TARGETS: tl.constexpr, CONTEXTUAL_SEQ_LEN: tl.constexpr,\n    ALLOW_TF32: tl.constexpr, BLOCK_D_Q: tl.constexpr, BLOCK_D_V: tl.\n    constexpr, BLOCK_N: tl.constexpr, NUM_N_BLOCKS: tl.constexpr,\n    NUM_OUT_GROUPS: tl.constexpr):\n    off_mn = tl.program_id(0)\n    off_m = off_mn // NUM_N_BLOCKS\n    off_n = off_mn % NUM_N_BLOCKS\n    widx = off_m * (off_m + 1) // 2 + off_n\n    widx = widx % NUM_OUT_GROUPS\n    start_m = off_m * BLOCK_N\n    start_n = off_n * BLOCK_N\n    offs_m = start_m + tl.arange(0, BLOCK_N)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    offs_qk_d = tl.arange(0, BLOCK_D_Q)\n    offs_v_d = tl.arange(0, BLOCK_D_V)\n    dbias_pos = None\n    offs_pos_w = None\n    if USE_POS_BIAS:\n        if not HAS_MULTIPLE_TARGETS:\n            dbias_pos = tl.zeros((BLOCK_N, BLOCK_N), dtype=tl.float32)\n            if HAS_MAX_POS_IND:\n                offs_pos_w = offs_n[None, :] - offs_m[:, None\n                    ] + max_pos_ind - 1\n                offs_pos_w = tl.where(offs_pos_w > 0, offs_pos_w, 0)\n                offs_pos_w = tl.where(offs_pos_w < 2 * max_pos_ind - 2,\n                    offs_pos_w, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w = offs_n[None, :] - offs_m[:, None\n                    ] + MAX_SEQ_LEN - 1\n    if HAS_MULTIPLE_TARGETS:\n        invalid_mask = offs_m[:, None] == offs_n[None, :]\n    elif MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask = offs_m[:, None] >= offs_n[None, :] and offs_m[:,\n                None] - offs_n[None, :] <= MAX_ATTN_LEN\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask = offs_m[:, None] <= offs_n[None, :] and offs_n[\n                None, :] - offs_m[:, None] <= MAX_ATTN_LEN\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask = offs_m[:, None] >= offs_n[None, :]\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask = offs_m[:, None] <= offs_n[None, :]\n    for off_z in range(Z):\n        seq_start = tl.load(seq_offsets + off_z)\n        seq_end = tl.load(seq_offsets + off_z + 1)\n        seq_len = (seq_end - seq_start).to(tl.int32)\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            if HAS_MULTIPLE_TARGETS:\n                low = start_n\n                if MAX_ATTN_LEN > 0:\n                    n_targets = tl.load(num_targets + off_z).to(tl.int32)\n                    high = start_n + MAX_ATTN_LEN + BLOCK_N\n                    high = high if high + n_targets < seq_len else seq_len\n                else:\n                    high = seq_len\n            else:\n                low = start_n\n                if MAX_ATTN_LEN > 0:\n                    high = start_n + MAX_ATTN_LEN + BLOCK_N\n                    high = high if high < seq_len else seq_len\n                else:\n                    high = seq_len\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            low = 0\n            high = start_n + BLOCK_N\n        if start_n < seq_len and (start_m >= low and start_m < high):\n            q_ptrs = Q + seq_start * stride_qm + offs_m[:, None\n                ] * stride_qm + offs_qk_d[None, :]\n            k_ptrs = K + seq_start * stride_kn + offs_n[:, None\n                ] * stride_kn + offs_qk_d[None, :]\n            v_ptrs = V + seq_start * stride_vn + offs_n[:, None\n                ] * stride_vn + offs_v_d[None, :]\n            do_ptrs = DOut + seq_start * stride_dom + offs_m[:, None\n                ] * stride_dom + offs_v_d[None, :]\n            mask_m = offs_m < seq_len\n            mask_n = offs_n < seq_len\n            if HAS_MULTIPLE_TARGETS:\n                if (INVALID_MASK_TYPE != 'lower_triangular' or MAX_ATTN_LEN ==\n                    0):\n                    n_targets = tl.load(num_targets + off_z).to(tl.int32)\n                if INVALID_MASK_TYPE == 'lower_triangular':\n                    pos_offs_m = tl.where(offs_m < seq_len - n_targets,\n                        offs_m, seq_len - n_targets)\n                    pos_offs_n = tl.where(offs_n < seq_len - n_targets,\n                        offs_n, seq_len - n_targets)\n                elif INVALID_MASK_TYPE == 'upper_triangular':\n                    pos_offs_m = tl.where(offs_m > n_targets - 1, offs_m, \n                        n_targets - 1)\n                    pos_offs_n = tl.where(offs_n > n_targets - 1, offs_n, \n                        n_targets - 1)\n            else:\n                pos_offs_n = offs_n\n                pos_offs_m = offs_m\n            mt_offs_pos_w = None\n            if USE_POS_BIAS:\n                if HAS_MULTIPLE_TARGETS:\n                    if HAS_MAX_POS_IND:\n                        mt_offs_pos_w = pos_offs_n[None, :] - pos_offs_m[:,\n                            None] + max_pos_ind - 1\n                        mt_offs_pos_w = tl.where(mt_offs_pos_w > 0,\n                            mt_offs_pos_w, 0)\n                        mt_offs_pos_w = tl.where(mt_offs_pos_w < 2 *\n                            max_pos_ind - 2, mt_offs_pos_w, 2 * max_pos_ind - 2\n                            )\n                    else:\n                        mt_offs_pos_w = pos_offs_n[None, :] - pos_offs_m[:,\n                            None] + MAX_SEQ_LEN - 1\n                else:\n                    mt_offs_pos_w = offs_pos_w\n            if HAS_MULTIPLE_TARGETS:\n                if MAX_ATTN_LEN > 0:\n                    if INVALID_MASK_TYPE == 'lower_triangular':\n                        mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                            ] > pos_offs_n[None, :] and pos_offs_n[None, :\n                            ] - pos_offs_m[:, None] >= -MAX_ATTN_LEN\n                    elif INVALID_MASK_TYPE == 'upper_triangular':\n                        mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                            ] < pos_offs_n[None, :] and pos_offs_n[None, :\n                            ] - pos_offs_m[:, None] <= MAX_ATTN_LEN\n                elif INVALID_MASK_TYPE == 'lower_triangular':\n                    mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                        ] > pos_offs_n[None, :]\n                elif INVALID_MASK_TYPE == 'upper_triangular':\n                    mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                        ] < pos_offs_n[None, :]\n            else:\n                mt_invalid_mask = invalid_mask\n            if CONTEXTUAL_SEQ_LEN > 0:\n                if INVALID_MASK_TYPE == 'lower_triangular':\n                    row_filter = offs_m < CONTEXTUAL_SEQ_LEN\n                    if HAS_MULTIPLE_TARGETS:\n                        col_filter = offs_n < seq_len - n_targets\n                    else:\n                        col_filter = offs_n < seq_len\n                    invalid_mask = invalid_mask or row_filter[:, None\n                        ] and col_filter[None, :]\n            ts = None\n            if USE_TIME_BIAS:\n                ts_ptrs = TS + off_z * stride_ts\n                ts_0_ptrs = ts_ptrs + offs_m\n                ts_1_ptrs = ts_ptrs + offs_n\n                if CAUSAL:\n                    ts_0 = tl.load(ts_0_ptrs + 1, mask=mask_m)\n                    ts_1 = tl.load(ts_1_ptrs, mask=mask_n)\n                else:\n                    ts_0 = tl.load(ts_0_ptrs, mask=mask_m)\n                    ts_1 = tl.load(ts_1_ptrs + 1, mask=mask_n)\n                ts = ts_0[:, None] - ts_1[None, :]\n                ts = ts + time_delta\n                ts = tl.where(ts > 1e-06, ts, 1e-06)\n                ts = ts * (1.0 / time_bucket_incr)\n                if BUCKET_FN == 'log':\n                    ts = tl.log(ts)\n                elif BUCKET_FN == 'sqrt':\n                    ts = tl.sqrt(ts)\n                ts = ts * (1.0 / time_bucket_div)\n                ts = ts.to(tl.int32)\n                ts = tl.where(ts > 0, ts, 0)\n                ts = tl.where(ts < num_buckets, ts, num_buckets)\n            attn_bias = tl.zeros([BLOCK_N, BLOCK_N], dtype=tl.float32)\n            if USE_TIME_BIAS:\n                ts_w = tl.load(TW + ts, mask=mask_m[:, None] & mask_n[None,\n                    :] & mt_invalid_mask)\n                attn_bias = attn_bias + ts_w\n            if USE_POS_BIAS:\n                pos_w = tl.load(PW + mt_offs_pos_w, mask=mask_m[:, None] &\n                    mask_n[None, :] & mt_invalid_mask)\n                attn_bias = attn_bias + pos_w\n            dbias = tl.zeros((BLOCK_N, BLOCK_N), dtype=tl.float32)\n            for off_h in range(H):\n                q = tl.load(q_ptrs + off_h * stride_qh, mask=mask_m[:, None\n                    ], other=0.0)\n                k = tl.load(k_ptrs + off_h * stride_kh, mask=mask_n[:, None\n                    ], other=0.0)\n                qk = tl.dot(q, tl.trans(k), allow_tf32=ALLOW_TF32) * alpha\n                qk = qk + attn_bias\n                sig = fast_dividef(1.0, 1.0 + tl.exp(-qk))\n                do = tl.load(do_ptrs + off_h * stride_doh, mask=mask_m[:,\n                    None], other=0.0)\n                v = tl.load(v_ptrs + off_h * stride_vh, mask=mask_n[:, None\n                    ], other=0.0)\n                dqk = tl.dot(do, tl.trans(v), allow_tf32=ALLOW_TF32)\n                dqk = dqk * sig * (1 + qk * (1 - sig)) * (1.0 / MAX_SEQ_LEN)\n                dbias = dbias + dqk\n            if USE_TIME_BIAS:\n                dtw_ptrs = DTW + widx * (num_buckets + 1)\n                tl.atomic_add(dtw_ptrs + ts, dbias, mask=mask_m[:, None] &\n                    mask_n[None, :] & mt_invalid_mask, sem='relaxed')\n            if USE_POS_BIAS:\n                if HAS_MULTIPLE_TARGETS:\n                    if HAS_MAX_POS_IND:\n                        dpw_ptrs = DPW + widx * (2 * max_pos_ind - 1)\n                    else:\n                        dpw_ptrs = DPW + widx * (2 * MAX_SEQ_LEN - 1)\n                    tl.atomic_add(dpw_ptrs + mt_offs_pos_w, dbias, mask=\n                        mask_m[:, None] & mask_n[None, :] & mt_invalid_mask,\n                        sem='relaxed')\n                else:\n                    dbias_pos += dbias\n    if USE_POS_BIAS and not HAS_MULTIPLE_TARGETS:\n        if HAS_MAX_POS_IND:\n            dpw_ptrs = DPW + widx * (2 * max_pos_ind - 1)\n        else:\n            dpw_ptrs = DPW + widx * (2 * MAX_SEQ_LEN - 1)\n        tl.atomic_add(dpw_ptrs + offs_pos_w, dbias_pos, mask=invalid_mask,\n            sem='relaxed')\n"
    },
    {
      "input": "@triton_autotune(configs=_add_position_embeddings_configs(), key=[\n    'AUTOTUNE_MAX_SEQ_LEN'])\n@triton.jit\ndef _add_position_embeddings_kernel(Jagged, seq_offsets, high_inds, Dense,\n    Out, AUTOTUNE_MAX_SEQ_LEN, D, scale, stride_jn, stride_dk, stride_on,\n    SCALE_JAGGED: tl.constexpr, BLOCK_D: tl.constexpr, BLOCK_N: tl.constexpr):\n    \"\"\"\n    Jagged has shape (sum_B(N_i), D),\n    Dense has shape (K, D),\n    Out has shape (sum_B(N_i), D)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(seq_offsets + off_b)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    max_ind = tl.load(high_inds + off_b)\n    seq_len = seq_end - seq_start\n    start_n = off_n * BLOCK_N\n    if start_n >= seq_len:\n        return\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    clamped_offs_n = tl.where(offs_n >= max_ind, max_ind, offs_n)\n    offs_d = tl.arange(0, BLOCK_D)\n    Jagged += seq_start.to(tl.int64) * stride_jn\n    jagged_ptr_offsets = offs_n[:, None] * stride_jn + offs_d[None, :]\n    Out += seq_start.to(tl.int64) * stride_on\n    out_ptrs = Out + offs_n[:, None] * stride_on + offs_d[None, :]\n    dense_ptrs = Dense + clamped_offs_n[:, None] * stride_dk + offs_d[None, :]\n    for _d in range(0, D, BLOCK_D):\n        mask = offs_n[:, None] < seq_len and offs_d[None, :] < D\n        jg = tl.load(Jagged + jagged_ptr_offsets, mask=mask)\n        if SCALE_JAGGED:\n            jg = jg * scale\n        dn = tl.load(dense_ptrs, mask=mask)\n        jg += dn\n        tl.store(out_ptrs, jg, mask=mask)\n        dense_ptrs += BLOCK_D\n        out_ptrs += BLOCK_D\n        offs_d += BLOCK_D\n        jagged_ptr_offsets += BLOCK_D\n"
    },
    {
      "input": "@triton.jit\ndef _add_position_embeddings_bwd_kernel(Jagged, seq_offsets, high_inds,\n    DenseOut, JaggedOut, B, D, scale, stride_jn, stride_jon, stride_don,\n    SCALE_JAGGED: tl.constexpr, BLOCK_D: tl.constexpr):\n    off_k = tl.program_id(0)\n    offs_d = tl.arange(0, BLOCK_D)\n    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)\n    for off_b in range(0, B):\n        max_ind = tl.load(high_inds + off_b)\n        if off_k < max_ind:\n            seq_start = tl.load(seq_offsets + off_b)\n            jagged_ptr = Jagged + seq_start.to(tl.int64\n                ) * stride_jn + off_k.to(tl.int64) * stride_jn\n            jagged_ptrs = jagged_ptr + offs_d\n            jg = tl.load(jagged_ptrs, mask=offs_d < D)\n            accumulator += jg\n            if SCALE_JAGGED:\n                out_jagged_ptr = JaggedOut + seq_start.to(tl.int64\n                    ) * stride_jon + off_k.to(tl.int64) * stride_jon\n                out_jagged_ptrs = out_jagged_ptr + offs_d\n                tl.store(out_jagged_ptrs, jg * scale, mask=offs_d < D)\n        elif off_k == max_ind:\n            seq_start = tl.load(seq_offsets + off_b).to(tl.int64)\n            seq_end = tl.load(seq_offsets + off_b + 1)\n            for k in range(seq_start + max_ind, seq_end):\n                jagged_ptr = Jagged + k * stride_jn\n                jagged_ptrs = jagged_ptr + offs_d\n                jg = tl.load(jagged_ptrs, mask=offs_d < D)\n                accumulator += jg\n                if SCALE_JAGGED:\n                    out_jagged_ptr = JaggedOut + k * stride_jon\n                    out_jagged_ptrs = out_jagged_ptr + offs_d\n                    tl.store(out_jagged_ptrs, jg * scale, mask=offs_d < D)\n    out = accumulator.to(DenseOut.dtype.element_ty)\n    out_ptrs = DenseOut + off_k * stride_don + offs_d\n    tl.store(out_ptrs, out, mask=offs_d < D)\n"
    },
    {
      "input": "@triton_autotune(configs=_add_position_embeddings_configs(), key=[\n    'AUTOTUNE_MAX_SEQ_LEN'])\n@triton.jit\ndef _add_timestamp_position_embeddings_kernel(SeqEmb, Offsets, Lengths,\n    PosEmb, TsEmb, Out, TS, PosInds, TsInds, NumTargets,\n    AUTOTUNE_MAX_SEQ_LEN, D, num_time_buckets, time_bucket_increments,\n    time_bucket_scale, time_delta, max_contextual_seq_len, max_pos_ind,\n    stride_sn, stride_pn, stride_tn, stride_ts, stride_on, TRAINING: tl.\n    constexpr, HAS_MULTIPLE_TARGETS: tl.constexpr, INTERLEAVE_TARGETS: tl.\n    constexpr, TIME_BUCKET_FN: tl.constexpr, BLOCK_D: tl.constexpr, BLOCK_N:\n    tl.constexpr):\n    \"\"\"\n    SeqEmb has shape (sum_B(N_i), D),\n    PosEmb has shape (N_p, D),\n    TsEmb has shape (N_t, D),\n    Out has shape (sum_B(N_i), D)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(Offsets + off_b)\n    seq_end = tl.load(Offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    start_n = off_n * BLOCK_N\n    if start_n >= seq_len:\n        return\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    seq_emb_offsets = offs_n[:, None] * stride_sn + offs_d[None, :]\n    SeqEmb += seq_start.to(tl.int64) * stride_sn\n    mask_n = offs_n < seq_len\n    seq_len = tl.load(Lengths + off_b)\n    if HAS_MULTIPLE_TARGETS:\n        num_targets = tl.load(NumTargets + off_b)\n        if INTERLEAVE_TARGETS:\n            high_ind = seq_len - num_targets * 2\n        else:\n            high_ind = seq_len - num_targets\n    else:\n        high_ind = seq_len\n    pos_inds = tl.where(offs_n < high_ind, offs_n, high_ind)\n    pos_inds = high_ind - pos_inds + max_contextual_seq_len\n    pos_inds = tl.where(pos_inds < max_pos_ind - 1, pos_inds, max_pos_ind - 1)\n    pos_inds = tl.where(offs_n < max_contextual_seq_len, offs_n, pos_inds)\n    if TRAINING:\n        tl.store(PosInds + seq_start + offs_n, pos_inds, mask=mask_n)\n    pos_emb_offsets = pos_inds[:, None] * stride_pn + offs_d[None, :]\n    ts = tl.load(TS + off_b * stride_ts + offs_n, mask=mask_n)\n    query_time = tl.load(TS + off_b * stride_ts + seq_len - 1)\n    ts = query_time - ts + time_delta\n    ts = tl.where(ts > 1e-06, ts, 1e-06) / time_bucket_increments\n    if TIME_BUCKET_FN == 'log':\n        ts = tl.log(ts)\n    else:\n        ts = tl.sqrt(ts)\n    ts = ts * time_bucket_scale\n    ts = ts.to(tl.int32)\n    ts = tl.where(ts > 0, ts, 0)\n    ts = tl.where(ts < num_time_buckets, ts, num_time_buckets)\n    if TRAINING:\n        tl.store(TsInds + seq_start + offs_n, ts, mask=mask_n)\n    ts_emb_offsets = ts[:, None] * stride_tn + offs_d[None, :]\n    Out += seq_start.to(tl.int64) * stride_on\n    out_offsets = Out + offs_n[:, None] * stride_on + offs_d[None, :]\n    for _d in range(0, D, BLOCK_D):\n        mask = offs_n[:, None] < seq_len and offs_d[None, :] < D\n        seq_emb = tl.load(SeqEmb + seq_emb_offsets, mask=mask)\n        pos_emb = tl.load(PosEmb + pos_emb_offsets, mask=mask)\n        ts_emb = tl.load(TsEmb + ts_emb_offsets, mask=mask)\n        tl.store(out_offsets, seq_emb + (pos_emb + ts_emb).to(seq_emb.dtype\n            ), mask=mask)\n        seq_emb_offsets += BLOCK_D\n        pos_emb_offsets += BLOCK_D\n        ts_emb_offsets += BLOCK_D\n        out_offsets += BLOCK_D\n        offs_d += BLOCK_D\n"
    },
    {
      "input": "@triton_autotune(configs=_add_embeddings_bwd_configs(), key=[\n    'AUTOTUNE_MAX_SEQ_LEN', 'AUTOTUNE_B', 'D'])\n@triton.jit\ndef _add_embeddings_bwd_kernel(In, KeyInds, ValueInds, Out,\n    AUTOTUNE_MAX_SEQ_LEN, B, AUTOTUNE_B, D, jagged_size, stride_in,\n    stride_on, BLOCK_D: tl.constexpr, BLOCK: tl.constexpr):\n    off_block = tl.program_id(0)\n    offs_d = tl.arange(0, BLOCK_D)\n    mask_d = offs_d < D\n    key_ind = -1\n    key_ind = key_ind.to(KeyInds.dtype.element_ty)\n    accumulator = tl.zeros((BLOCK_D,), dtype=In.dtype.element_ty)\n    for off_i in range(0, BLOCK):\n        off = off_block * BLOCK + off_i\n        if off < jagged_size:\n            value_ind = tl.load(ValueInds + off)\n            in_offset = In + value_ind.to(tl.int64) * stride_in\n            jagged_in = tl.load(in_offset + offs_d, mask=mask_d)\n            key_ind_new = tl.load(KeyInds + off)\n            if key_ind == key_ind_new:\n                accumulator += jagged_in\n            else:\n                if key_ind >= 0:\n                    out_offset = Out + key_ind.to(tl.int64) * stride_on\n                    tl.atomic_add(out_offset + offs_d, accumulator.to(Out.\n                        dtype.element_ty), mask=mask_d, sem='relaxed')\n                key_ind = key_ind_new\n                accumulator = jagged_in\n    if key_ind >= 0:\n        out_offset = Out + key_ind.to(tl.int64) * stride_on\n        tl.atomic_add(out_offset + offs_d, accumulator.to(Out.dtype.\n            element_ty), mask=mask_d, sem='relaxed')\n"
    },
    {
      "input": "@triton.autotune(configs=get_mm_configs(), key=['N', 'K'])\n@triton.jit\ndef _addmm_fwd(x_ptr, w_ptr, y_ptr, z_ptr, M, N, K, stride_xm, stride_xk,\n    stride_wk, stride_wn, stride_ym, stride_yn, stride_zm, stride_zn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr, ALLOW_TF32: tl.constexpr, BROADCAST_Y: tl.constexpr\n    ):\n    pid_0, pid_1 = tl.program_id(axis=0), tl.program_id(axis=1)\n    pid = pid_0 * tl.num_programs(axis=1) + pid_1\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_K)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (pid_m * BLOCK_M + offs_m)[:, None] < M\n    mask_n = (pid_n * BLOCK_N + offs_n)[None, :] < N\n    x_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_xm\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n        )\n    w_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_wn\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n        )\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        mask_k = offs_k[None, :] < K - k * BLOCK_K\n        x = tl.load(x_ptrs, mask=mask_k & mask_m, other=0.0)\n        mask_k = offs_k[:, None] < K - k * BLOCK_K\n        w = tl.load(w_ptrs, mask=mask_k & mask_n, other=0.0)\n        accumulator += tl.dot(x, w, allow_tf32=ALLOW_TF32)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    z_mask = mask_m & mask_n\n    if BROADCAST_Y:\n        y_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_yn\n        y_ptrs = y_ptr + stride_yn * offs_n[None, :]\n        y = tl.load(y_ptrs, mask=mask_n)\n    else:\n        y_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_ym\n        y_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_yn\n        y_ptrs = y_ptr + stride_ym * offs_m[:, None] + stride_yn * offs_n[\n            None, :]\n        y = tl.load(y_ptrs, mask=z_mask)\n    z = (accumulator + y.to(tl.float32)).to(z_ptr.dtype.element_ty)\n    z_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_zm\n    z_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_zn\n    z_ptrs = z_ptr + stride_zm * offs_m[:, None] + stride_zn * offs_n[None, :]\n    tl.store(z_ptrs, z, mask=z_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd(X, Y, Mean, Rstd, D, eps, stride_x, stride_y, TRAINING:\n    tl.constexpr, BLOCK_D: tl.constexpr, COMPUTE_MEAN_AND_RSTD: tl.constexpr):\n    row = tl.program_id(0)\n    X += row.to(tl.int64) * stride_x\n    Y += row.to(tl.int64) * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)\n    if COMPUTE_MEAN_AND_RSTD:\n        mean = tl.sum(x, axis=0) / D\n    else:\n        mean = tl.load(Mean + row)\n    x_mean = tl.where(cols < D, x - mean, 0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n        _var += x_mean * x_mean\n        var = tl.sum(_var, axis=0) / D\n        rstd = 1 / tl.sqrt(var + eps)\n        if TRAINING:\n            tl.store(Mean + row, mean)\n            tl.store(Rstd + row, rstd)\n    else:\n        rstd = tl.load(Rstd + row)\n    mask = cols < D\n    y = x_mean * rstd\n    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _weighted_layer_norm_fwd(X, Y, W, B, Mean, Rstd, D, eps, stride_x,\n    stride_y, IS_SWISH: tl.constexpr, TRAINING: tl.constexpr, BLOCK_D: tl.\n    constexpr, COMPUTE_MEAN_AND_RSTD: tl.constexpr):\n    row = tl.program_id(0)\n    X += row.to(tl.int64) * stride_x\n    Y += row.to(tl.int64) * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)\n    if COMPUTE_MEAN_AND_RSTD:\n        mean = tl.sum(x, axis=0) / D\n    else:\n        mean = tl.load(Mean + row)\n    x_mean = tl.where(cols < D, x - mean, 0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n        _var += x_mean * x_mean\n        var = tl.sum(_var, axis=0) / D\n        rstd = 1 / tl.sqrt(var + eps)\n        if TRAINING:\n            tl.store(Mean + row, mean)\n            tl.store(Rstd + row, rstd)\n    else:\n        rstd = tl.load(Rstd + row)\n    mask = cols < D\n    y = x_mean * rstd\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\n    y = y * w + b\n    if IS_SWISH:\n        y = tl.sigmoid(y) * x\n    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx(DX, DY, X, Mean, Rstd, stride_dx, stride_dy,\n    stride_x, D, eps, BLOCK_D: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    X += row.to(tl.int64) * stride_x\n    DY += row.to(tl.int64) * stride_dy\n    DX += row.to(tl.int64) * stride_dx\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    dy = tl.where(mask, dy, 0.0)\n    c1 = tl.sum(xhat * dy, axis=0) / D\n    c2 = tl.sum(dy, axis=0) / D\n    dx = (dy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _weighted_layer_norm_bwd_dx(DX, DY, DW, DB, X, W, B, Mean, Rstd,\n    stride_dx, stride_dy, stride_x, D, eps, IS_SWISH: tl.constexpr, N,\n    BLOCK_D: tl.constexpr):\n    pid = tl.program_id(0)\n    tile_num = tl.num_programs(0)\n    rows_per_tile = N // tile_num\n    if pid < N % tile_num:\n        rows_per_tile += 1\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    row = pid\n    for idx in range(rows_per_tile):\n        x_ptrs = X + row.to(tl.int64) * stride_x\n        dy_ptrs = DY + row.to(tl.int64) * stride_dy\n        dx_ptrs = DX + row.to(tl.int64) * stride_dx\n        dw_ptrs = DW + pid.to(tl.int64) * D\n        dw_ptrs += cols\n        db_ptrs = DB + pid.to(tl.int64) * D\n        db_ptrs += cols\n        x = tl.load(x_ptrs + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(dy_ptrs + cols, mask=mask, other=0).to(tl.float32)\n        mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        wdy = w * dy\n        xhat = tl.where(mask, xhat, 0.0)\n        wdy = tl.where(mask, wdy, 0.0)\n        sigmoid_layer_norm = None\n        if IS_SWISH:\n            b = tl.load(B + cols, mask=mask).to(tl.float32)\n            sigmoid_layer_norm = tl.sigmoid(xhat * w + b)\n            sigmoid_layer_norm = tl.where(mask, sigmoid_layer_norm, 0.0)\n            x_ = wdy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)\n            x_ = tl.where(mask, x_, 0.0)\n            c1 = tl.sum(xhat * x_, axis=0) / D\n            c2 = tl.sum(x_, axis=0) / D\n            dx = (x_ - (xhat * c1 + c2)) * rstd\n            dx = dy * sigmoid_layer_norm + dx\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / D\n            c2 = tl.sum(wdy, axis=0) / D\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        tl.store(dx_ptrs + cols, dx, mask=mask)\n        if IS_SWISH:\n            partial_dw = dy * x * xhat * sigmoid_layer_norm * (1 -\n                sigmoid_layer_norm)\n            partial_db = dy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)\n        else:\n            partial_dw = dy * xhat\n            partial_db = dy\n        if idx > 0:\n            partial_dw += tl.load(dw_ptrs, mask=mask)\n            partial_db += tl.load(db_ptrs, mask=mask)\n        tl.store(dw_ptrs, partial_dw, mask=mask)\n        tl.store(db_ptrs, partial_db, mask=mask)\n        row += tile_num\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, D, BLOCK_N: tl.\n    constexpr, BLOCK_D: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=\n        cols < D)\n    tl.store(FINAL_DB + cols, sum_db.to(FINAL_DB.dtype.element_ty), mask=\n        cols < D)\n"
    },
    {
      "input": "@triton.jit\ndef _weighted_rms_norm_fwd(X, Y, W, Rstd, D, eps, stride_x, stride_y,\n    BLOCK_D: tl.constexpr):\n    row = tl.program_id(0)\n    X += row.to(tl.int64) * stride_x\n    Y += row.to(tl.int64) * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)\n    _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n    x_mean = tl.where(cols < D, x, 0.0)\n    _var += x_mean * x_mean\n    var = tl.sum(_var, axis=0) / D\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < D\n    y = x_mean * rstd\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    y = y * w\n    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _weighted_rms_norm_bwd_dx(DX, DY, DW, X, W, Rstd, Lock, stride_dx,\n    stride_dy, stride_x, D, eps, GROUP_N, BLOCK_D: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    X += row.to(tl.int64) * stride_x\n    DY += row.to(tl.int64) * stride_dy\n    DX += row.to(tl.int64) * stride_dx\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    rstd = tl.load(Rstd + row)\n    xhat = x * rstd\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / D\n    dx = (wdy - xhat * c1) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    lock_id = row % GROUP_N\n    Lock += lock_id\n    Count = Lock + GROUP_N\n    DW = DW + lock_id * D + cols\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _rms_norm_bwd_dwdb(DW, FINAL_DW, N, D, BLOCK_N: tl.constexpr, BLOCK_D:\n    tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=\n        cols < D)\n"
    },
    {
      "input": "@triton.jit\ndef _ln_mul_dropout_fwd(X, U, Y, W, B, Mean, Rstd, D, eps, seed,\n    dropout_ratio, stride_x, stride_u, stride_y, BLOCK_D: tl.constexpr,\n    TRAINING: tl.constexpr, CONCAT_UX: tl.constexpr):\n    row = tl.program_id(0)\n    X += row.to(tl.int64) * stride_x\n    U += row.to(tl.int64) * stride_u\n    Y += row.to(tl.int64) * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    mean = 0.0\n    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)\n    mean = tl.sum(x, axis=0) / D\n    _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n    x_mean = tl.where(cols < D, x - mean, 0.0)\n    _var += x_mean * x_mean\n    var = tl.sum(_var, axis=0) / D\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    mask = cols < D\n    y = x_mean * rstd\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\n    y = y * w + b\n    u = tl.load(U + cols, mask=cols < D, other=0.0).to(tl.float32)\n    y = y * u\n    if TRAINING:\n        random_offsets = row * BLOCK_D + cols\n        if CONCAT_UX:\n            random_u = tl.rand(seed, random_offsets)\n            u_keep = random_u > dropout_ratio\n            u = tl.where(u_keep, u / (1.0 - dropout_ratio), 0.0)\n            random_x = tl.rand(seed, random_offsets + D)\n            x_keep = random_x > dropout_ratio\n            x = tl.where(x_keep, x / (1.0 - dropout_ratio), 0.0)\n            random_y = tl.rand(seed, random_offsets + 2 * D)\n            y_keep = random_y > dropout_ratio\n            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)\n        else:\n            random = tl.rand(seed, random_offsets)\n            y_keep = random > dropout_ratio\n            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)\n    if CONCAT_UX:\n        tl.store(Y + cols, u.to(Y.dtype.element_ty), mask=mask)\n        tl.store(Y + D + cols, x.to(Y.dtype.element_ty), mask=mask)\n        tl.store(Y + 2 * D + cols, y.to(Y.dtype.element_ty), mask=mask)\n    else:\n        tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _ln_mul_dropout_bwd_dx_du(DX, DU, DY, DW, DB, X, U, Y, W, B, Mean, Rstd,\n    stride_dx, stride_du, stride_dy, stride_x, stride_u, stride_y, D, eps,\n    seed, dropout_ratio, N, BLOCK_D: tl.constexpr, TRAINING: tl.constexpr,\n    CONCAT_UX: tl.constexpr, COMPUTE_Y: tl.constexpr):\n    pid = tl.program_id(0)\n    tile_num = tl.num_programs(0)\n    rows_per_tile = N // tile_num\n    if pid < N % tile_num:\n        rows_per_tile += 1\n    if rows_per_tile == 0:\n        return\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    row = pid\n    X += row.to(tl.int64) * stride_x\n    U += row.to(tl.int64) * stride_u\n    if COMPUTE_Y:\n        Y += row.to(tl.int64) * stride_y\n    DY += row.to(tl.int64) * stride_dy\n    DX += row.to(tl.int64) * stride_dx\n    DU += row.to(tl.int64) * stride_du\n    DW = DW + pid * D + cols\n    DB = DB + pid * D + cols\n    for idx in range(0, rows_per_tile):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        if CONCAT_UX:\n            du = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n            dx = tl.load(DY + D + cols, mask=mask, other=0).to(tl.float32)\n            dy = tl.load(DY + 2 * D + cols, mask=mask, other=0).to(tl.float32)\n        else:\n            du = tl.zeros([BLOCK_D], dtype=tl.float32)\n            dx = tl.zeros([BLOCK_D], dtype=tl.float32)\n            dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if TRAINING:\n            random_offsets = row * BLOCK_D + cols\n            if CONCAT_UX:\n                random_du = tl.rand(seed, random_offsets)\n                du_keep = random_du > dropout_ratio\n                du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)\n                random_dx = tl.rand(seed, random_offsets + D)\n                dx_keep = random_dx > dropout_ratio\n                dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)\n                random_dy = tl.rand(seed, random_offsets + 2 * D)\n                dy_keep = random_dy > dropout_ratio\n                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n            else:\n                random = tl.rand(seed, random_offsets)\n                dy_keep = random > dropout_ratio\n                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n        mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n        ln = xhat * w + b\n        du += dy * ln\n        tl.store(DU + cols, du.to(DU.dtype.element_ty), mask=mask)\n        u = tl.load(U + cols, mask=mask, other=0).to(tl.float32)\n        dy = dy * u\n        wdy = w * dy\n        if COMPUTE_Y:\n            y = ln * u\n            if TRAINING:\n                if CONCAT_UX:\n                    u = tl.where(du_keep, u / (1.0 - dropout_ratio), 0.0)\n                    x = tl.where(dx_keep, x / (1.0 - dropout_ratio), 0.0)\n                    y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n                else:\n                    y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n            if CONCAT_UX:\n                tl.store(Y + cols, u.to(Y.dtype.element_ty), mask=mask)\n                tl.store(Y + D + cols, x.to(Y.dtype.element_ty), mask=mask)\n                tl.store(Y + 2 * D + cols, y.to(Y.dtype.element_ty), mask=mask)\n            else:\n                tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)\n            Y += tile_num.to(tl.int64) * stride_y\n        xhat = tl.where(mask, xhat, 0.0)\n        wdy = tl.where(mask, wdy, 0.0)\n        c1 = tl.sum(xhat * wdy, axis=0) / D\n        c2 = tl.sum(wdy, axis=0) / D\n        dx += (wdy - (xhat * c1 + c2)) * rstd\n        tl.store(DX + cols, dx, mask=mask)\n        partial_dw = dy * xhat\n        partial_db = dy\n        if idx > 0:\n            partial_dw += tl.load(DW, mask=mask)\n            partial_db += tl.load(DB, mask=mask)\n        tl.store(DW, partial_dw, mask=mask)\n        tl.store(DB, partial_db, mask=mask)\n        X += tile_num.to(tl.int64) * stride_x\n        U += tile_num.to(tl.int64) * stride_u\n        DY += tile_num.to(tl.int64) * stride_dy\n        DX += tile_num.to(tl.int64) * stride_dx\n        DU += tile_num.to(tl.int64) * stride_du\n        row += tile_num\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _ln_mul_dropout_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, D, BLOCK_N: tl.\n    constexpr, BLOCK_D: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=\n        cols < D)\n    tl.store(FINAL_DB + cols, sum_db.to(FINAL_DB.dtype.element_ty), mask=\n        cols < D)\n"
    },
    {
      "input": "@triton.jit\ndef _group_norm_mul_dropout_fwd(X, U, Y, W, B, Mean, Rstd, D, Heads, eps,\n    seed, dropout_ratio, stride_x, stride_u, stride_y, BLOCK_D: tl.\n    constexpr, BLOCK_H: tl.constexpr, TRAINING: tl.constexpr, CONCAT_UX: tl\n    .constexpr):\n    row = tl.program_id(0)\n    X += row.to(tl.int64) * stride_x\n    U += row.to(tl.int64) * stride_u\n    Y += row.to(tl.int64) * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    heads = tl.arange(0, BLOCK_H)\n    offsets = heads[:, None] * D + cols[None, :]\n    mask_h = heads < Heads\n    mask_c = cols < D\n    mask = mask_c[None, :] & mask_h[:, None]\n    mean = 0.0\n    x = tl.load(X + offsets, mask=mask, other=0.0).to(tl.float32)\n    mean = tl.sum(x, axis=1) / D\n    mean = tl.ravel(mean)\n    _var = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)\n    x_mean = tl.where(mask, x - mean[:, None], 0.0)\n    _var += x_mean * x_mean\n    var = tl.sum(_var, axis=1) / D\n    var = tl.ravel(var)\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row * Heads + heads, mean, mask=mask_h)\n    tl.store(Rstd + row * Heads + heads, rstd, mask=mask_h)\n    y = x_mean * rstd[:, None]\n    w = tl.load(W + heads, mask=mask_h).to(tl.float32)\n    b = tl.load(B + heads, mask=mask_h).to(tl.float32)\n    y = y * w[:, None] + b[:, None]\n    u = tl.load(U + offsets, mask=mask, other=0.0).to(tl.float32)\n    y = y * u\n    if TRAINING:\n        if CONCAT_UX:\n            random_offsets = row * 3 * D * Heads + offsets\n            random_u = tl.rand(seed, random_offsets)\n            u_keep = random_u > dropout_ratio\n            u = tl.where(u_keep, u / (1.0 - dropout_ratio), 0.0)\n            random_x = tl.rand(seed, random_offsets + Heads * D)\n            x_keep = random_x > dropout_ratio\n            x = tl.where(x_keep, x / (1.0 - dropout_ratio), 0.0)\n            random_y = tl.rand(seed, random_offsets + 2 * Heads * D)\n            y_keep = random_y > dropout_ratio\n            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)\n        else:\n            random_offsets = row * D * Heads + offsets\n            random = tl.rand(seed, random_offsets)\n            y_keep = random > dropout_ratio\n            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)\n    if CONCAT_UX:\n        tl.store(Y + offsets, u.to(Y.dtype.element_ty), mask=mask)\n        tl.store(Y + Heads * D + offsets, x.to(Y.dtype.element_ty), mask=mask)\n        tl.store(Y + 2 * Heads * D + offsets, y.to(Y.dtype.element_ty),\n            mask=mask)\n    else:\n        tl.store(Y + offsets, y.to(Y.dtype.element_ty), mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _group_norm_mul_dropout_bwd_dx_du(DX, DU, DY, DW, DB, X, U, Y, W, B,\n    Mean, Rstd, stride_dx, stride_du, stride_dy, stride_x, stride_u,\n    stride_y, D, Heads, eps, seed, dropout_ratio, GROUP_N: tl.constexpr,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, TRAINING: tl.constexpr,\n    CONCAT_UX: tl.constexpr, COMPUTE_Y: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    off_heads = tl.arange(0, BLOCK_H)\n    mask_c = cols < D\n    mask_h = off_heads < Heads\n    mask = mask_c[None, :] & mask_h[:, None]\n    X += row.to(tl.int64) * stride_x\n    U += row.to(tl.int64) * stride_u\n    DY += row.to(tl.int64) * stride_dy\n    DX += row.to(tl.int64) * stride_dx\n    DU += row.to(tl.int64) * stride_du\n    offsets = off_heads[:, None] * D + cols[None, :]\n    x = tl.load(X + offsets, mask=mask, other=0).to(tl.float32)\n    if CONCAT_UX:\n        du = tl.load(DY + offsets, mask=mask, other=0).to(tl.float32)\n        dx = tl.load(DY + Heads * D + offsets, mask=mask, other=0).to(tl.\n            float32)\n        dy = tl.load(DY + 2 * Heads * D + offsets, mask=mask, other=0).to(tl\n            .float32)\n    else:\n        du = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)\n        dx = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)\n        dy = tl.load(DY + offsets, mask=mask, other=0).to(tl.float32)\n    if TRAINING:\n        if CONCAT_UX:\n            random_offsets = row * 3 * D * Heads + offsets\n            random_du = tl.rand(seed, random_offsets)\n            du_keep = random_du > dropout_ratio\n            du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)\n            random_dx = tl.rand(seed, random_offsets + Heads * D)\n            dx_keep = random_dx > dropout_ratio\n            dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)\n            random_dy = tl.rand(seed, random_offsets + 2 * Heads * D)\n            dy_keep = random_dy > dropout_ratio\n            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n        else:\n            random_offsets = row * D * Heads + offsets\n            random = tl.rand(seed, random_offsets)\n            dy_keep = random > dropout_ratio\n            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n    mean = tl.load(Mean + row * Heads + off_heads)\n    rstd = tl.load(Rstd + row * Heads + off_heads)\n    xhat = (x - mean[:, None]) * rstd[:, None]\n    w = tl.load(W + off_heads, mask=mask_h).to(tl.float32)\n    b = tl.load(B + off_heads, mask=mask_h).to(tl.float32)\n    ln = xhat * w[:, None] + b[:, None]\n    du += dy * ln\n    tl.store(DU + offsets, du.to(DU.dtype.element_ty), mask=mask)\n    u = tl.load(U + offsets, mask=mask, other=0).to(tl.float32)\n    dy = dy * u\n    wdy = w[:, None] * dy\n    if COMPUTE_Y:\n        Y += row.to(tl.int64) * stride_y\n        y = ln * u\n        if TRAINING:\n            if CONCAT_UX:\n                u = tl.where(du_keep, u / (1.0 - dropout_ratio), 0.0)\n                x = tl.where(dx_keep, x / (1.0 - dropout_ratio), 0.0)\n                y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n            else:\n                y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n        if CONCAT_UX:\n            tl.store(Y + offsets, u.to(Y.dtype.element_ty), mask=mask)\n            tl.store(Y + Heads * D + offsets, x.to(Y.dtype.element_ty),\n                mask=mask)\n            tl.store(Y + 2 * Heads * D + offsets, y.to(Y.dtype.element_ty),\n                mask=mask)\n        else:\n            tl.store(Y + offsets, y.to(Y.dtype.element_ty), mask=mask)\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=1) / D\n    c2 = tl.sum(wdy, axis=1) / D\n    dx += (wdy - (xhat * c1[:, None] + c2[:, None])) * rstd[:, None]\n    tl.store(DX + offsets, dx, mask=mask)\n    lock_id = row % GROUP_N\n    DW = DW + lock_id * Heads + off_heads\n    DB = DB + lock_id * Heads + off_heads\n    partial_dw = tl.sum(dy * xhat, axis=1)\n    partial_dw = tl.ravel(partial_dw)\n    partial_db = tl.sum(dy, axis=1)\n    partial_db = tl.ravel(partial_db)\n    tl.atomic_add(DW, partial_dw, mask=mask_h, sem='relaxed')\n    tl.atomic_add(DB, partial_db, mask=mask_h, sem='relaxed')\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=[])\n@triton.jit\ndef _group_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, BLOCK_N: tl.constexpr):\n    col = tl.program_id(0)\n    num_heads = tl.num_programs(0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = rows < N\n        offs = rows * num_heads + col\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + col, sum_dw.to(FINAL_DW.dtype.element_ty))\n    tl.store(FINAL_DB + col, sum_db.to(FINAL_DB.dtype.element_ty))\n"
    },
    {
      "input": "@triton_autotune(configs=_get_configs(), key=['Z', 'H', 'N'])\n@triton.heuristics({'EVEN_M': lambda args: args['N'] % args['BLOCK_M'] == 0,\n    'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _jagged_bias_to_dense(Z, H, N, jg_offsets_ptr, jg2_offsets_ptr,\n    jagged_ptr, dense_bias_ptr, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr):\n    start_m = tl.program_id(0) * BLOCK_M\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    seq_start = tl.load(jg_offsets_ptr + off_z)\n    seq_end = tl.load(jg_offsets_ptr + off_z + 1)\n    seq_len = seq_end - seq_start\n    if start_m >= seq_len:\n        return\n    bias_start = tl.load(jg2_offsets_ptr + off_z\n        ) * H + off_h * seq_len * seq_len\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (offs_m < seq_len)[:, None]\n    off_jg_bias = bias_start + offs_m[:, None] * seq_len + offs_n[None, :]\n    jg_bias_ptrs = jagged_ptr + off_jg_bias\n    off_d_bias = off_hz * N * N + offs_m[:, None] * N + offs_n[None, :]\n    d_bias_ptrs = dense_bias_ptr + off_d_bias\n    for start_n in range(0, seq_len, BLOCK_N):\n        maxk_n = (offs_n < seq_len - start_n)[None, :]\n        jg_bias = tl.load(jg_bias_ptrs + start_n, mask=mask_m & maxk_n,\n            other=0.0)\n        tl.store(d_bias_ptrs + start_n, jg_bias, mask=mask_m & maxk_n)\n"
    },
    {
      "input": "@triton_autotune(configs=_get_configs(), key=['Z', 'H', 'N'])\n@triton.heuristics({'EVEN_M': lambda args: args['N'] % args['BLOCK_M'] == 0,\n    'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _dense_bias_to_jagged(Z, H, N, jg_offsets_ptr, jg2_offsets_ptr,\n    dense_bias_ptr, jagged_ptr, BLOCK_M: tl.constexpr, BLOCK_N: tl.\n    constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr):\n    start_m = tl.program_id(0) * BLOCK_M\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    seq_start = tl.load(jg_offsets_ptr + off_z)\n    seq_end = tl.load(jg_offsets_ptr + off_z + 1)\n    seq_len = seq_end - seq_start\n    if start_m >= seq_len:\n        return\n    bias_start = tl.load(jg2_offsets_ptr + off_z\n        ) * H + off_h * seq_len * seq_len\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (offs_m < seq_len)[:, None]\n    off_jg_bias = bias_start + offs_m[:, None] * seq_len + offs_n[None, :]\n    jg_bias_ptrs = jagged_ptr + off_jg_bias\n    off_d_bias = off_hz * N * N + offs_m[:, None] * N + offs_n[None, :]\n    d_bias_ptrs = dense_bias_ptr + off_d_bias\n    for start_n in range(0, seq_len, BLOCK_N):\n        maxk_n = (offs_n < seq_len - start_n)[None, :]\n        d_bias = tl.load(d_bias_ptrs + start_n, mask=mask_m & maxk_n, other=0.0\n            )\n        tl.store(jg_bias_ptrs + start_n, d_bias, mask=mask_m & maxk_n)\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bmm_configs(), key=['AUTOTUNE_MAX_SEQ_LEN',\n    'N', 'K'])\n@triton.jit\ndef jagged_dense_bmm_broadcast_add_kernel(seq_offsets, Jagged, Dense, Bias,\n    Out, AUTOTUNE_MAX_SEQ_LEN, N, K, stride_jm, stride_db, stride_dk,\n    stride_dn, stride_bias_b, stride_om, HAS_BIAS: tl.constexpr, ALLOW_TF32:\n    tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl\n    .constexpr):\n    \"\"\"\n    Computing bmm Out = Jagged x Dense + Bias\n    M is the jagged dimension\n    Jagged has shape (sum_B(M_i), K), Dense has shape (B, K, N), Bias has shape (B, N), and Out has shape (sum_B(M_i), N)\n    \"\"\"\n    off_n = tl.program_id(0)\n    off_m = tl.program_id(1)\n    off_b = tl.program_id(2)\n    seq_start = tl.load(seq_offsets + off_b).to(tl.int64)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    start_m = off_m * BLOCK_M\n    start_n = off_n * BLOCK_N\n    if start_m >= seq_len:\n        return\n    Jagged += seq_start * stride_jm\n    Dense += off_b.to(tl.int64) * stride_db\n    Out += seq_start * stride_om\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    jg_ptrs = Jagged + offs_m[:, None] * stride_jm + offs_k[None, :]\n    dn_ptrs = Dense + offs_k[:, None] * stride_dk + offs_n[None, :] * stride_dn\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        jg = tl.load(jg_ptrs, mask=offs_m[:, None] < seq_len and (k +\n            offs_k)[None, :] < K, other=0.0)\n        dn = tl.load(dn_ptrs, mask=(k + offs_k)[:, None] < K and offs_n[\n            None, :] < N, other=0.0)\n        accumulator += tl.dot(jg, dn, allow_tf32=ALLOW_TF32)\n        jg_ptrs += BLOCK_K\n        dn_ptrs += BLOCK_K * stride_dk\n    if HAS_BIAS:\n        bias_ptrs = Bias + off_b * stride_bias_b + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N)\n        accumulator += bias[None, :].to(tl.float32)\n    out = accumulator.to(Out.dtype.element_ty)\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    out_ptrs = Out + offs_m[:, None] * stride_om + offs_n[None, :]\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < seq_len) & (offs_n[None,\n        :] < N))\n"
    },
    {
      "input": "@triton_autotune(configs=_get_bmm_configs(), key=['M', 'N',\n    'AUTOTUNE_MAX_SEQ_LEN'])\n@triton.jit\ndef _jagged_jagged_bmm_reduce_sum(seq_offsets, JaggedA, JaggedB, Out,\n    ReduceOut, M, N, AUTOTUNE_MAX_SEQ_LEN, stride_ak, stride_bk, stride_ob,\n    stride_om, stride_on, stride_orb, stride_orn, REDUCE_JAGGEDB: tl.\n    constexpr, ALLOW_TF32: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl\n    .constexpr, BLOCK_K: tl.constexpr):\n    \"\"\"\n    Computing bmm Out = Jagged x Jagged\n    K is the jagged dimension\n    JaggedA has shape (sum_B(K_i), M), JaggedB has shape (sum_B(K_i), N), and Out has shape (B, M, N)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_m = tl.program_id(1)\n    off_n = tl.program_id(2)\n    seq_start = tl.load(seq_offsets + off_b).to(tl.int64)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    start_m = off_m * BLOCK_M\n    start_n = off_n * BLOCK_N\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    Out += off_b.to(tl.int64) * stride_ob\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    out_ptrs = Out + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    if REDUCE_JAGGEDB:\n        out_reduce_ptrs = ReduceOut + off_b * stride_orb + offs_n * stride_orn\n        acc_reduce = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if seq_len == 0:\n        out = accumulator.to(Out.dtype.element_ty)\n        tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None,\n            :] < N))\n        if REDUCE_JAGGEDB:\n            if off_m == 0:\n                tl.store(out_reduce_ptrs, acc_reduce.to(ReduceOut.dtype.\n                    element_ty), mask=offs_n < N)\n        return\n    JaggedA += seq_start * stride_ak\n    JaggedB += seq_start * stride_bk\n    offs_k = tl.arange(0, BLOCK_K)\n    jg_a_ptrs = JaggedA + offs_k[None, :] * stride_ak + offs_m[:, None]\n    jg_b_ptrs = JaggedB + offs_k[:, None] * stride_bk + offs_n[None, :]\n    for k in range(0, seq_len, BLOCK_K):\n        jg_a = tl.load(jg_a_ptrs, mask=offs_m[:, None] < M and (k + offs_k)\n            [None, :] < seq_len, other=0.0)\n        jg_b = tl.load(jg_b_ptrs, mask=offs_n[None, :] < N and (k + offs_k)\n            [:, None] < seq_len, other=0.0)\n        accumulator += tl.dot(jg_a, jg_b, allow_tf32=ALLOW_TF32)\n        if REDUCE_JAGGEDB:\n            if off_m == 0:\n                acc_reduce += tl.sum(jg_b, axis=0)\n        jg_a_ptrs += BLOCK_K * stride_ak\n        jg_b_ptrs += BLOCK_K * stride_bk\n    out = accumulator.to(Out.dtype.element_ty)\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n    if REDUCE_JAGGEDB:\n        if off_m == 0:\n            tl.store(out_reduce_ptrs, acc_reduce.to(ReduceOut.dtype.\n                element_ty), mask=offs_n < N)\n"
    },
    {
      "input": "@triton_autotune(configs=_get_jagged_dense_broadcast_add_configs(), key=[\n    'AUTOTUNE_MAX_SEQ_LEN'])\n@triton.jit\ndef jagged_dense_broadcast_add_kernel(seq_offsets, Jagged, Dense, Out,\n    AUTOTUNE_MAX_SEQ_LEN, D, stride_jn, stride_db, stride_on, BLOCK_N: tl.\n    constexpr, BLOCK_D: tl.constexpr):\n    \"\"\"\n    Computing Out = Jagged + Dense\n    JaggedA has shape (sum_B(N_i), D), Dense has shape (B, D), and Out has shape (sum_B(N_i), D)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(seq_offsets + off_b)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    start_n = off_n * BLOCK_N\n    if start_n >= seq_len:\n        return\n    Jagged += seq_start * stride_jn\n    Dense += off_b * stride_db\n    Out += seq_start * stride_on\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    jagged_ptrs = Jagged + offs_n[:, None] * stride_jn + offs_d[None, :]\n    dense_ptrs = Dense + offs_d\n    out_ptrs = Out + offs_n[:, None] * stride_jn + offs_d[None, :]\n    for d in range(0, D, BLOCK_D):\n        jg = tl.load(jagged_ptrs, mask=offs_n[:, None] < seq_len and (d +\n            offs_d)[None, :] < D)\n        dn = tl.load(dense_ptrs, mask=d + offs_d < D)\n        out = jg + dn[None, :]\n        tl.store(out_ptrs, out, mask=offs_n[:, None] < seq_len and (d +\n            offs_d)[None, :] < D)\n        dense_ptrs += BLOCK_D\n        jagged_ptrs += BLOCK_D\n        out_ptrs += BLOCK_D\n"
    },
    {
      "input": "@triton.jit\ndef jagged_reduce_sum(seq_offsets, Jagged, Out, D, stride_jn, stride_ob,\n    BLOCK_D: tl.constexpr):\n    \"\"\"\n    Computing Out = Jagged + Dense\n    JaggedA has shape (sum_B(N_i), D), Dense has shape (B, D), and Out has shape (sum_B(N_i), D)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_d = tl.program_id(1) * BLOCK_D\n    seq_start = tl.load(seq_offsets + off_b)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    Jagged += seq_start * stride_jn\n    Out += off_b * stride_ob\n    offs_d = off_d + tl.arange(0, BLOCK_D)\n    jagged_ptrs = Jagged + offs_d\n    out_ptrs = Out + offs_d\n    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)\n    for _ in range(0, seq_len):\n        jg = tl.load(jagged_ptrs, mask=offs_d < D)\n        accumulator += jg\n        jagged_ptrs += stride_jn\n    out = accumulator.to(Out.dtype.element_ty)\n    tl.store(out_ptrs, out, mask=offs_d < D)\n"
    },
    {
      "input": "@triton.jit\ndef concat_2D_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB,\n    DenseSize, Out, D, stride_ad, stride_bd, stride_dense_batch, stride_od,\n    n_prefix_from_B, IS_DENSE_A: tl.constexpr, IS_DENSE_B: tl.constexpr,\n    BLOCK_D: tl.constexpr, IS_REPLACE: tl.constexpr):\n    off_z = tl.program_id(1)\n    off_n = tl.program_id(0)\n    if IS_DENSE_A:\n        seq_start_a = off_z * DenseSize\n        seq_len_a = DenseSize\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    elif IS_DENSE_B:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = off_z * DenseSize\n        seq_len_b = DenseSize\n    else:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    if IS_REPLACE:\n        seq_len = seq_len_a\n    else:\n        seq_len = seq_len_a + seq_len_b\n    if off_n >= seq_len:\n        return\n    offs_d = tl.arange(0, BLOCK_D)\n    if IS_REPLACE:\n        out_seq_start = seq_start_a + off_n\n        out_seq_b_start = seq_len_a - seq_len_b\n    else:\n        out_seq_start = seq_start_a + seq_start_b + off_n\n        out_seq_b_start = seq_len_a + n_prefix_from_B\n    out_ptrs = Out + out_seq_start.to(tl.int64) * stride_od + offs_d\n    if off_n < out_seq_b_start and off_n >= n_prefix_from_B:\n        off_a = off_n - n_prefix_from_B\n        if IS_DENSE_A:\n            in_ptrs = ValuesA + off_a.to(tl.int64) * stride_ad + off_z.to(tl\n                .int64) * stride_dense_batch + offs_d\n        else:\n            in_ptrs = ValuesA + (off_a + seq_start_a).to(tl.int64\n                ) * stride_ad + offs_d\n    else:\n        off_b = off_n - out_seq_b_start + n_prefix_from_B\n        if off_n < n_prefix_from_B:\n            off_b += out_seq_b_start - n_prefix_from_B\n        if IS_DENSE_B:\n            in_ptrs = ValuesB + off_b.to(tl.int64) * stride_bd + off_z.to(tl\n                .int64) * stride_dense_batch + offs_d\n        else:\n            in_ptrs = ValuesB + (off_b + seq_start_b).to(tl.int64\n                ) * stride_bd + offs_d\n    v = tl.load(in_ptrs, mask=offs_d < D)\n    tl.store(out_ptrs, v, mask=offs_d < D)\n"
    },
    {
      "input": "@triton.jit\ndef concat_2D_jagged(OffsetsA, ValuesA, OffsetsB, ValuesB, DenseSize, Out,\n    D, stride_ad, stride_bd, stride_dense_batch, stride_od, IS_DENSE_A: tl.\n    constexpr, IS_DENSE_B: tl.constexpr, BLOCK_D: tl.constexpr, IS_REPLACE:\n    tl.constexpr):\n    concat_2D_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB,\n        DenseSize, Out, D, stride_ad, stride_bd, stride_dense_batch,\n        stride_od, 0, IS_DENSE_A, IS_DENSE_B, BLOCK_D, IS_REPLACE)\n"
    },
    {
      "input": "@triton.jit\ndef concat_2D_jagged_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB,\n    Out, D, stride_ad, stride_bd, stride_od, n_prefix_from_B, BLOCK_D: tl.\n    constexpr):\n    concat_2D_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB, 0, Out,\n        D, stride_ad, stride_bd, 0, stride_od, n_prefix_from_B, IS_DENSE_A=\n        False, IS_DENSE_B=False, BLOCK_D=BLOCK_D, IS_REPLACE=False)\n"
    },
    {
      "input": "@triton.jit\ndef split_2D_jagged_w_prefix(JaggedIn, DenseSize, OffsetsA, OffsetsB, OutA,\n    OutB, D, stride_id, stride_ad, stride_bd, n_prefix_to_B, IS_DENSE_A: tl\n    .constexpr, IS_DENSE_B: tl.constexpr, BLOCK_D: tl.constexpr, IS_REPLACE:\n    tl.constexpr):\n    off_z = tl.program_id(1)\n    off_n = tl.program_id(0)\n    if IS_DENSE_A:\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_start_a = off_z * DenseSize\n        seq_len_a = DenseSize\n        seq_len_b = seq_end_b - seq_start_b\n    elif IS_DENSE_B:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = off_z * DenseSize\n        seq_len_b = DenseSize\n    else:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    if IS_REPLACE:\n        seq_len = seq_len_a\n    else:\n        seq_len = seq_len_a + seq_len_b\n    if off_n >= seq_len:\n        return\n    if IS_REPLACE:\n        seq_start = seq_start_a\n        out_seq_b_start = seq_len_a - seq_len_b\n    else:\n        seq_start = seq_start_a + seq_start_b\n        out_seq_b_start = seq_len_a + n_prefix_to_B\n    offs_d = tl.arange(0, BLOCK_D)\n    in_ptrs = JaggedIn + (seq_start + off_n).to(tl.int64) * stride_id + offs_d\n    if off_n < out_seq_b_start and off_n >= n_prefix_to_B:\n        off_a = off_n - n_prefix_to_B\n        out_ptrs = OutA + (off_a + seq_start_a).to(tl.int64\n            ) * stride_ad + offs_d\n    else:\n        off_b = off_n - out_seq_b_start + n_prefix_to_B\n        if off_n < n_prefix_to_B:\n            off_b += out_seq_b_start - n_prefix_to_B\n        out_ptrs = OutB + (off_b + seq_start_b).to(tl.int64\n            ) * stride_bd + offs_d\n    v = tl.load(in_ptrs, mask=offs_d < D)\n    tl.store(out_ptrs, v, mask=offs_d < D)\n"
    },
    {
      "input": "@triton.jit\ndef split_2D_jagged(JaggedIn, DenseSize, OffsetsA, OffsetsB, OutA, OutB, D,\n    stride_id, stride_ad, stride_bd, IS_DENSE_A: tl.constexpr, IS_DENSE_B:\n    tl.constexpr, BLOCK_D: tl.constexpr, IS_REPLACE: tl.constexpr):\n    split_2D_jagged_w_prefix(JaggedIn, DenseSize, OffsetsA, OffsetsB, OutA,\n        OutB, D, stride_id, stride_ad, stride_bd, 0, IS_DENSE_A, IS_DENSE_B,\n        BLOCK_D, IS_REPLACE)\n"
    },
    {
      "input": "@triton.jit\ndef split_2D_jagged_jagged_w_prefix(JaggedIn, OffsetsA, OffsetsB, OutA,\n    OutB, D, stride_id, stride_ad, stride_bd, n_prefix_to_B, BLOCK_D: tl.\n    constexpr):\n    split_2D_jagged_w_prefix(JaggedIn, 0, OffsetsA, OffsetsB, OutA, OutB, D,\n        stride_id, stride_ad, stride_bd, n_prefix_to_B, IS_DENSE_A=False,\n        IS_DENSE_B=False, BLOCK_D=BLOCK_D, IS_REPLACE=False)\n"
    },
    {
      "input": "@triton.jit\ndef copy_2D_jagged(JaggedIn, Offsets, JaggedOut, OffsetsOut, D, stride_id,\n    stride_od, len_in, len_out, BLOCK_D: tl.constexpr):\n    off_z = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(Offsets + off_z)\n    seq_end = tl.load(Offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    seq_start_out = tl.load(OffsetsOut + off_z)\n    if off_n >= seq_len:\n        return\n    in_block_start = seq_start + off_n\n    out_block_start = seq_start_out + off_n\n    x = tl.load(tl.make_block_ptr(base=JaggedIn, shape=(len_in, D), strides\n        =(stride_id, 1), offsets=(in_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), boundary_check=(1, 0))\n    tl.store(tl.make_block_ptr(base=JaggedOut, shape=(len_out, D), strides=\n        (stride_od, 1), offsets=(out_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), x, boundary_check=(1, 0))\n"
    },
    {
      "input": "@triton.jit\ndef shrink_2D_jagged_from_tail(JaggedIn, Offsets, JaggedOut, OffsetsOut, D,\n    stride_id, stride_od, len_in, len_out, BLOCK_D: tl.constexpr):\n    off_z = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(Offsets + off_z)\n    seq_end = tl.load(Offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    seq_start_out = tl.load(OffsetsOut + off_z)\n    seq_end_out = tl.load(OffsetsOut + off_z + 1)\n    seq_len_out = seq_end_out - seq_start_out\n    if off_n >= seq_len_out:\n        return\n    to_skip = seq_len - seq_len_out\n    in_block_start = seq_start + to_skip + off_n\n    out_block_start = seq_start_out + off_n\n    x = tl.load(tl.make_block_ptr(base=JaggedIn, shape=(len_in, D), strides\n        =(stride_id, 1), offsets=(in_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), boundary_check=(1, 0))\n    tl.store(tl.make_block_ptr(base=JaggedOut, shape=(len_out, D), strides=\n        (stride_od, 1), offsets=(out_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), x, boundary_check=(1, 0))\n"
    },
    {
      "input": "@triton.jit\ndef unshrink_2D_jagged_from_tail(JaggedIn, Offsets, JaggedOut, OffsetsOut,\n    D, stride_id, stride_od, len_in, len_out, BLOCK_D: tl.constexpr):\n    off_z = tl.program_id(0)\n    off_n = tl.program_id(1)\n    seq_start = tl.load(Offsets + off_z)\n    seq_end = tl.load(Offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    seq_start_out = tl.load(OffsetsOut + off_z)\n    seq_end_out = tl.load(OffsetsOut + off_z + 1)\n    seq_len_out = seq_end_out - seq_start_out\n    if off_n >= seq_len:\n        return\n    to_skip = seq_len_out - seq_len\n    in_block_start = seq_start + off_n\n    out_block_start = seq_start_out + to_skip + off_n\n    x = tl.load(tl.make_block_ptr(base=JaggedIn, shape=(len_in, D), strides\n        =(stride_id, 1), offsets=(in_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), boundary_check=(1, 0))\n    tl.store(tl.make_block_ptr(base=JaggedOut, shape=(len_out, D), strides=\n        (stride_od, 1), offsets=(out_block_start.to(tl.int32), 0),\n        block_shape=(1, BLOCK_D), order=(1, 0)), x, boundary_check=(1, 0))\n"
    },
    {
      "input": "@triton.jit\ndef jagged_remove_first_or_last_1D_kernel(JaggedIn, Offsets,\n    JaggedOut_no_first, JaggedOut_no_last, BLOCK_N: tl.constexpr):\n    off_z = tl.program_id(0)\n    group_n = tl.program_id(1)\n    in_seq_start = tl.load(Offsets + off_z)\n    in_seq_end = tl.load(Offsets + off_z + 1)\n    in_seq_len = in_seq_end - in_seq_start\n    out_seq_start = tl.load(Offsets + off_z) - off_z\n    off_n = group_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    x = tl.load(JaggedIn + in_seq_start + off_n, mask=off_n < in_seq_len)\n    tl.store(JaggedOut_no_first + out_seq_start + off_n - 1, x, mask=(off_n >\n        0) & (off_n < in_seq_len))\n    tl.store(JaggedOut_no_last + out_seq_start + off_n, x, mask=off_n < \n        in_seq_len - 1)\n"
    },
    {
      "input": "@triton.jit\ndef jagged_recover_first_or_last_1D_kernel(JaggedIn_no_first,\n    JaggedIn_no_last, JaggedOut, OffsetsOut, BLOCK_N: tl.constexpr):\n    off_z = tl.program_id(0)\n    group_n = tl.program_id(1)\n    in_no_last_seq_start = tl.load(OffsetsOut + off_z) - off_z\n    in_no_last_seq_end = tl.load(OffsetsOut + off_z + 1) - off_z - 1\n    in_seq_len = in_no_last_seq_end - in_no_last_seq_start\n    out_seq_start = tl.load(OffsetsOut + off_z)\n    off_n = group_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    x_first = tl.load(JaggedIn_no_last + in_no_last_seq_start + off_n, mask\n        =off_n == 0)\n    tl.store(JaggedOut + out_seq_start + off_n, x_first, mask=off_n == 0)\n    x = tl.load(JaggedIn_no_last + in_no_last_seq_start + off_n, mask=(\n        off_n < in_seq_len) & (off_n > 0)) + tl.load(JaggedIn_no_first +\n        in_no_last_seq_start + off_n - 1, mask=(off_n < in_seq_len) & (\n        off_n > 0))\n    tl.store(JaggedOut + out_seq_start + off_n, x, mask=(off_n < in_seq_len\n        ) & (off_n > 0))\n    x_last = tl.load(JaggedIn_no_first + in_no_last_seq_start + off_n, mask\n        =off_n == in_seq_len - 1)\n    tl.store(JaggedOut + out_seq_start + off_n + 1, x_last, mask=off_n == \n        in_seq_len - 1)\n"
    },
    {
      "input": "@custom_autotune.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256,\n    'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N'],\n    nearest_power_of_two=True)\n@triton.jit\ndef matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M,\n    N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm,\n    stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\n    tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32 \n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_k\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = b.to(tl.float16)\n        a = a.to(tl.float16)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@custom_autotune.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256,\n    'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 256,\n    'BLOCK_SIZE_N': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'K'],\n    nearest_power_of_two=True)\n@triton.jit\ndef trans_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32 \n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_k = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_bk[:, None] // infearure_per_bits * stride_bk + \n        offs_n[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + offs_n[None, :] // infearure_per_bits + g_idx[\n        :, None] * stride_zeros\n    shifter = offs_bk % infearure_per_bits * bits\n    zeros_shifter = offs_n % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(0, num_pid_n):\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = tl.trans(b)\n        b = b.to(tl.float16)\n        a = a.to(tl.float16)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += BLOCK_SIZE_N // infearure_per_bits\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N'],\n    nearest_power_of_two=True)\n@triton.jit\ndef matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M,\n    N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm,\n    stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M:\n    tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_k\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_K': 64,\n    'BLOCK_SIZE_N': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_N':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'K'],\n    nearest_power_of_two=True)\n@triton.jit\ndef trans_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.\n    constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_k = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_bk[:, None] // infearure_per_bits * stride_bk + \n        offs_n[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + offs_n[None, :] // infearure_per_bits + g_idx[\n        :, None] * stride_zeros\n    shifter = offs_bk % infearure_per_bits * bits\n    zeros_shifter = offs_n % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(0, num_pid_n):\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = tl.trans(b)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += BLOCK_SIZE_N // infearure_per_bits\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    }
  ]